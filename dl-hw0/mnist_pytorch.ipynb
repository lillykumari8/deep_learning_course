{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6QA1D6eIszX_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 0: Initial setup\n",
        "\n",
        "To enable GPU:\n",
        "1.   Click Edit -> Notebook settings\n",
        "2.   Under Hardware Accelerator select GPU\n",
        "3.   On the right side of this page, click connect to a hosted runtime\n",
        "\n",
        "\n",
        "If you ever see an error about needing third-party cookies enabled, you can disable blocking them or whitelist them.\n",
        "Here is a simple way to whitelist (in chrome)\n",
        "\n",
        "For old chrome:\n",
        "1.   Goto chrome://settings/content/cookies>search=cookie\n",
        "2.   Uncheck \"Block third-party cookies\" or\n",
        "3.   Click Add next to Allow and type https://[*.]googleusercontent.com:443\n",
        "\n",
        "For new chrome:\n",
        "1.   Goto settings and search \"cookie\"\n",
        "2.   Click the \"content settings\" button\n",
        "3.   Follow steps 2 or 3 from above\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DHxEgxT6YwR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "613aa215-e9fc-4506-9068-070d6a917d76"
      },
      "cell_type": "code",
      "source": [
        "# This shows how to connect your google drive account with a colab instance. It's pretty easy.\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/gdrive')\n",
        "# Create a directory and mount Google Drive using that directory.\n"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gv3f_-svjwm6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Drive not connecting after it seemed like it worked before?\n",
        "1. First try restarting the runtime via Runtime -> Restart Runtime\n",
        "2. Then try to run the above again.\n",
        "3. If this still doesn't work, call Reset All Runtimes. This is the nuclear option that will delete all your data not saved on your personal drive account, and will erase everything you installed.\n"
      ]
    },
    {
      "metadata": {
        "id": "Ae5vt-EyTl-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54290800-a0fa-4bdf-a143-6d9271b1c6c5"
      },
      "cell_type": "code",
      "source": [
        "# Now let's test that Google Drive is up and running. \n",
        "# You may have to change \"My Drive\" if you have renamed it something else.\n",
        "!ls \"/gdrive/My Drive\"\n",
        "\n",
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat \"/gdrive/My Drive/foo.txt\"\n",
        "!rm \"/gdrive/My Drive/foo.txt\""
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 000.pt        001.pt   003.pt\t colab_files\t    mnist2\n",
            "'001 (1).pt'   002.pt   010.pt\t'Colab Notebooks'  'mnist2 (1)'\n",
            "Hello Google Drive!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oINm7sOOdpaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f402b259-2320-4090-ba9e-b8ba802850db"
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "  \n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())\n",
        "  \n",
        "# NOTE: This one takes a while the first time you run it, and you will likely see \n",
        "# tcmalloc: large alloc 1073750016 bytes == 0x5c54a000 @ or something similar.\n",
        "# It should then print out:\n",
        "# Version 0.4.1\n",
        "# CUDA enabled: True"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lrYXOGpsM6TV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define some useful save and restoring functions. \n",
        "# You can thank your TAs for providing this code, \n",
        "# it will probably be useful for you in the future as well.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "class pt_util(object):\n",
        "    @staticmethod\n",
        "    # This does more than the simple Pytorch restore. It checks that the names \n",
        "    # of variables match, and if they don't doesn't throw a fit. It is similar \n",
        "    # to how Caffe acts. This is especially useful if you decide to change your\n",
        "    # network architecture but don't want to retrain from scratch.\n",
        "    def restore(net, save_file):\n",
        "        net_state_dict = net.state_dict()\n",
        "        restore_state_dict = torch.load(save_file)\n",
        "\n",
        "        restored_var_names = set()\n",
        "\n",
        "        print('Restoring:')\n",
        "        for var_name in restore_state_dict.keys():\n",
        "            if var_name in net_state_dict:\n",
        "                var_size = net_state_dict[var_name].size()\n",
        "                restore_size = restore_state_dict[var_name].size()\n",
        "                if var_size != restore_size:\n",
        "                    print('Shape mismatch for var', var_name, 'expected', var_size, 'got', restore_size)\n",
        "                else:\n",
        "                    if isinstance(net_state_dict[var_name], torch.nn.Parameter):\n",
        "                        # backwards compatibility for serialized parameters\n",
        "                        net_state_dict[var_name] = restore_state_dict[var_name].data\n",
        "                    try:\n",
        "                        net_state_dict[var_name].copy_(restore_state_dict[var_name])\n",
        "                        print(str(var_name) + ' -> \\t' + str(var_size) + ' = ' + str(int(np.prod(var_size) * 4 / 10**6)) + 'MB')\n",
        "                        restored_var_names.add(var_name)\n",
        "                    except:\n",
        "                        print('While copying the parameter named {}, whose dimensions in the model are'\n",
        "                              ' {} and whose dimensions in the checkpoint are {}, ...'.format(\n",
        "                                  var_name, var_size, restore_size))\n",
        "                        raise\n",
        "\n",
        "        ignored_var_names = sorted(list(set(restore_state_dict.keys()) - restored_var_names))\n",
        "        unset_var_names = sorted(list(set(net_state_dict.keys()) - restored_var_names))\n",
        "        print('')\n",
        "        if len(ignored_var_names) == 0:\n",
        "            print('Restored all variables')\n",
        "        else:\n",
        "            print('Did not restore:\\n\\t' + '\\n\\t'.join(ignored_var_names))\n",
        "        if len(unset_var_names) == 0:\n",
        "            print('No new variables')\n",
        "        else:\n",
        "            print('Initialized but did not modify:\\n\\t' + '\\n\\t'.join(unset_var_names))\n",
        "\n",
        "        print('Restored %s' % save_file)\n",
        "        \n",
        "    @staticmethod\n",
        "    def restore_latest(net, folder):\n",
        "        import glob\n",
        "        import re\n",
        "        checkpoints = sorted(glob.glob(folder + '/*.pt'), key=os.path.getmtime)\n",
        "        start_it = 0\n",
        "        if len(checkpoints) > 0:\n",
        "            pt_util.restore(net, checkpoints[-1])\n",
        "            start_it = int(re.findall(r'\\d+', checkpoints[-1])[-1])\n",
        "        return start_it\n",
        "\n",
        "    @staticmethod\n",
        "    def save(net, file_name, num_to_keep=1):\n",
        "        folder = os.path.dirname(file_name)\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "        torch.save(net.state_dict(), file_name)\n",
        "        import glob\n",
        "        extension = os.path.splitext(file_name)[1]\n",
        "        checkpoints = sorted(glob.glob(folder + '/*' + extension), key=os.path.getmtime)\n",
        "        print('Saved %s\\n' % file_name)\n",
        "        if num_to_keep > 0:\n",
        "            for ff in checkpoints[:-num_to_keep]:\n",
        "                os.remove(ff)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u486U-pUJnDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implementing a network for MNIST"
      ]
    },
    {
      "metadata": {
        "id": "0oKKR9EsoK9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is where you define your network architecture.\n",
        "# Note: The TAs know this follows the PyTorch MNIST tutorial available at \n",
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "# Where do you think we got it from? \n",
        "# So we are asking you to implement something slightly different. \n",
        "# You can use that as a guide, but make sure you understand what it all does.\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "class MNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTNet, self).__init__()\n",
        "        # The network should be as follows:\n",
        "        # One fully connected layer with 1024 outputs.\n",
        "        # One fully connected layer with 512 outputs.\n",
        "        # Then the final classification layer.\n",
        "        # All the nonlinearities should be ReLU.\n",
        "        # These instructions are vague on purpose.\n",
        "        #raise NotImplementedError('Define the layers here')\n",
        "        self.fc1 = nn.Linear(28*28, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #raise NotImplementedError('Define the forward pass')\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "      \n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        # You should also use the cross_entropy loss rather than the NLL loss.\n",
        "        #raise NotImplementedError('Define the loss here')\n",
        "        loss_ce = F.cross_entropy(prediction, label, reduction=reduction)\n",
        "        return loss_ce\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #raise NotImplementedError('Define the forward pass and loss here')\n",
        "        output = model(data)\n",
        "        loss = model.loss(output, label, reduction='elementwise_mean')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in test_loader:\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += model.loss(output, label,reduction='elementwise_mean').item()\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pNf3AoHVvKXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Play around with these constants, you may find a better setting.\n",
        "BATCH_SIZE = 128\n",
        "TEST_BATCH_SIZE = 1000\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.025\n",
        "MOMENTUM = 0.5\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "LOG_INTERVAL = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "50zDbqjXu_Qq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1955
        },
        "outputId": "0535eb9e-5948-49f3-fc47-5e486cf55a21"
      },
      "cell_type": "code",
      "source": [
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=TEST_BATCH_SIZE, **kwargs)\n",
        "\n",
        "\n",
        "model = MNISTNet().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "# This will save checkpoints in your Google Drive account.\n",
        "start_epoch = model.load_last_model('/gdrive/My Drive/colab_files/homework1/mnist/checkpoints')\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        train(model, device, train_loader, optimizer, epoch, LOG_INTERVAL)\n",
        "        test(model, device, test_loader)\n",
        "        model.save_model('/gdrive/My Drive/colab_files/homework1/mnist/checkpoints/%03d.pt' % epoch)\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model('/gdrive/My Drive/colab_files/homework1/mnist/checkpoints/%03d.pt' % epoch)\n",
        "        "
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n",
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.295281\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.435663\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.400372\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.363470\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.300813\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9367/10000 (94%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.185505\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.210933\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.205953\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.159513\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.208536\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9543/10000 (95%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/001.pt\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.136005\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.074999\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.129063\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.121414\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.207775\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9627/10000 (96%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/002.pt\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.077163\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.109271\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.034907\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.073402\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.075513\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9682/10000 (97%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/003.pt\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.057544\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.098275\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.083053\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.082848\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.082360\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9722/10000 (97%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/004.pt\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.059485\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.036707\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.038406\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.073659\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.092390\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/005.pt\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.054433\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.079970\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.118740\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024015\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.033031\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/006.pt\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.032776\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.056801\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.025348\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.033234\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.060900\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/007.pt\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.045776\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.022570\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.017921\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.054967\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.038652\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/008.pt\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.032591\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.009583\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.024224\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.058104\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.020857\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/009.pt\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.031574\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.016887\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.019119\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.041302\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.025703\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/010.pt\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/010.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bBHdoWQZkaZD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Reimplementing the Cross Entropy loss function."
      ]
    },
    {
      "metadata": {
        "id": "wg8_YkUxk1QB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNISTNetNewLoss(MNISTNet):\n",
        "  \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'): \n",
        "        # Reimplement the Cross Entropy loss function using mathematical primitives.\n",
        "        # This means you are not allowed to use any function in the \"Loss functions\" \n",
        "        # section of https://pytorch.org/docs/stable/nn.html#id50 nor the \n",
        "        # functional versions. You can use them to verify that your output looks correct.\n",
        "        # You should implement reduction for none (i.e. return a vector, sum, and elementwise_mean).\n",
        "        # Note: Due to floating point errors, the values won't be exactly equal.\n",
        "        # Second note: You can assume inputs will be 2D (batch X features).\n",
        "        loss_val_old = super(MNISTNetNewLoss, self).loss(prediction, label, reduction)\n",
        "        #raise NotImplementedError('Define the loss here')\n",
        "        \n",
        "        m = prediction.size()[0]\n",
        "        prediction = F.softmax(prediction)\n",
        "        prediction = prediction[range(m), label]\n",
        "        loss_val = -torch.log(prediction)\n",
        "        if reduction=='none':\n",
        "          loss_val_new = loss_val\n",
        "        elif reduction=='sum':\n",
        "          loss_val_new = torch.sum(loss_val)\n",
        "        else:\n",
        "          loss_val_new = torch.sum(loss_val)/m\n",
        "        \n",
        "        assert(abs(loss_val_new - loss_val_old).item() < 0.01)\n",
        "        return loss_val_new        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MIKHYa71mD79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1989
        },
        "outputId": "402c25a3-59d5-4113-c218-16ae867e4ec6"
      },
      "cell_type": "code",
      "source": [
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=TEST_BATCH_SIZE, **kwargs)\n",
        "\n",
        "\n",
        "model = MNISTNetNewLoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "start_epoch = model.load_last_model('/gdrive/My Drive/colab_files/homework1/mnist2/checkpoints')\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        train(model, device, train_loader, optimizer, epoch, LOG_INTERVAL)\n",
        "        test(model, device, test_loader)\n",
        "        model.save_model('/gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/%03d.pt' % epoch)\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model('/gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/%03d.pt' % epoch)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n",
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.295281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.435656\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.400289\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.363284\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.300673\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9367/10000 (94%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.185565\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.210928\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.205784\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.159453\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.208128\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9542/10000 (95%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/001.pt\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.135887\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.074812\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.129010\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.121466\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.208816\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9628/10000 (96%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/002.pt\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.077277\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.109619\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.034869\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.073353\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.075808\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9685/10000 (97%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/003.pt\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.057399\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.097835\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.082035\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.082451\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.082731\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9723/10000 (97%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/004.pt\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.059740\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.036931\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.038466\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.074150\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.092376\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9752/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/005.pt\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.054008\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.079229\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.117667\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024411\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.033375\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9751/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/006.pt\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.032760\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.057494\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.025581\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.033293\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.061249\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/007.pt\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.045073\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.023078\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.017795\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.055071\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.038744\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/008.pt\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.033166\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.009198\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.023676\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.058172\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.021296\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/009.pt\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.031606\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.016996\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.019125\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.041491\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.026413\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/010.pt\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/010.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lilly.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "5EJQLxb5_Zh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "In this homework, we will have you train some convolutional neural networks! We will start with a small dataset, and then work our way up to ImageNet!\n",
        "\n",
        "For this homework we will train a classifier for Tiny Imagenet which you can read about here https://tiny-imagenet.herokuapp.com/ but don't bother downloading it just yet.\n",
        "\n",
        "A note on file paths: Last homework, we used the homework1 folder. That was a mistake as it was technically homework 0. You may want to rename the folder from last time to homework0 before you begin.\n",
        "\n",
        "# Part 0: Initial Setup\n",
        "You should recognize this code from last time.\n"
      ]
    },
    {
      "metadata": {
        "id": "BkEgmb7Odtec",
        "colab_type": "code",
        "outputId": "0de114e4-af4e-4928-b463-a3300db350a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (483.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 483.0MB 51.1MB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x57d22000 @  0x7f1f331482a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n",
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gptyTwi0AQHf",
        "colab_type": "code",
        "outputId": "678682f9-3a10-4ca3-971b-64cc73b60b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "'My Drive'  'Team Drives'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BjUlLqslACEW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Upload the Dataset\n",
        "Run this code to navigate to the BASE_PATH directory and upload the homework1.tar file inside the BASE_PATH, then extract it.\n",
        "\n",
        "Have a look at pt_util. We moved some of the useful functions out of the python notebook to make it less cluttered, and added a few more useful functions.\n",
        "\n",
        "I made the BASE_PATH and DATA_PATH variables so you don't have to copy the same strings all over the place if you want to move the locations of the files around.\n"
      ]
    },
    {
      "metadata": {
        "id": "2UtsxBCpChPn",
        "colab_type": "code",
        "outputId": "197c9dd7-3ed8-403f-fc26-26b8cd3da157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/gdrive/My Drive/colab_files/homework1/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'tiny_imagenet/'\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(DATA_PATH + 'train.h5'):\n",
        "    !wget http://pjreddie.com/media/files/homework1.tar\n",
        "    !tar -xvf homework1.tar\n",
        "    !rm homework1.tar\n",
        "os.chdir('/content')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yvK-kdPRav5L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import h5py\n",
        "import sys\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util\n",
        "from PIL import Image\n",
        "from skimage import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b8NWTxZvJeAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Defining the Network\n",
        "We're giving you no instructions on this part. Welcome to deep learning research! See if you can get above 40% accuracy. You probably want to use the Cross Entropy error again, but who knows, maybe you can find a better loss function. We will give you a few hints of things to try. Have a look at https://github.com/pytorch/examples/blob/master/mnist/main.py for a basic neural network implementation.\n",
        "\n",
        "\n",
        "- Activation functions other than ReLU\n",
        "- Batch Norm\n",
        "- Dropout\n",
        "- Residual connections\n",
        "\n",
        "Additionally, the current saving function always saves the latest results. You may want to modify it to only save the results if they have the highest test accuracy. Please make this modification.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LBIiezWlXCSv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TinyImagenetNet1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyImagenetNet, self).__init__()\n",
        "        # TODO define the layers\n",
        "        # raise NotImplementedError('Need to define the layers for your network')\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_1 = nn.ReLU(True);\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(64);\n",
        "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "\n",
        "        self.conv_2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_2 = nn.ReLU(True);\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(128);\n",
        "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv_3 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_3 = nn.ReLU(True);\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(128);\n",
        "        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv_4 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_4 = nn.ReLU(True);\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(128);\n",
        "        self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "       \n",
        "        self.conv_5 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_5 = nn.ReLU(True);\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(128);\n",
        "\n",
        "        self.fc_1 = nn.Linear(512, 400);\n",
        "        self.relu_6 = nn.Softmax(1);\n",
        "        self.batch_norm_6 = nn.BatchNorm1d(400);\n",
        "        self.dropout_1 = nn.Dropout(p=0.5);\n",
        "\n",
        "        # Softmax or SVM\n",
        "        self.fc_2 = nn.Linear(400, 200);\n",
        "\n",
        "    def forward(self, y):\n",
        "        # TODO define the forward pass\n",
        "        #raise NotImplementedError('Need to define the forward pass')\n",
        "        y = self.conv_1(y)\n",
        "        y = self.relu_1(y)\n",
        "        y = self.batch_norm_1(y)\n",
        "        y = self.pool_1(y)\n",
        "        \n",
        "        y = self.conv_2(y)\n",
        "        y = self.relu_2(y)\n",
        "        y = self.batch_norm_2(y)\n",
        "        y = self.pool_2(y)\n",
        "        \n",
        "        y = self.conv_3(y)\n",
        "        y = self.relu_3(y)\n",
        "        y = self.batch_norm_3(y)\n",
        "        y = self.pool_3(y)\n",
        "\n",
        "        y = self.conv_4(y)\n",
        "        y = self.relu_4(y)\n",
        "        y = self.batch_norm_4(y)\n",
        "        y = self.pool_4(y)\n",
        "        \n",
        "        y = self.conv_5(y)\n",
        "        y = self.relu_5(y)\n",
        "        y = self.batch_norm_5(y)\n",
        "        \n",
        "        y = y.view(y.size(0), -1)\n",
        "        y = self.fc_1(y)\n",
        "        y = self.relu_6(y)\n",
        "        y = self.batch_norm_6(y)\n",
        "        y = self.dropout_1(y)\n",
        "        \n",
        "        y = self.fc_2(y)\n",
        "        return y\n",
        "      \n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, is_best, num_to_keep=1):\n",
        "        # TODO save the model if it is the best\n",
        "        if is_best:\n",
        "          print(\"Saving new best checkpoint with accuracy,\", accuracy)\n",
        "          pt_util.save(self, file_path, num_to_keep)\n",
        "        else:\n",
        "          print (\"Test accuracy did not improve\")\n",
        "        #raise NotImplementedError('Need to implement save_best_model')\n",
        "        \n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RA6lPT8Ceubk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TinyImagenetNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyImagenetNet, self).__init__()\n",
        "        # TODO define the layers\n",
        "        # raise NotImplementedError('Need to define the layers for your network')\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(3, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_1 = nn.ReLU(True);\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(128);\n",
        "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "\n",
        "        self.conv_2 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_2 = nn.ReLU(True);\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(128);\n",
        "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv_3 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_3 = nn.ReLU(True);\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(128);\n",
        "        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv_4 = nn.Conv2d(128, 128, kernel_size=2, stride=1, padding=2)\n",
        "        self.relu_4 = nn.ReLU(True);\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(128);\n",
        "        self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "       \n",
        "        self.conv_5 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_5 = nn.ReLU(True);\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(128);\n",
        "\n",
        "        self.fc_1 = nn.Linear(3200, 1024);\n",
        "        self.relu_6 = nn.ReLU(True);\n",
        "        self.batch_norm_6 = nn.BatchNorm1d(1024);\n",
        "        self.dropout_1 = nn.Dropout(p=0.5);\n",
        "\n",
        "        # Softmax or SVM\n",
        "        self.fc_2 = nn.Linear(1024, 200);\n",
        "\n",
        "\n",
        "    def forward(self, y):\n",
        "        # TODO define the forward pass\n",
        "        # raise NotImplementedError('Need to define the forward pass')\n",
        "        y = self.conv_1(y)\n",
        "        y = self.relu_1(y)\n",
        "        y = self.batch_norm_1(y)\n",
        "        y = self.pool_1(y)\n",
        "        \n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_2(y)\n",
        "        y = self.relu_2(y)\n",
        "        y = self.batch_norm_2(y)\n",
        "        y = self.pool_2(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_3(y)\n",
        "        y = self.relu_3(y)\n",
        "        y = self.batch_norm_3(y)\n",
        "        y = self.pool_3(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_4(y)\n",
        "        y = self.relu_4(y)\n",
        "        y = self.batch_norm_4(y)\n",
        "        y = self.pool_4(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_5(y)\n",
        "        y = self.relu_5(y)\n",
        "        y = self.batch_norm_5(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = y.view(y.size(0), -1)\n",
        "#         print (y.shape)\n",
        "        y = self.fc_1(y)\n",
        "        y = self.relu_6(y)\n",
        "        y = self.batch_norm_6(y)\n",
        "        y = self.dropout_1(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.fc_2(y)\n",
        "#         print (y.shape)\n",
        "        return y\n",
        "\n",
        "      \n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "      \n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, is_best, num_to_keep=1):\n",
        "        # TODO save the model if it is the best\n",
        "        if is_best:\n",
        "          print(\"Saving new best checkpoint with accuracy,\", accuracy)\n",
        "          pt_util.save(self, file_path, num_to_keep)\n",
        "        else:\n",
        "          print (\"Test accuracy did not improve\")\n",
        "        #raise NotImplementedError('Need to implement save_best_model')\n",
        "        \n",
        "        \n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yiJSkXjiKpDL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This time we are giving you the train and test functions, but feel free to modify them if you want. \n",
        "\n",
        "You may need to return some additional information for the logging portion of this assignment.\n"
      ]
    },
    {
      "metadata": {
        "id": "pmuzixXrkuYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        loss = model.loss(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                time.ctime(time.time()),\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        return loss\n",
        "\n",
        "def test(model, device, test_loader, return_images=False, log_interval=None):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    correct_images = []\n",
        "    correct_values = []\n",
        "\n",
        "    error_images = []\n",
        "    predicted_values = []\n",
        "    gt_values = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            test_loss_on = model.loss(output, label, reduction='sum').item()\n",
        "            test_loss += test_loss_on\n",
        "            pred = output.max(1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            if return_images:\n",
        "                if num_correct > 0:\n",
        "                    correct_images.append(data[correct_mask, ...].data.cpu().numpy())\n",
        "                    correct_value_data = label[correct_mask].data.cpu().numpy()[:, 0]\n",
        "                    correct_values.append(correct_value_data)\n",
        "                if num_correct < len(label):\n",
        "                    error_data = data[~correct_mask, ...].data.cpu().numpy()\n",
        "                    error_images.append(error_data)\n",
        "                    predicted_value_data = pred[~correct_mask].data.cpu().numpy()\n",
        "                    predicted_values.append(predicted_value_data)\n",
        "                    gt_value_data = label[~correct_mask].data.cpu().numpy()[:, 0]\n",
        "                    gt_values.append(gt_value_data)\n",
        "            if log_interval is not None and batch_idx % log_interval == 0:\n",
        "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    time.ctime(time.time()),\n",
        "                    batch_idx * len(data), len(test_loader.dataset),\n",
        "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
        "    if return_images:\n",
        "        correct_images = np.concatenate(correct_images, axis=0)\n",
        "        error_images = np.concatenate(error_images, axis=0)\n",
        "        predicted_values = np.concatenate(predicted_values, axis=0)\n",
        "        correct_values = np.concatenate(correct_values, axis=0)\n",
        "        gt_values = np.concatenate(gt_values, axis=0)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "    if return_images:\n",
        "        return correct_images, correct_values, error_images, predicted_values, gt_values, test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EwMDBwoCDRS_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PART 3: Loading Data\n",
        "PyTorch has a nice interface for dealing with a variety of data. You can read a good tutorial here https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "Your friendly neighborhood TAs have made it even easier by preprocessing the data into a nice format. The data you uploaded is stored using hdf5 files which can be acecces a lot like Numpy arrays using the h5py package. In each of the files, there is a \"dataset\" called 'images', and one called 'labels'. Read more about h5py here http://docs.h5py.org/en/latest/quick.html\n",
        "\n",
        "Speed hint: With small datasets, it is almost always a good idea to cache the data to disk rather than continually read from files.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hMwpbpm8voQn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Data loader\n",
        "class H5Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, h5_file, transform=None):\n",
        "        # TODO Implement data loading.\n",
        "        self.h5_file = h5_file\n",
        "        self.transform = transform\n",
        "        #raise NotImplementedError('Need to implement the data loading')\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        # TODO Implement the length function\n",
        "        #raise NotImplementedError('Need to return the lengeth of the dataset')\n",
        "        with h5py.File(self.h5_file, 'r') as db:\n",
        "          lengt = len(db['images'])\n",
        "        return lengt\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO implement the getitem function\n",
        "        # You should return a tuple of:\n",
        "        #    a torch tensor containing single image in CxHxW format and\n",
        "        #    the label as a single tensor scalar.\n",
        "        #raise NotImplementedError('Need to implement the data loading')\n",
        "      \n",
        "        with h5py.File(self.h5_file, 'r') as db:\n",
        "            image = torch.from_numpy(db['images'][idx].transpose(2,0,1))\n",
        "            label = torch.from_numpy(db['labels'][idx].astype(np.long))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "#         if (label<0 or label>200):\n",
        "#             print (idx, \"------>\", label)\n",
        "        data = image.type(torch.FloatTensor)\n",
        "        return (data, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tfWvYzBaKcO7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Dataset Augmentation\n",
        "In the MNIST assignment, we didn't do any data augmentation because MNIST is kind of easy.\n",
        "\n",
        "In this assignment, you may find that data augmentation helps you a lot (or possibly hurts your performance).\n",
        "\n",
        "You can find a bunch preimplemented here https://pytorch.org/docs/stable/torchvision/transforms.html and you can also do your own as seen in the tutorial from part 3.\n",
        "\n",
        "Play around with various data augmentations we will suggest some.\n",
        "\n",
        "- ToPILImage - This one is useful for a lot of the built in transforms which expect PIL images. \n",
        "- RandomHorizontalFlip\n",
        "- RandomResizedCrop\n",
        "- ColorJitter\n",
        "- RandomRotation\n",
        "- Normalize\n",
        "- Adding various types of noise\n",
        "- ToTensor - PyTorch expects the output from the dataset to be a tensor in CxHxW format.\n",
        "\n",
        "\n",
        "Note: You should be careful about which of these you apply to the test data. You usually don't want to apply noise to the test data, but you do want to normalize it in the same way for example.\n"
      ]
    },
    {
      "metadata": {
        "id": "-5JeXSx9LIx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_transforms = None\n",
        "# test_transforms = None\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(64),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(18),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_train = H5Dataset(DATA_PATH + 'train.h5', transform=train_transforms)\n",
        "data_test = H5Dataset(DATA_PATH + 'val.h5', transform=test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "piz_PoP-N5mK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 5: Training the network\n",
        "Generally, it is useful to see how your training is going. Often people print their loss to make sure it goes down and their accuracy to make sure it goes up. But pictures are better than words. So for this part, you should record and plot the training loss, test loss, and test accuracy (and whatever else you want). \n",
        "\n",
        "We have created a very simple logging interface which essentially just saves and restores files via pickle in pt_util. Saving and restoring log data is important if you end your run early and want to continue where you left off rather than starting over.\n",
        "\n",
        "We have also provided a plot function which can plot a single line graph. You can use it and plot each value independently, or change it to plot them all in one graph. \n",
        "\n",
        "\n",
        "__Important note: Do not forget to title your graphs and label your axes. Plots are meaningless without a way to read them.__\n",
        "\n",
        "Second Note: It will be helpful for you when deciding what network structure, data augmentation, and such work to title the graphs accordingly so you remember.\n",
        "Third Note: The default setup right now saves and restores the network weights from a single folder. When you modify network architectures, you may want to save the resulting files in different folders (with appropriate names).\n",
        "\n",
        "We also provided a function for showing some results, because it's not satisfying to train a neural net, you also want to see what it can do! This can also be useful for figuring out what your network is doing well, and what it is failing at. This type of error analysis is very common when training neural networks.\n"
      ]
    },
    {
      "metadata": {
        "id": "Hj-JBTfwk-4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Play around with these constants, you may find a better setting.\n",
        "BATCH_SIZE = 256\n",
        "best_accuracy = torch.FloatTensor([0])\n",
        "TEST_BATCH_SIZE = 10\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0.0005\n",
        "LOG_PATH = DATA_PATH + 'log.pkl'\n",
        "CHECKPOINT_PATH = BASE_PATH + 'tiny_imagenet/checkpoints'\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "#\n",
        "class_names = [line.strip().split(', ') for line in open(DATA_PATH + 'class_names.txt')]\n",
        "name_to_class = {line[1]: line[0] for line in class_names}\n",
        "class_names = [line[1] for line in class_names]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = TinyImagenetNet().to(device)\n",
        "# model = TinyImagenetNet1().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "\n",
        "# You may want to define another default for your log data depending on how you save it.\n",
        "log_data = pt_util.read_log(LOG_PATH, [])\n",
        "\n",
        "correct_images, correct_val, error_images, predicted_val, gt_val, test_loss, acc = test(model, device, test_loader, True)\n",
        "correct_images = pt_util.to_scaled_uint8(correct_images.transpose(0, 2, 3, 1))\n",
        "error_images = pt_util.to_scaled_uint8(error_images.transpose(0, 2, 3, 1))\n",
        "pt_util.show_images(correct_images, ['correct: %s' % class_names[aa] for aa in correct_val])\n",
        "pt_util.show_images(error_images, ['pred: %s, actual: %s' % (class_names[aa], class_names[bb]) for aa, bb in zip(predicted_val, gt_val)])\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\n",
        "        correct_images, correct_val, error_images, predicted_val, gt_val, test_loss, acc = test(model, device, test_loader, True)\n",
        "        acc = torch.FloatTensor([acc])\n",
        "        is_best = bool(acc.numpy() > best_accuracy.numpy())\n",
        "        best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n",
        "        model.save_best_model(acc, CHECKPOINT_PATH + '/%03d.pt' % epoch, is_best=is_best)\n",
        "        # TODO define other things to do at the end of each loop like logging and saving the best model.\n",
        "        data2dump = [epoch, loss.item(), test_loss, acc.item()]\n",
        "        print (data2dump)\n",
        "        log_data.append(data2dump)\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    # Always save the most recent model, but don't delete any existing ones.\n",
        "    model.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)\n",
        "    pt_util.write_log(LOG_PATH, log_data)\n",
        "    # Show some current correct/incorrect images.\n",
        "    correct_images = pt_util.to_scaled_uint8(correct_images.transpose(0, 2, 3, 1))\n",
        "    error_images = pt_util.to_scaled_uint8(error_images.transpose(0, 2, 3, 1))\n",
        "    pt_util.show_images(correct_images, ['correct: %s' % class_names[aa] for aa in correct_val])\n",
        "    pt_util.show_images(error_images, ['pred: %s, actual: %s' % (class_names[aa], class_names[bb]) for aa, bb in zip(predicted_val, gt_val)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1Wm5_t4W3jz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logs of running TinyImagenetNet1 \n",
        "# executed this training on another google account, hence copying just the logs over here\n",
        "\n",
        "# Using device cuda\n",
        "# num cpus: 2\n",
        "\n",
        "# Test set: Average loss: 5.2987, Accuracy: 50/10000 (0%)\n",
        "\n",
        "\n",
        "\n",
        "# Sun Oct 28 08:49:18 2018 Train Epoch: 0 [0/100000 (0%)]\tLoss: 5.307612\n",
        "# Sun Oct 28 08:50:36 2018 Train Epoch: 0 [25600/100000 (26%)]\tLoss: 5.116283\n",
        "# Sun Oct 28 08:51:54 2018 Train Epoch: 0 [51200/100000 (51%)]\tLoss: 4.976608\n",
        "# Sun Oct 28 08:53:13 2018 Train Epoch: 0 [76800/100000 (77%)]\tLoss: 4.870999\n",
        "\n",
        "# Test set: Average loss: 4.7776, Accuracy: 489/10000 (5%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([4.8900])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/000.pt\n",
        "\n",
        "# Sun Oct 28 08:54:59 2018 Train Epoch: 1 [0/100000 (0%)]\tLoss: 4.821878\n",
        "# Sun Oct 28 08:56:15 2018 Train Epoch: 1 [25600/100000 (26%)]\tLoss: 4.700505\n",
        "# Sun Oct 28 08:57:33 2018 Train Epoch: 1 [51200/100000 (51%)]\tLoss: 4.748496\n",
        "# Sun Oct 28 08:58:50 2018 Train Epoch: 1 [76800/100000 (77%)]\tLoss: 4.515704\n",
        "\n",
        "# Test set: Average loss: 4.6302, Accuracy: 658/10000 (7%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([6.5800])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/001.pt\n",
        "\n",
        "# Sun Oct 28 09:00:35 2018 Train Epoch: 2 [0/100000 (0%)]\tLoss: 4.430208\n",
        "# Sun Oct 28 09:01:52 2018 Train Epoch: 2 [25600/100000 (26%)]\tLoss: 4.393720\n",
        "# Sun Oct 28 09:03:10 2018 Train Epoch: 2 [51200/100000 (51%)]\tLoss: 4.431036\n",
        "# Sun Oct 28 09:04:27 2018 Train Epoch: 2 [76800/100000 (77%)]\tLoss: 4.349104\n",
        "\n",
        "# Test set: Average loss: 4.3792, Accuracy: 886/10000 (9%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([8.8600])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/002.pt\n",
        "\n",
        "# Sun Oct 28 09:06:12 2018 Train Epoch: 3 [0/100000 (0%)]\tLoss: 4.371693\n",
        "# Sun Oct 28 09:07:30 2018 Train Epoch: 3 [25600/100000 (26%)]\tLoss: 4.169692\n",
        "# Sun Oct 28 09:08:48 2018 Train Epoch: 3 [51200/100000 (51%)]\tLoss: 4.155505\n",
        "# Sun Oct 28 09:10:06 2018 Train Epoch: 3 [76800/100000 (77%)]\tLoss: 4.259796\n",
        "\n",
        "# Test set: Average loss: 4.3232, Accuracy: 943/10000 (9%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([9.4300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/003.pt\n",
        "\n",
        "# Sun Oct 28 09:11:50 2018 Train Epoch: 4 [0/100000 (0%)]\tLoss: 4.187215\n",
        "# Sun Oct 28 09:13:08 2018 Train Epoch: 4 [25600/100000 (26%)]\tLoss: 4.099497\n",
        "# Sun Oct 28 09:14:26 2018 Train Epoch: 4 [51200/100000 (51%)]\tLoss: 4.144973\n",
        "# Sun Oct 28 09:15:43 2018 Train Epoch: 4 [76800/100000 (77%)]\tLoss: 4.291788\n",
        "\n",
        "# Test set: Average loss: 4.2500, Accuracy: 1051/10000 (11%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([10.5100])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/004.pt\n",
        "\n",
        "# Sun Oct 28 09:17:28 2018 Train Epoch: 5 [0/100000 (0%)]\tLoss: 4.048487\n",
        "# Sun Oct 28 09:18:47 2018 Train Epoch: 5 [25600/100000 (26%)]\tLoss: 4.077675\n",
        "# Sun Oct 28 09:20:04 2018 Train Epoch: 5 [51200/100000 (51%)]\tLoss: 4.123528\n",
        "# Sun Oct 28 09:21:21 2018 Train Epoch: 5 [76800/100000 (77%)]\tLoss: 4.243632\n",
        "\n",
        "# Test set: Average loss: 4.1727, Accuracy: 1153/10000 (12%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([11.5300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/005.pt\n",
        "\n",
        "# Sun Oct 28 09:23:05 2018 Train Epoch: 6 [0/100000 (0%)]\tLoss: 4.262159\n",
        "# Sun Oct 28 09:24:23 2018 Train Epoch: 6 [25600/100000 (26%)]\tLoss: 4.054272\n",
        "# Sun Oct 28 09:25:40 2018 Train Epoch: 6 [51200/100000 (51%)]\tLoss: 4.218178\n",
        "# Sun Oct 28 09:26:57 2018 Train Epoch: 6 [76800/100000 (77%)]\tLoss: 4.068530\n",
        "\n",
        "# Test set: Average loss: 4.1640, Accuracy: 1162/10000 (12%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([11.6200])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/006.pt\n",
        "\n",
        "# Sun Oct 28 09:28:41 2018 Train Epoch: 7 [0/100000 (0%)]\tLoss: 3.933676\n",
        "# Sun Oct 28 09:29:58 2018 Train Epoch: 7 [25600/100000 (26%)]\tLoss: 3.879549\n",
        "# Sun Oct 28 09:31:15 2018 Train Epoch: 7 [51200/100000 (51%)]\tLoss: 4.072111\n",
        "# Sun Oct 28 09:32:33 2018 Train Epoch: 7 [76800/100000 (77%)]\tLoss: 3.706096\n",
        "\n",
        "# Test set: Average loss: 4.0562, Accuracy: 1366/10000 (14%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([13.6600])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/007.pt\n",
        "\n",
        "# Sun Oct 28 09:34:18 2018 Train Epoch: 8 [0/100000 (0%)]\tLoss: 3.773777\n",
        "# Sun Oct 28 09:35:36 2018 Train Epoch: 8 [25600/100000 (26%)]\tLoss: 3.836999\n",
        "# Sun Oct 28 09:36:53 2018 Train Epoch: 8 [51200/100000 (51%)]\tLoss: 4.005557\n",
        "# Sun Oct 28 09:38:11 2018 Train Epoch: 8 [76800/100000 (77%)]\tLoss: 4.003318\n",
        "\n",
        "# Test set: Average loss: 4.0964, Accuracy: 1361/10000 (14%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 09:39:56 2018 Train Epoch: 9 [0/100000 (0%)]\tLoss: 3.796965\n",
        "# Sun Oct 28 09:41:13 2018 Train Epoch: 9 [25600/100000 (26%)]\tLoss: 3.869662\n",
        "# Sun Oct 28 09:42:31 2018 Train Epoch: 9 [51200/100000 (51%)]\tLoss: 3.749494\n",
        "# Sun Oct 28 09:43:49 2018 Train Epoch: 9 [76800/100000 (77%)]\tLoss: 3.839813\n",
        "\n",
        "# Test set: Average loss: 4.0303, Accuracy: 1440/10000 (14%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([14.4000])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/009.pt\n",
        "\n",
        "# Sun Oct 28 09:45:35 2018 Train Epoch: 10 [0/100000 (0%)]\tLoss: 3.862079\n",
        "# Sun Oct 28 09:46:53 2018 Train Epoch: 10 [25600/100000 (26%)]\tLoss: 3.878287\n",
        "# Sun Oct 28 09:48:12 2018 Train Epoch: 10 [51200/100000 (51%)]\tLoss: 3.681420\n",
        "# Sun Oct 28 09:49:31 2018 Train Epoch: 10 [76800/100000 (77%)]\tLoss: 3.707539\n",
        "\n",
        "# Test set: Average loss: 3.9315, Accuracy: 1544/10000 (15%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([15.4400])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/010.pt\n",
        "\n",
        "# Sun Oct 28 09:51:17 2018 Train Epoch: 11 [0/100000 (0%)]\tLoss: 3.884487\n",
        "# Sun Oct 28 09:52:35 2018 Train Epoch: 11 [25600/100000 (26%)]\tLoss: 3.840786\n",
        "# Sun Oct 28 09:53:54 2018 Train Epoch: 11 [51200/100000 (51%)]\tLoss: 3.787436\n",
        "# Sun Oct 28 09:55:12 2018 Train Epoch: 11 [76800/100000 (77%)]\tLoss: 3.766667\n",
        "\n",
        "# Test set: Average loss: 3.9508, Accuracy: 1552/10000 (16%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([15.5200])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/011.pt\n",
        "\n",
        "# Sun Oct 28 09:56:57 2018 Train Epoch: 12 [0/100000 (0%)]\tLoss: 3.702295\n",
        "# Sun Oct 28 09:58:15 2018 Train Epoch: 12 [25600/100000 (26%)]\tLoss: 3.690314\n",
        "# Sun Oct 28 09:59:34 2018 Train Epoch: 12 [51200/100000 (51%)]\tLoss: 3.866076\n",
        "# Sun Oct 28 10:00:52 2018 Train Epoch: 12 [76800/100000 (77%)]\tLoss: 3.727367\n",
        "\n",
        "# Test set: Average loss: 3.9355, Accuracy: 1556/10000 (16%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([15.5600])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/012.pt\n",
        "\n",
        "# Sun Oct 28 10:02:37 2018 Train Epoch: 13 [0/100000 (0%)]\tLoss: 3.652793\n",
        "# Sun Oct 28 10:03:56 2018 Train Epoch: 13 [25600/100000 (26%)]\tLoss: 3.622422\n",
        "# Sun Oct 28 10:05:16 2018 Train Epoch: 13 [51200/100000 (51%)]\tLoss: 3.522907\n",
        "# Sun Oct 28 10:06:35 2018 Train Epoch: 13 [76800/100000 (77%)]\tLoss: 3.624150\n",
        "\n",
        "# Test set: Average loss: 3.8291, Accuracy: 1733/10000 (17%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([17.3300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/013.pt\n",
        "\n",
        "# Sun Oct 28 10:08:22 2018 Train Epoch: 14 [0/100000 (0%)]\tLoss: 3.547174\n",
        "# Sun Oct 28 10:09:41 2018 Train Epoch: 14 [25600/100000 (26%)]\tLoss: 3.549110\n",
        "# Sun Oct 28 10:11:00 2018 Train Epoch: 14 [51200/100000 (51%)]\tLoss: 3.831081\n",
        "# Sun Oct 28 10:12:18 2018 Train Epoch: 14 [76800/100000 (77%)]\tLoss: 3.688018\n",
        "\n",
        "# Test set: Average loss: 3.8779, Accuracy: 1673/10000 (17%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 10:14:04 2018 Train Epoch: 15 [0/100000 (0%)]\tLoss: 3.550976\n",
        "# Sun Oct 28 10:15:22 2018 Train Epoch: 15 [25600/100000 (26%)]\tLoss: 3.507267\n",
        "# Sun Oct 28 10:16:41 2018 Train Epoch: 15 [51200/100000 (51%)]\tLoss: 3.668062\n",
        "# Sun Oct 28 10:18:00 2018 Train Epoch: 15 [76800/100000 (77%)]\tLoss: 3.719428\n",
        "\n",
        "# Test set: Average loss: 3.8502, Accuracy: 1668/10000 (17%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 10:19:45 2018 Train Epoch: 16 [0/100000 (0%)]\tLoss: 3.509626\n",
        "# Sun Oct 28 10:21:02 2018 Train Epoch: 16 [25600/100000 (26%)]\tLoss: 3.577994\n",
        "# Sun Oct 28 10:22:22 2018 Train Epoch: 16 [51200/100000 (51%)]\tLoss: 3.548817\n",
        "# Sun Oct 28 10:23:41 2018 Train Epoch: 16 [76800/100000 (77%)]\tLoss: 3.539297\n",
        "\n",
        "# Test set: Average loss: 3.8898, Accuracy: 1693/10000 (17%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 10:25:26 2018 Train Epoch: 17 [0/100000 (0%)]\tLoss: 3.734539\n",
        "# Sun Oct 28 10:26:45 2018 Train Epoch: 17 [25600/100000 (26%)]\tLoss: 3.538955\n",
        "# Sun Oct 28 10:28:04 2018 Train Epoch: 17 [51200/100000 (51%)]\tLoss: 3.644810\n",
        "# Sun Oct 28 10:29:23 2018 Train Epoch: 17 [76800/100000 (77%)]\tLoss: 3.562787\n",
        "\n",
        "# Test set: Average loss: 3.8700, Accuracy: 1681/10000 (17%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 10:31:09 2018 Train Epoch: 18 [0/100000 (0%)]\tLoss: 3.503741\n",
        "# Sun Oct 28 10:32:27 2018 Train Epoch: 18 [25600/100000 (26%)]\tLoss: 3.728157\n",
        "# Sun Oct 28 10:33:46 2018 Train Epoch: 18 [51200/100000 (51%)]\tLoss: 3.542797\n",
        "# Sun Oct 28 10:35:05 2018 Train Epoch: 18 [76800/100000 (77%)]\tLoss: 3.705638\n",
        "\n",
        "# Test set: Average loss: 3.7637, Accuracy: 1809/10000 (18%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([18.0900])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/018.pt\n",
        "\n",
        "# Sun Oct 28 10:36:51 2018 Train Epoch: 19 [0/100000 (0%)]\tLoss: 3.652968\n",
        "# Sun Oct 28 10:38:09 2018 Train Epoch: 19 [25600/100000 (26%)]\tLoss: 3.616206\n",
        "# Sun Oct 28 10:39:28 2018 Train Epoch: 19 [51200/100000 (51%)]\tLoss: 3.662362\n",
        "# Sun Oct 28 10:40:47 2018 Train Epoch: 19 [76800/100000 (77%)]\tLoss: 3.562883\n",
        "\n",
        "# Test set: Average loss: 3.8718, Accuracy: 1649/10000 (16%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 10:42:33 2018 Train Epoch: 20 [0/100000 (0%)]\tLoss: 3.725989\n",
        "# Sun Oct 28 10:43:53 2018 Train Epoch: 20 [25600/100000 (26%)]\tLoss: 3.540779\n",
        "# Sun Oct 28 10:45:11 2018 Train Epoch: 20 [51200/100000 (51%)]\tLoss: 3.443722\n",
        "# Sun Oct 28 10:46:30 2018 Train Epoch: 20 [76800/100000 (77%)]\tLoss: 3.488822\n",
        "\n",
        "# Test set: Average loss: 3.6389, Accuracy: 2028/10000 (20%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([20.2800])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/020.pt\n",
        "\n",
        "# Sun Oct 28 10:48:17 2018 Train Epoch: 21 [0/100000 (0%)]\tLoss: 3.380661\n",
        "# Sun Oct 28 10:49:37 2018 Train Epoch: 21 [25600/100000 (26%)]\tLoss: 3.355520\n",
        "# Sun Oct 28 10:50:56 2018 Train Epoch: 21 [51200/100000 (51%)]\tLoss: 3.531863\n",
        "# Sun Oct 28 10:52:15 2018 Train Epoch: 21 [76800/100000 (77%)]\tLoss: 3.296263\n",
        "\n",
        "# Test set: Average loss: 3.7673, Accuracy: 1797/10000 (18%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 10:54:03 2018 Train Epoch: 22 [0/100000 (0%)]\tLoss: 3.392919\n",
        "# Sun Oct 28 10:55:21 2018 Train Epoch: 22 [25600/100000 (26%)]\tLoss: 3.408404\n",
        "# Sun Oct 28 10:56:39 2018 Train Epoch: 22 [51200/100000 (51%)]\tLoss: 3.457714\n",
        "# Sun Oct 28 10:57:57 2018 Train Epoch: 22 [76800/100000 (77%)]\tLoss: 3.591716\n",
        "\n",
        "# Test set: Average loss: 3.7459, Accuracy: 1848/10000 (18%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 10:59:43 2018 Train Epoch: 23 [0/100000 (0%)]\tLoss: 3.403530\n",
        "# Sun Oct 28 11:01:00 2018 Train Epoch: 23 [25600/100000 (26%)]\tLoss: 3.340816\n",
        "# Sun Oct 28 11:02:17 2018 Train Epoch: 23 [51200/100000 (51%)]\tLoss: 3.546796\n",
        "# Sun Oct 28 11:03:35 2018 Train Epoch: 23 [76800/100000 (77%)]\tLoss: 3.608815\n",
        "\n",
        "# Test set: Average loss: 3.7550, Accuracy: 1863/10000 (19%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:05:20 2018 Train Epoch: 24 [0/100000 (0%)]\tLoss: 3.453706\n",
        "# Sun Oct 28 11:06:38 2018 Train Epoch: 24 [25600/100000 (26%)]\tLoss: 3.505566\n",
        "# Sun Oct 28 11:07:55 2018 Train Epoch: 24 [51200/100000 (51%)]\tLoss: 3.482967\n",
        "# Sun Oct 28 11:09:13 2018 Train Epoch: 24 [76800/100000 (77%)]\tLoss: 3.326702\n",
        "\n",
        "# Test set: Average loss: 3.7953, Accuracy: 1805/10000 (18%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:10:58 2018 Train Epoch: 25 [0/100000 (0%)]\tLoss: 3.444536\n",
        "# Sun Oct 28 11:12:16 2018 Train Epoch: 25 [25600/100000 (26%)]\tLoss: 3.589061\n",
        "# Sun Oct 28 11:13:34 2018 Train Epoch: 25 [51200/100000 (51%)]\tLoss: 3.764149\n",
        "# Sun Oct 28 11:14:53 2018 Train Epoch: 25 [76800/100000 (77%)]\tLoss: 3.340777\n",
        "\n",
        "# Test set: Average loss: 3.7338, Accuracy: 1929/10000 (19%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:16:37 2018 Train Epoch: 26 [0/100000 (0%)]\tLoss: 3.284796\n",
        "# Sun Oct 28 11:17:55 2018 Train Epoch: 26 [25600/100000 (26%)]\tLoss: 3.712459\n",
        "# Sun Oct 28 11:19:14 2018 Train Epoch: 26 [51200/100000 (51%)]\tLoss: 3.404422\n",
        "# Sun Oct 28 11:20:32 2018 Train Epoch: 26 [76800/100000 (77%)]\tLoss: 3.478382\n",
        "\n",
        "# Test set: Average loss: 3.7125, Accuracy: 1929/10000 (19%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:22:18 2018 Train Epoch: 27 [0/100000 (0%)]\tLoss: 3.401621\n",
        "# Sun Oct 28 11:23:35 2018 Train Epoch: 27 [25600/100000 (26%)]\tLoss: 3.313875\n",
        "# Sun Oct 28 11:24:54 2018 Train Epoch: 27 [51200/100000 (51%)]\tLoss: 3.204078\n",
        "# Sun Oct 28 11:26:11 2018 Train Epoch: 27 [76800/100000 (77%)]\tLoss: 3.608027\n",
        "\n",
        "# Test set: Average loss: 3.6930, Accuracy: 1946/10000 (19%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:27:57 2018 Train Epoch: 28 [0/100000 (0%)]\tLoss: 3.495766\n",
        "# Sun Oct 28 11:29:14 2018 Train Epoch: 28 [25600/100000 (26%)]\tLoss: 3.378552\n",
        "# Sun Oct 28 11:30:30 2018 Train Epoch: 28 [51200/100000 (51%)]\tLoss: 3.326232\n",
        "# Sun Oct 28 11:31:47 2018 Train Epoch: 28 [76800/100000 (77%)]\tLoss: 3.223704\n",
        "\n",
        "# Test set: Average loss: 3.7297, Accuracy: 1926/10000 (19%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:33:30 2018 Train Epoch: 29 [0/100000 (0%)]\tLoss: 3.285259\n",
        "# Sun Oct 28 11:34:47 2018 Train Epoch: 29 [25600/100000 (26%)]\tLoss: 3.501091\n",
        "# Sun Oct 28 11:36:03 2018 Train Epoch: 29 [51200/100000 (51%)]\tLoss: 3.288198\n",
        "# Sun Oct 28 11:37:20 2018 Train Epoch: 29 [76800/100000 (77%)]\tLoss: 3.388712\n",
        "\n",
        "# Test set: Average loss: 3.6769, Accuracy: 1988/10000 (20%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:39:04 2018 Train Epoch: 30 [0/100000 (0%)]\tLoss: 3.350830\n",
        "# Sun Oct 28 11:40:19 2018 Train Epoch: 30 [25600/100000 (26%)]\tLoss: 3.223070\n",
        "# Sun Oct 28 11:41:34 2018 Train Epoch: 30 [51200/100000 (51%)]\tLoss: 3.348273\n",
        "# Sun Oct 28 11:42:50 2018 Train Epoch: 30 [76800/100000 (77%)]\tLoss: 3.370072\n",
        "\n",
        "# Test set: Average loss: 3.6055, Accuracy: 2115/10000 (21%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([21.1500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/030.pt\n",
        "\n",
        "# Sun Oct 28 11:44:32 2018 Train Epoch: 31 [0/100000 (0%)]\tLoss: 3.272603\n",
        "# Sun Oct 28 11:45:46 2018 Train Epoch: 31 [25600/100000 (26%)]\tLoss: 3.327802\n",
        "# Sun Oct 28 11:47:00 2018 Train Epoch: 31 [51200/100000 (51%)]\tLoss: 3.461334\n",
        "# Sun Oct 28 11:48:16 2018 Train Epoch: 31 [76800/100000 (77%)]\tLoss: 3.448921\n",
        "\n",
        "# Test set: Average loss: 3.6508, Accuracy: 2030/10000 (20%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:49:57 2018 Train Epoch: 32 [0/100000 (0%)]\tLoss: 3.450518\n",
        "# Sun Oct 28 11:51:12 2018 Train Epoch: 32 [25600/100000 (26%)]\tLoss: 3.375097\n",
        "# Sun Oct 28 11:52:27 2018 Train Epoch: 32 [51200/100000 (51%)]\tLoss: 3.344909\n",
        "# Sun Oct 28 11:53:43 2018 Train Epoch: 32 [76800/100000 (77%)]\tLoss: 3.222729\n",
        "\n",
        "# Test set: Average loss: 3.7365, Accuracy: 1940/10000 (19%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 11:55:24 2018 Train Epoch: 33 [0/100000 (0%)]\tLoss: 3.272067\n",
        "# Sun Oct 28 11:56:39 2018 Train Epoch: 33 [25600/100000 (26%)]\tLoss: 3.357075\n",
        "# Sun Oct 28 11:57:55 2018 Train Epoch: 33 [51200/100000 (51%)]\tLoss: 3.253914\n",
        "# Sun Oct 28 11:59:12 2018 Train Epoch: 33 [76800/100000 (77%)]\tLoss: 3.364290\n",
        "\n",
        "# Test set: Average loss: 3.6979, Accuracy: 2004/10000 (20%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 12:00:54 2018 Train Epoch: 34 [0/100000 (0%)]\tLoss: 3.372559\n",
        "# Sun Oct 28 12:02:11 2018 Train Epoch: 34 [25600/100000 (26%)]\tLoss: 3.300415\n",
        "# Sun Oct 28 12:03:27 2018 Train Epoch: 34 [51200/100000 (51%)]\tLoss: 3.194876\n",
        "# Sun Oct 28 12:04:44 2018 Train Epoch: 34 [76800/100000 (77%)]\tLoss: 3.085931\n",
        "\n",
        "# Test set: Average loss: 3.5878, Accuracy: 2190/10000 (22%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([21.9000])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/034.pt\n",
        "\n",
        "# Sun Oct 28 12:06:27 2018 Train Epoch: 35 [0/100000 (0%)]\tLoss: 3.056293\n",
        "# Sun Oct 28 12:07:43 2018 Train Epoch: 35 [25600/100000 (26%)]\tLoss: 3.295201\n",
        "# Sun Oct 28 12:09:00 2018 Train Epoch: 35 [51200/100000 (51%)]\tLoss: 3.520723\n",
        "# Sun Oct 28 12:10:18 2018 Train Epoch: 35 [76800/100000 (77%)]\tLoss: 3.167100\n",
        "\n",
        "# Test set: Average loss: 3.6117, Accuracy: 2119/10000 (21%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 12:12:03 2018 Train Epoch: 36 [0/100000 (0%)]\tLoss: 3.337354\n",
        "# Sun Oct 28 12:13:20 2018 Train Epoch: 36 [25600/100000 (26%)]\tLoss: 3.047043\n",
        "# Sun Oct 28 12:14:39 2018 Train Epoch: 36 [51200/100000 (51%)]\tLoss: 3.267047\n",
        "# Sun Oct 28 12:15:58 2018 Train Epoch: 36 [76800/100000 (77%)]\tLoss: 3.253247\n",
        "\n",
        "# Test set: Average loss: 3.6029, Accuracy: 2162/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 12:17:42 2018 Train Epoch: 37 [0/100000 (0%)]\tLoss: 3.450312\n",
        "# Sun Oct 28 12:18:59 2018 Train Epoch: 37 [25600/100000 (26%)]\tLoss: 3.194050\n",
        "# Sun Oct 28 12:20:18 2018 Train Epoch: 37 [51200/100000 (51%)]\tLoss: 3.136184\n",
        "# Sun Oct 28 12:21:37 2018 Train Epoch: 37 [76800/100000 (77%)]\tLoss: 3.510668\n",
        "\n",
        "# Test set: Average loss: 3.6666, Accuracy: 2065/10000 (21%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 12:23:21 2018 Train Epoch: 38 [0/100000 (0%)]\tLoss: 3.274927\n",
        "# Sun Oct 28 12:24:39 2018 Train Epoch: 38 [25600/100000 (26%)]\tLoss: 3.211324\n",
        "# Sun Oct 28 12:25:56 2018 Train Epoch: 38 [51200/100000 (51%)]\tLoss: 3.180830\n",
        "# Sun Oct 28 12:27:14 2018 Train Epoch: 38 [76800/100000 (77%)]\tLoss: 3.235213\n",
        "\n",
        "# Test set: Average loss: 3.5514, Accuracy: 2193/10000 (22%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([21.9300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/038.pt\n",
        "\n",
        "# Sun Oct 28 12:28:58 2018 Train Epoch: 39 [0/100000 (0%)]\tLoss: 3.192174\n",
        "# Sun Oct 28 12:30:15 2018 Train Epoch: 39 [25600/100000 (26%)]\tLoss: 3.281785\n",
        "# Sun Oct 28 12:31:32 2018 Train Epoch: 39 [51200/100000 (51%)]\tLoss: 3.069725\n",
        "# Sun Oct 28 12:32:49 2018 Train Epoch: 39 [76800/100000 (77%)]\tLoss: 3.281837\n",
        "\n",
        "# Test set: Average loss: 3.6192, Accuracy: 2137/10000 (21%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 12:34:32 2018 Train Epoch: 40 [0/100000 (0%)]\tLoss: 3.287760\n",
        "# Sun Oct 28 12:35:50 2018 Train Epoch: 40 [25600/100000 (26%)]\tLoss: 3.434256\n",
        "# Sun Oct 28 12:37:08 2018 Train Epoch: 40 [51200/100000 (51%)]\tLoss: 3.140772\n",
        "# Sun Oct 28 12:38:25 2018 Train Epoch: 40 [76800/100000 (77%)]\tLoss: 3.244732\n",
        "\n",
        "# Test set: Average loss: 3.5088, Accuracy: 2255/10000 (23%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([22.5500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/040.pt\n",
        "\n",
        "# Sun Oct 28 12:40:10 2018 Train Epoch: 41 [0/100000 (0%)]\tLoss: 3.081690\n",
        "# Sun Oct 28 12:41:27 2018 Train Epoch: 41 [25600/100000 (26%)]\tLoss: 3.204222\n",
        "# Sun Oct 28 12:42:43 2018 Train Epoch: 41 [51200/100000 (51%)]\tLoss: 3.212636\n",
        "# Sun Oct 28 12:44:00 2018 Train Epoch: 41 [76800/100000 (77%)]\tLoss: 3.310124\n",
        "\n",
        "# Test set: Average loss: 3.5461, Accuracy: 2185/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 12:45:45 2018 Train Epoch: 42 [0/100000 (0%)]\tLoss: 3.148613\n",
        "# Sun Oct 28 12:47:02 2018 Train Epoch: 42 [25600/100000 (26%)]\tLoss: 3.253901\n",
        "# Sun Oct 28 12:48:19 2018 Train Epoch: 42 [51200/100000 (51%)]\tLoss: 3.265591\n",
        "# Sun Oct 28 12:49:36 2018 Train Epoch: 42 [76800/100000 (77%)]\tLoss: 3.135966\n",
        "\n",
        "# Test set: Average loss: 3.5213, Accuracy: 2283/10000 (23%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([22.8300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/042.pt\n",
        "\n",
        "# Sun Oct 28 12:51:22 2018 Train Epoch: 43 [0/100000 (0%)]\tLoss: 3.299712\n",
        "# Sun Oct 28 12:52:39 2018 Train Epoch: 43 [25600/100000 (26%)]\tLoss: 3.218050\n",
        "# Sun Oct 28 12:53:57 2018 Train Epoch: 43 [51200/100000 (51%)]\tLoss: 3.139254\n",
        "# Sun Oct 28 12:55:15 2018 Train Epoch: 43 [76800/100000 (77%)]\tLoss: 3.301295\n",
        "\n",
        "# Test set: Average loss: 3.4304, Accuracy: 2454/10000 (25%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([24.5400])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/043.pt\n",
        "\n",
        "# Sun Oct 28 12:57:00 2018 Train Epoch: 44 [0/100000 (0%)]\tLoss: 3.272900\n",
        "# Sun Oct 28 12:58:18 2018 Train Epoch: 44 [25600/100000 (26%)]\tLoss: 3.278439\n",
        "# Sun Oct 28 12:59:36 2018 Train Epoch: 44 [51200/100000 (51%)]\tLoss: 3.324661\n",
        "# Sun Oct 28 13:00:54 2018 Train Epoch: 44 [76800/100000 (77%)]\tLoss: 3.161658\n",
        "\n",
        "# Test set: Average loss: 3.5846, Accuracy: 2173/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:02:37 2018 Train Epoch: 45 [0/100000 (0%)]\tLoss: 3.383158\n",
        "# Sun Oct 28 13:03:54 2018 Train Epoch: 45 [25600/100000 (26%)]\tLoss: 3.036631\n",
        "# Sun Oct 28 13:05:13 2018 Train Epoch: 45 [51200/100000 (51%)]\tLoss: 3.357647\n",
        "# Sun Oct 28 13:06:31 2018 Train Epoch: 45 [76800/100000 (77%)]\tLoss: 3.284893\n",
        "\n",
        "# Test set: Average loss: 3.5489, Accuracy: 2229/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:08:15 2018 Train Epoch: 46 [0/100000 (0%)]\tLoss: 3.112463\n",
        "# Sun Oct 28 13:09:32 2018 Train Epoch: 46 [25600/100000 (26%)]\tLoss: 3.264755\n",
        "# Sun Oct 28 13:10:50 2018 Train Epoch: 46 [51200/100000 (51%)]\tLoss: 2.951614\n",
        "# Sun Oct 28 13:12:08 2018 Train Epoch: 46 [76800/100000 (77%)]\tLoss: 3.167840\n",
        "\n",
        "# Test set: Average loss: 3.6168, Accuracy: 2120/10000 (21%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:13:53 2018 Train Epoch: 47 [0/100000 (0%)]\tLoss: 3.305137\n",
        "# Sun Oct 28 13:15:12 2018 Train Epoch: 47 [25600/100000 (26%)]\tLoss: 3.191120\n",
        "# Sun Oct 28 13:16:30 2018 Train Epoch: 47 [51200/100000 (51%)]\tLoss: 3.515236\n",
        "# Sun Oct 28 13:17:49 2018 Train Epoch: 47 [76800/100000 (77%)]\tLoss: 3.231783\n",
        "\n",
        "# Test set: Average loss: 3.6425, Accuracy: 2079/10000 (21%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:19:33 2018 Train Epoch: 48 [0/100000 (0%)]\tLoss: 3.064181\n",
        "# Sun Oct 28 13:20:51 2018 Train Epoch: 48 [25600/100000 (26%)]\tLoss: 3.139247\n",
        "# Sun Oct 28 13:22:09 2018 Train Epoch: 48 [51200/100000 (51%)]\tLoss: 3.179358\n",
        "# Sun Oct 28 13:23:27 2018 Train Epoch: 48 [76800/100000 (77%)]\tLoss: 3.026627\n",
        "\n",
        "# Test set: Average loss: 3.5804, Accuracy: 2196/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:25:11 2018 Train Epoch: 49 [0/100000 (0%)]\tLoss: 3.104980\n",
        "# Sun Oct 28 13:26:29 2018 Train Epoch: 49 [25600/100000 (26%)]\tLoss: 3.090651\n",
        "# Sun Oct 28 13:27:48 2018 Train Epoch: 49 [51200/100000 (51%)]\tLoss: 3.302260\n",
        "# Sun Oct 28 13:29:05 2018 Train Epoch: 49 [76800/100000 (77%)]\tLoss: 3.102632\n",
        "\n",
        "# Test set: Average loss: 3.5799, Accuracy: 2202/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:30:50 2018 Train Epoch: 50 [0/100000 (0%)]\tLoss: 3.430751\n",
        "# Sun Oct 28 13:32:08 2018 Train Epoch: 50 [25600/100000 (26%)]\tLoss: 3.164438\n",
        "# Sun Oct 28 13:33:26 2018 Train Epoch: 50 [51200/100000 (51%)]\tLoss: 3.243676\n",
        "# Sun Oct 28 13:34:44 2018 Train Epoch: 50 [76800/100000 (77%)]\tLoss: 3.012201\n",
        "\n",
        "# Test set: Average loss: 3.5967, Accuracy: 2220/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:36:28 2018 Train Epoch: 51 [0/100000 (0%)]\tLoss: 3.110829\n",
        "# Sun Oct 28 13:37:46 2018 Train Epoch: 51 [25600/100000 (26%)]\tLoss: 3.315958\n",
        "# Sun Oct 28 13:39:04 2018 Train Epoch: 51 [51200/100000 (51%)]\tLoss: 3.164300\n",
        "# Sun Oct 28 13:40:22 2018 Train Epoch: 51 [76800/100000 (77%)]\tLoss: 3.289052\n",
        "\n",
        "# Test set: Average loss: 3.5040, Accuracy: 2318/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:42:08 2018 Train Epoch: 52 [0/100000 (0%)]\tLoss: 2.924158\n",
        "# Sun Oct 28 13:43:26 2018 Train Epoch: 52 [25600/100000 (26%)]\tLoss: 3.098467\n",
        "# Sun Oct 28 13:44:44 2018 Train Epoch: 52 [51200/100000 (51%)]\tLoss: 3.112684\n",
        "# Sun Oct 28 13:46:02 2018 Train Epoch: 52 [76800/100000 (77%)]\tLoss: 3.131296\n",
        "\n",
        "# Test set: Average loss: 3.5859, Accuracy: 2158/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:47:47 2018 Train Epoch: 53 [0/100000 (0%)]\tLoss: 3.233947\n",
        "# Sun Oct 28 13:49:06 2018 Train Epoch: 53 [25600/100000 (26%)]\tLoss: 2.970840\n",
        "# Sun Oct 28 13:50:24 2018 Train Epoch: 53 [51200/100000 (51%)]\tLoss: 3.136236\n",
        "# Sun Oct 28 13:51:43 2018 Train Epoch: 53 [76800/100000 (77%)]\tLoss: 3.015904\n",
        "\n",
        "# Test set: Average loss: 3.6291, Accuracy: 2141/10000 (21%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:53:28 2018 Train Epoch: 54 [0/100000 (0%)]\tLoss: 3.052514\n",
        "# Sun Oct 28 13:54:46 2018 Train Epoch: 54 [25600/100000 (26%)]\tLoss: 3.174851\n",
        "# Sun Oct 28 13:56:04 2018 Train Epoch: 54 [51200/100000 (51%)]\tLoss: 3.074706\n",
        "# Sun Oct 28 13:57:22 2018 Train Epoch: 54 [76800/100000 (77%)]\tLoss: 3.247977\n",
        "\n",
        "# Test set: Average loss: 3.4858, Accuracy: 2377/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:59:07 2018 Train Epoch: 55 [0/100000 (0%)]\tLoss: 3.133544\n",
        "# Sun Oct 28 14:00:25 2018 Train Epoch: 55 [25600/100000 (26%)]\tLoss: 3.167764\n",
        "# Sun Oct 28 14:01:43 2018 Train Epoch: 55 [51200/100000 (51%)]\tLoss: 3.221013\n",
        "# Sun Oct 28 14:03:01 2018 Train Epoch: 55 [76800/100000 (77%)]\tLoss: 3.110483\n",
        "\n",
        "# Test set: Average loss: 3.6105, Accuracy: 2186/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:04:46 2018 Train Epoch: 56 [0/100000 (0%)]\tLoss: 3.075941\n",
        "# Sun Oct 28 14:06:05 2018 Train Epoch: 56 [25600/100000 (26%)]\tLoss: 3.125409\n",
        "# Sun Oct 28 14:07:23 2018 Train Epoch: 56 [51200/100000 (51%)]\tLoss: 3.107131\n",
        "# Sun Oct 28 14:08:41 2018 Train Epoch: 56 [76800/100000 (77%)]\tLoss: 3.291129\n",
        "\n",
        "# Test set: Average loss: 3.4433, Accuracy: 2432/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:10:26 2018 Train Epoch: 57 [0/100000 (0%)]\tLoss: 3.122875\n",
        "# Sun Oct 28 14:11:44 2018 Train Epoch: 57 [25600/100000 (26%)]\tLoss: 3.058686\n",
        "# Sun Oct 28 14:13:01 2018 Train Epoch: 57 [51200/100000 (51%)]\tLoss: 3.208047\n",
        "# Sun Oct 28 14:14:19 2018 Train Epoch: 57 [76800/100000 (77%)]\tLoss: 3.445005\n",
        "\n",
        "# Test set: Average loss: 3.5384, Accuracy: 2272/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:16:05 2018 Train Epoch: 58 [0/100000 (0%)]\tLoss: 3.058595\n",
        "# Sun Oct 28 14:17:23 2018 Train Epoch: 58 [25600/100000 (26%)]\tLoss: 2.969599\n",
        "# Sun Oct 28 14:18:42 2018 Train Epoch: 58 [51200/100000 (51%)]\tLoss: 3.011316\n",
        "# Sun Oct 28 14:20:00 2018 Train Epoch: 58 [76800/100000 (77%)]\tLoss: 3.150543\n",
        "\n",
        "# Test set: Average loss: 3.5317, Accuracy: 2315/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:21:45 2018 Train Epoch: 59 [0/100000 (0%)]\tLoss: 3.007098\n",
        "# Sun Oct 28 14:23:04 2018 Train Epoch: 59 [25600/100000 (26%)]\tLoss: 3.125684\n",
        "# Sun Oct 28 14:24:22 2018 Train Epoch: 59 [51200/100000 (51%)]\tLoss: 3.030260\n",
        "# Sun Oct 28 14:25:41 2018 Train Epoch: 59 [76800/100000 (77%)]\tLoss: 2.956497\n",
        "\n",
        "# Test set: Average loss: 3.5077, Accuracy: 2313/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:27:25 2018 Train Epoch: 60 [0/100000 (0%)]\tLoss: 3.197628\n",
        "# Sun Oct 28 14:28:43 2018 Train Epoch: 60 [25600/100000 (26%)]\tLoss: 2.822612\n",
        "# Sun Oct 28 14:30:00 2018 Train Epoch: 60 [51200/100000 (51%)]\tLoss: 3.071411\n",
        "# Sun Oct 28 14:31:18 2018 Train Epoch: 60 [76800/100000 (77%)]\tLoss: 3.034223\n",
        "\n",
        "# Test set: Average loss: 3.4964, Accuracy: 2333/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:33:02 2018 Train Epoch: 61 [0/100000 (0%)]\tLoss: 3.120903\n",
        "# Sun Oct 28 14:34:20 2018 Train Epoch: 61 [25600/100000 (26%)]\tLoss: 3.145950\n",
        "# Sun Oct 28 14:35:38 2018 Train Epoch: 61 [51200/100000 (51%)]\tLoss: 3.050297\n",
        "# Sun Oct 28 14:36:56 2018 Train Epoch: 61 [76800/100000 (77%)]\tLoss: 3.170979\n",
        "\n",
        "# Test set: Average loss: 3.5478, Accuracy: 2209/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:38:43 2018 Train Epoch: 62 [0/100000 (0%)]\tLoss: 3.103210\n",
        "# Sun Oct 28 14:40:01 2018 Train Epoch: 62 [25600/100000 (26%)]\tLoss: 2.994385\n",
        "# Sun Oct 28 14:41:19 2018 Train Epoch: 62 [51200/100000 (51%)]\tLoss: 3.177859\n",
        "# Sun Oct 28 14:42:37 2018 Train Epoch: 62 [76800/100000 (77%)]\tLoss: 3.062442\n",
        "\n",
        "# Test set: Average loss: 3.3962, Accuracy: 2440/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:44:22 2018 Train Epoch: 63 [0/100000 (0%)]\tLoss: 2.891522\n",
        "# Sun Oct 28 14:45:38 2018 Train Epoch: 63 [25600/100000 (26%)]\tLoss: 3.057771\n",
        "# Sun Oct 28 14:46:55 2018 Train Epoch: 63 [51200/100000 (51%)]\tLoss: 3.160944\n",
        "# Sun Oct 28 14:48:13 2018 Train Epoch: 63 [76800/100000 (77%)]\tLoss: 3.203206\n",
        "\n",
        "# Test set: Average loss: 3.5502, Accuracy: 2288/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:49:57 2018 Train Epoch: 64 [0/100000 (0%)]\tLoss: 2.957571\n",
        "# Sun Oct 28 14:51:14 2018 Train Epoch: 64 [25600/100000 (26%)]\tLoss: 3.036634\n",
        "# Sun Oct 28 14:52:32 2018 Train Epoch: 64 [51200/100000 (51%)]\tLoss: 3.077394\n",
        "# Sun Oct 28 14:53:48 2018 Train Epoch: 64 [76800/100000 (77%)]\tLoss: 3.179166\n",
        "\n",
        "# Test set: Average loss: 3.5172, Accuracy: 2324/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 14:55:33 2018 Train Epoch: 65 [0/100000 (0%)]\tLoss: 2.965264\n",
        "# Sun Oct 28 14:56:49 2018 Train Epoch: 65 [25600/100000 (26%)]\tLoss: 2.903270\n",
        "# Sun Oct 28 14:58:06 2018 Train Epoch: 65 [51200/100000 (51%)]\tLoss: 2.979055\n",
        "# Sun Oct 28 14:59:23 2018 Train Epoch: 65 [76800/100000 (77%)]\tLoss: 2.982553\n",
        "\n",
        "# Test set: Average loss: 3.4409, Accuracy: 2443/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:01:08 2018 Train Epoch: 66 [0/100000 (0%)]\tLoss: 3.030339\n",
        "# Sun Oct 28 15:02:25 2018 Train Epoch: 66 [25600/100000 (26%)]\tLoss: 3.229693\n",
        "# Sun Oct 28 15:03:43 2018 Train Epoch: 66 [51200/100000 (51%)]\tLoss: 3.057702\n",
        "# Sun Oct 28 15:05:02 2018 Train Epoch: 66 [76800/100000 (77%)]\tLoss: 3.075365\n",
        "\n",
        "# Test set: Average loss: 3.5539, Accuracy: 2241/10000 (22%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:06:46 2018 Train Epoch: 67 [0/100000 (0%)]\tLoss: 2.988237\n",
        "# Sun Oct 28 15:08:03 2018 Train Epoch: 67 [25600/100000 (26%)]\tLoss: 3.043791\n",
        "# Sun Oct 28 15:09:20 2018 Train Epoch: 67 [51200/100000 (51%)]\tLoss: 3.044308\n",
        "# Sun Oct 28 15:10:39 2018 Train Epoch: 67 [76800/100000 (77%)]\tLoss: 3.198491\n",
        "\n",
        "# Test set: Average loss: 3.3854, Accuracy: 2515/10000 (25%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([25.1500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/067.pt\n",
        "\n",
        "# Sun Oct 28 15:12:24 2018 Train Epoch: 68 [0/100000 (0%)]\tLoss: 3.065977\n",
        "# Sun Oct 28 15:13:42 2018 Train Epoch: 68 [25600/100000 (26%)]\tLoss: 3.025027\n",
        "# Sun Oct 28 15:14:59 2018 Train Epoch: 68 [51200/100000 (51%)]\tLoss: 3.105316\n",
        "# Sun Oct 28 15:16:18 2018 Train Epoch: 68 [76800/100000 (77%)]\tLoss: 3.358328\n",
        "\n",
        "# Test set: Average loss: 3.5144, Accuracy: 2305/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:18:01 2018 Train Epoch: 69 [0/100000 (0%)]\tLoss: 2.941035\n",
        "# Sun Oct 28 15:19:19 2018 Train Epoch: 69 [25600/100000 (26%)]\tLoss: 2.918886\n",
        "# Sun Oct 28 15:20:36 2018 Train Epoch: 69 [51200/100000 (51%)]\tLoss: 2.980460\n",
        "# Sun Oct 28 15:21:53 2018 Train Epoch: 69 [76800/100000 (77%)]\tLoss: 2.836328\n",
        "\n",
        "# Test set: Average loss: 3.4858, Accuracy: 2357/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:23:39 2018 Train Epoch: 70 [0/100000 (0%)]\tLoss: 3.032157\n",
        "# Sun Oct 28 15:24:56 2018 Train Epoch: 70 [25600/100000 (26%)]\tLoss: 3.184124\n",
        "# Sun Oct 28 15:26:15 2018 Train Epoch: 70 [51200/100000 (51%)]\tLoss: 2.943442\n",
        "# Sun Oct 28 15:27:32 2018 Train Epoch: 70 [76800/100000 (77%)]\tLoss: 3.223605\n",
        "\n",
        "# Test set: Average loss: 3.4215, Accuracy: 2460/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:29:16 2018 Train Epoch: 71 [0/100000 (0%)]\tLoss: 2.867127\n",
        "# Sun Oct 28 15:30:33 2018 Train Epoch: 71 [25600/100000 (26%)]\tLoss: 2.980956\n",
        "# Sun Oct 28 15:31:51 2018 Train Epoch: 71 [51200/100000 (51%)]\tLoss: 3.111328\n",
        "# Sun Oct 28 15:33:08 2018 Train Epoch: 71 [76800/100000 (77%)]\tLoss: 3.031147\n",
        "\n",
        "# Test set: Average loss: 3.3946, Accuracy: 2477/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:34:52 2018 Train Epoch: 72 [0/100000 (0%)]\tLoss: 2.784684\n",
        "# Sun Oct 28 15:36:10 2018 Train Epoch: 72 [25600/100000 (26%)]\tLoss: 3.044655\n",
        "# Sun Oct 28 15:37:27 2018 Train Epoch: 72 [51200/100000 (51%)]\tLoss: 2.942614\n",
        "# Sun Oct 28 15:38:44 2018 Train Epoch: 72 [76800/100000 (77%)]\tLoss: 3.215572\n",
        "\n",
        "# Test set: Average loss: 3.4093, Accuracy: 2472/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:40:28 2018 Train Epoch: 73 [0/100000 (0%)]\tLoss: 2.835059\n",
        "# Sun Oct 28 15:41:46 2018 Train Epoch: 73 [25600/100000 (26%)]\tLoss: 2.907418\n",
        "# Sun Oct 28 15:43:03 2018 Train Epoch: 73 [51200/100000 (51%)]\tLoss: 3.246102\n",
        "# Sun Oct 28 15:44:20 2018 Train Epoch: 73 [76800/100000 (77%)]\tLoss: 3.218823\n",
        "\n",
        "# Test set: Average loss: 3.4586, Accuracy: 2343/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:46:05 2018 Train Epoch: 74 [0/100000 (0%)]\tLoss: 2.906260\n",
        "# Sun Oct 28 15:47:22 2018 Train Epoch: 74 [25600/100000 (26%)]\tLoss: 2.994660\n",
        "# Sun Oct 28 15:48:39 2018 Train Epoch: 74 [51200/100000 (51%)]\tLoss: 3.093323\n",
        "# Sun Oct 28 15:49:57 2018 Train Epoch: 74 [76800/100000 (77%)]\tLoss: 3.066345\n",
        "\n",
        "# Test set: Average loss: 3.4381, Accuracy: 2458/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:51:40 2018 Train Epoch: 75 [0/100000 (0%)]\tLoss: 3.124469\n",
        "# Sun Oct 28 15:52:58 2018 Train Epoch: 75 [25600/100000 (26%)]\tLoss: 3.255293\n",
        "# Sun Oct 28 15:54:15 2018 Train Epoch: 75 [51200/100000 (51%)]\tLoss: 3.255180\n",
        "# Sun Oct 28 15:55:31 2018 Train Epoch: 75 [76800/100000 (77%)]\tLoss: 2.925440\n",
        "\n",
        "# Test set: Average loss: 3.4226, Accuracy: 2504/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:57:16 2018 Train Epoch: 76 [0/100000 (0%)]\tLoss: 3.186196\n",
        "# Sun Oct 28 15:58:33 2018 Train Epoch: 76 [25600/100000 (26%)]\tLoss: 2.996958\n",
        "# Sun Oct 28 15:59:49 2018 Train Epoch: 76 [51200/100000 (51%)]\tLoss: 2.931269\n",
        "# Sun Oct 28 16:01:06 2018 Train Epoch: 76 [76800/100000 (77%)]\tLoss: 2.814854\n",
        "\n",
        "# Test set: Average loss: 3.4417, Accuracy: 2471/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:02:49 2018 Train Epoch: 77 [0/100000 (0%)]\tLoss: 3.007079\n",
        "# Sun Oct 28 16:04:06 2018 Train Epoch: 77 [25600/100000 (26%)]\tLoss: 3.233469\n",
        "# Sun Oct 28 16:05:23 2018 Train Epoch: 77 [51200/100000 (51%)]\tLoss: 3.175012\n",
        "# Sun Oct 28 16:06:39 2018 Train Epoch: 77 [76800/100000 (77%)]\tLoss: 3.155249\n",
        "\n",
        "# Test set: Average loss: 3.4889, Accuracy: 2391/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:08:21 2018 Train Epoch: 78 [0/100000 (0%)]\tLoss: 2.932850\n",
        "# Sun Oct 28 16:09:37 2018 Train Epoch: 78 [25600/100000 (26%)]\tLoss: 3.237348\n",
        "# Sun Oct 28 16:10:54 2018 Train Epoch: 78 [51200/100000 (51%)]\tLoss: 3.075943\n",
        "# Sun Oct 28 16:12:11 2018 Train Epoch: 78 [76800/100000 (77%)]\tLoss: 3.067964\n",
        "\n",
        "# Test set: Average loss: 3.5283, Accuracy: 2309/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:13:56 2018 Train Epoch: 79 [0/100000 (0%)]\tLoss: 3.088405\n",
        "# Sun Oct 28 16:15:13 2018 Train Epoch: 79 [25600/100000 (26%)]\tLoss: 3.158370\n",
        "# Sun Oct 28 16:16:31 2018 Train Epoch: 79 [51200/100000 (51%)]\tLoss: 3.138738\n",
        "# Sun Oct 28 16:17:49 2018 Train Epoch: 79 [76800/100000 (77%)]\tLoss: 3.231729\n",
        "\n",
        "# Test set: Average loss: 3.4141, Accuracy: 2459/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:19:34 2018 Train Epoch: 80 [0/100000 (0%)]\tLoss: 2.813667\n",
        "# Sun Oct 28 16:20:51 2018 Train Epoch: 80 [25600/100000 (26%)]\tLoss: 2.846924\n",
        "# Sun Oct 28 16:22:09 2018 Train Epoch: 80 [51200/100000 (51%)]\tLoss: 2.999192\n",
        "# Sun Oct 28 16:23:27 2018 Train Epoch: 80 [76800/100000 (77%)]\tLoss: 3.210023\n",
        "\n",
        "# Test set: Average loss: 3.4429, Accuracy: 2452/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:25:13 2018 Train Epoch: 81 [0/100000 (0%)]\tLoss: 2.975432\n",
        "# Sun Oct 28 16:26:31 2018 Train Epoch: 81 [25600/100000 (26%)]\tLoss: 3.119710\n",
        "# Sun Oct 28 16:27:50 2018 Train Epoch: 81 [51200/100000 (51%)]\tLoss: 3.085965\n",
        "# Sun Oct 28 16:29:07 2018 Train Epoch: 81 [76800/100000 (77%)]\tLoss: 2.974537\n",
        "\n",
        "# Test set: Average loss: 3.5769, Accuracy: 2254/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:30:53 2018 Train Epoch: 82 [0/100000 (0%)]\tLoss: 2.998208\n",
        "# Sun Oct 28 16:32:10 2018 Train Epoch: 82 [25600/100000 (26%)]\tLoss: 3.076187\n",
        "# Sun Oct 28 16:33:29 2018 Train Epoch: 82 [51200/100000 (51%)]\tLoss: 3.099533\n",
        "# Sun Oct 28 16:34:46 2018 Train Epoch: 82 [76800/100000 (77%)]\tLoss: 2.986466\n",
        "\n",
        "# Test set: Average loss: 3.3870, Accuracy: 2581/10000 (26%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([25.8100])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/082.pt\n",
        "\n",
        "# Sun Oct 28 16:36:31 2018 Train Epoch: 83 [0/100000 (0%)]\tLoss: 2.877162\n",
        "# Sun Oct 28 16:37:49 2018 Train Epoch: 83 [25600/100000 (26%)]\tLoss: 3.159907\n",
        "# Sun Oct 28 16:39:06 2018 Train Epoch: 83 [51200/100000 (51%)]\tLoss: 2.930652\n",
        "# Sun Oct 28 16:40:23 2018 Train Epoch: 83 [76800/100000 (77%)]\tLoss: 3.117169\n",
        "\n",
        "# Test set: Average loss: 3.5229, Accuracy: 2306/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:42:06 2018 Train Epoch: 84 [0/100000 (0%)]\tLoss: 3.030325\n",
        "# Sun Oct 28 16:43:25 2018 Train Epoch: 84 [25600/100000 (26%)]\tLoss: 3.129933\n",
        "# Sun Oct 28 16:44:43 2018 Train Epoch: 84 [51200/100000 (51%)]\tLoss: 2.968585\n",
        "# Sun Oct 28 16:46:00 2018 Train Epoch: 84 [76800/100000 (77%)]\tLoss: 3.091070\n",
        "\n",
        "# Test set: Average loss: 3.3456, Accuracy: 2572/10000 (26%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:47:45 2018 Train Epoch: 85 [0/100000 (0%)]\tLoss: 3.000796\n",
        "# Sun Oct 28 16:49:02 2018 Train Epoch: 85 [25600/100000 (26%)]\tLoss: 3.068975\n",
        "# Sun Oct 28 16:50:19 2018 Train Epoch: 85 [51200/100000 (51%)]\tLoss: 2.978965\n",
        "# Sun Oct 28 16:51:37 2018 Train Epoch: 85 [76800/100000 (77%)]\tLoss: 2.993422\n",
        "\n",
        "# Test set: Average loss: 3.3803, Accuracy: 2540/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:53:23 2018 Train Epoch: 86 [0/100000 (0%)]\tLoss: 2.787563\n",
        "# Sun Oct 28 16:54:42 2018 Train Epoch: 86 [25600/100000 (26%)]\tLoss: 3.179019\n",
        "# Sun Oct 28 16:56:00 2018 Train Epoch: 86 [51200/100000 (51%)]\tLoss: 2.852439\n",
        "# Sun Oct 28 16:57:18 2018 Train Epoch: 86 [76800/100000 (77%)]\tLoss: 2.885056\n",
        "\n",
        "# Test set: Average loss: 3.5542, Accuracy: 2357/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:59:03 2018 Train Epoch: 87 [0/100000 (0%)]\tLoss: 2.825193\n",
        "# Sun Oct 28 17:00:20 2018 Train Epoch: 87 [25600/100000 (26%)]\tLoss: 2.967957\n",
        "# Sun Oct 28 17:01:38 2018 Train Epoch: 87 [51200/100000 (51%)]\tLoss: 3.128246\n",
        "# Sun Oct 28 17:02:57 2018 Train Epoch: 87 [76800/100000 (77%)]\tLoss: 2.804996\n",
        "\n",
        "# Test set: Average loss: 3.3564, Accuracy: 2566/10000 (26%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:04:43 2018 Train Epoch: 88 [0/100000 (0%)]\tLoss: 2.991108\n",
        "# Sun Oct 28 17:06:00 2018 Train Epoch: 88 [25600/100000 (26%)]\tLoss: 2.919181\n",
        "# Sun Oct 28 17:07:18 2018 Train Epoch: 88 [51200/100000 (51%)]\tLoss: 2.942463\n",
        "# Sun Oct 28 17:08:36 2018 Train Epoch: 88 [76800/100000 (77%)]\tLoss: 3.179644\n",
        "\n",
        "# Test set: Average loss: 3.4539, Accuracy: 2470/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:10:22 2018 Train Epoch: 89 [0/100000 (0%)]\tLoss: 2.941020\n",
        "# Sun Oct 28 17:11:39 2018 Train Epoch: 89 [25600/100000 (26%)]\tLoss: 3.246779\n",
        "# Sun Oct 28 17:12:57 2018 Train Epoch: 89 [51200/100000 (51%)]\tLoss: 2.996041\n",
        "# Sun Oct 28 17:14:17 2018 Train Epoch: 89 [76800/100000 (77%)]\tLoss: 3.154358\n",
        "\n",
        "# Test set: Average loss: 3.4270, Accuracy: 2486/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:16:02 2018 Train Epoch: 90 [0/100000 (0%)]\tLoss: 3.110028\n",
        "# Sun Oct 28 17:17:20 2018 Train Epoch: 90 [25600/100000 (26%)]\tLoss: 2.926164\n",
        "# Sun Oct 28 17:18:37 2018 Train Epoch: 90 [51200/100000 (51%)]\tLoss: 3.118001\n",
        "# Sun Oct 28 17:19:55 2018 Train Epoch: 90 [76800/100000 (77%)]\tLoss: 3.096732\n",
        "\n",
        "# Test set: Average loss: 3.4217, Accuracy: 2467/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:21:41 2018 Train Epoch: 91 [0/100000 (0%)]\tLoss: 2.907834\n",
        "# Sun Oct 28 17:22:59 2018 Train Epoch: 91 [25600/100000 (26%)]\tLoss: 3.053635\n",
        "# Sun Oct 28 17:24:17 2018 Train Epoch: 91 [51200/100000 (51%)]\tLoss: 3.151702\n",
        "# Sun Oct 28 17:25:36 2018 Train Epoch: 91 [76800/100000 (77%)]\tLoss: 2.969858\n",
        "\n",
        "# Test set: Average loss: 3.3989, Accuracy: 2500/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:27:20 2018 Train Epoch: 92 [0/100000 (0%)]\tLoss: 2.797891\n",
        "# Sun Oct 28 17:28:38 2018 Train Epoch: 92 [25600/100000 (26%)]\tLoss: 3.061436\n",
        "# Sun Oct 28 17:29:57 2018 Train Epoch: 92 [51200/100000 (51%)]\tLoss: 3.000379\n",
        "# Sun Oct 28 17:31:15 2018 Train Epoch: 92 [76800/100000 (77%)]\tLoss: 2.999899\n",
        "\n",
        "# Test set: Average loss: 3.3899, Accuracy: 2528/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:33:00 2018 Train Epoch: 93 [0/100000 (0%)]\tLoss: 2.948611\n",
        "# Sun Oct 28 17:34:18 2018 Train Epoch: 93 [25600/100000 (26%)]\tLoss: 3.144225\n",
        "# Sun Oct 28 17:35:36 2018 Train Epoch: 93 [51200/100000 (51%)]\tLoss: 2.998856\n",
        "# Sun Oct 28 17:36:55 2018 Train Epoch: 93 [76800/100000 (77%)]\tLoss: 3.077009\n",
        "\n",
        "# Test set: Average loss: 3.3744, Accuracy: 2597/10000 (26%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([25.9700])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/093.pt\n",
        "\n",
        "# Sun Oct 28 17:38:40 2018 Train Epoch: 94 [0/100000 (0%)]\tLoss: 2.973971\n",
        "# Sun Oct 28 17:39:59 2018 Train Epoch: 94 [25600/100000 (26%)]\tLoss: 2.931156\n",
        "# Sun Oct 28 17:41:17 2018 Train Epoch: 94 [51200/100000 (51%)]\tLoss: 3.068395\n",
        "# Sun Oct 28 17:42:35 2018 Train Epoch: 94 [76800/100000 (77%)]\tLoss: 3.001729\n",
        "\n",
        "# Test set: Average loss: 3.4060, Accuracy: 2527/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:44:19 2018 Train Epoch: 95 [0/100000 (0%)]\tLoss: 2.929064\n",
        "# Sun Oct 28 17:45:38 2018 Train Epoch: 95 [25600/100000 (26%)]\tLoss: 2.977897\n",
        "# Sun Oct 28 17:46:55 2018 Train Epoch: 95 [51200/100000 (51%)]\tLoss: 3.105159\n",
        "# Sun Oct 28 17:48:13 2018 Train Epoch: 95 [76800/100000 (77%)]\tLoss: 3.170931\n",
        "\n",
        "# Test set: Average loss: 3.3527, Accuracy: 2606/10000 (26%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([26.0600])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/095.pt\n",
        "\n",
        "# Sun Oct 28 17:49:57 2018 Train Epoch: 96 [0/100000 (0%)]\tLoss: 2.881077\n",
        "# Sun Oct 28 17:51:16 2018 Train Epoch: 96 [25600/100000 (26%)]\tLoss: 3.086000\n",
        "# Sun Oct 28 17:52:33 2018 Train Epoch: 96 [51200/100000 (51%)]\tLoss: 2.796246\n",
        "# Sun Oct 28 17:53:51 2018 Train Epoch: 96 [76800/100000 (77%)]\tLoss: 3.073726\n",
        "\n",
        "# Test set: Average loss: 3.4838, Accuracy: 2372/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:55:35 2018 Train Epoch: 97 [0/100000 (0%)]\tLoss: 2.994545\n",
        "# Sun Oct 28 17:56:52 2018 Train Epoch: 97 [25600/100000 (26%)]\tLoss: 3.026195\n",
        "# Sun Oct 28 17:58:10 2018 Train Epoch: 97 [51200/100000 (51%)]\tLoss: 3.149062\n",
        "# Sun Oct 28 17:59:28 2018 Train Epoch: 97 [76800/100000 (77%)]\tLoss: 2.845840\n",
        "\n",
        "# Test set: Average loss: 3.4045, Accuracy: 2502/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:01:14 2018 Train Epoch: 98 [0/100000 (0%)]\tLoss: 2.938767\n",
        "# Sun Oct 28 18:02:32 2018 Train Epoch: 98 [25600/100000 (26%)]\tLoss: 3.196684\n",
        "# Sun Oct 28 18:03:50 2018 Train Epoch: 98 [51200/100000 (51%)]\tLoss: 2.869229\n",
        "# Sun Oct 28 18:05:07 2018 Train Epoch: 98 [76800/100000 (77%)]\tLoss: 2.838073\n",
        "\n",
        "# Test set: Average loss: 3.5127, Accuracy: 2333/10000 (23%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:06:54 2018 Train Epoch: 99 [0/100000 (0%)]\tLoss: 2.868000\n",
        "# Sun Oct 28 18:08:12 2018 Train Epoch: 99 [25600/100000 (26%)]\tLoss: 2.768371\n",
        "# Sun Oct 28 18:09:29 2018 Train Epoch: 99 [51200/100000 (51%)]\tLoss: 2.830742\n",
        "# Sun Oct 28 18:10:46 2018 Train Epoch: 99 [76800/100000 (77%)]\tLoss: 2.843505\n",
        "\n",
        "# Test set: Average loss: 3.3901, Accuracy: 2538/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:12:34 2018 Train Epoch: 100 [0/100000 (0%)]\tLoss: 2.909690\n",
        "# Sun Oct 28 18:13:54 2018 Train Epoch: 100 [25600/100000 (26%)]\tLoss: 3.072155\n",
        "# Sun Oct 28 18:15:11 2018 Train Epoch: 100 [51200/100000 (51%)]\tLoss: 2.983590\n",
        "# Sun Oct 28 18:16:29 2018 Train Epoch: 100 [76800/100000 (77%)]\tLoss: 2.837185\n",
        "\n",
        "# Test set: Average loss: 3.4514, Accuracy: 2456/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:18:14 2018 Train Epoch: 101 [0/100000 (0%)]\tLoss: 2.789055\n",
        "# Sun Oct 28 18:19:31 2018 Train Epoch: 101 [25600/100000 (26%)]\tLoss: 3.012294\n",
        "# Sun Oct 28 18:20:49 2018 Train Epoch: 101 [51200/100000 (51%)]\tLoss: 3.070429\n",
        "# Sun Oct 28 18:22:07 2018 Train Epoch: 101 [76800/100000 (77%)]\tLoss: 2.820081\n",
        "\n",
        "# Test set: Average loss: 3.4266, Accuracy: 2442/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:23:52 2018 Train Epoch: 102 [0/100000 (0%)]\tLoss: 3.042038\n",
        "# Sun Oct 28 18:25:10 2018 Train Epoch: 102 [25600/100000 (26%)]\tLoss: 3.000424\n",
        "# Sun Oct 28 18:26:28 2018 Train Epoch: 102 [51200/100000 (51%)]\tLoss: 3.149578\n",
        "# Sun Oct 28 18:27:46 2018 Train Epoch: 102 [76800/100000 (77%)]\tLoss: 2.975560\n",
        "\n",
        "# Test set: Average loss: 3.4082, Accuracy: 2519/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:29:31 2018 Train Epoch: 103 [0/100000 (0%)]\tLoss: 2.909395\n",
        "# Sun Oct 28 18:30:48 2018 Train Epoch: 103 [25600/100000 (26%)]\tLoss: 2.968518\n",
        "# Sun Oct 28 18:32:07 2018 Train Epoch: 103 [51200/100000 (51%)]\tLoss: 2.860015\n",
        "# Sun Oct 28 18:33:26 2018 Train Epoch: 103 [76800/100000 (77%)]\tLoss: 2.839124\n",
        "\n",
        "# Test set: Average loss: 3.4819, Accuracy: 2440/10000 (24%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:35:11 2018 Train Epoch: 104 [0/100000 (0%)]\tLoss: 2.952368\n",
        "# Sun Oct 28 18:36:29 2018 Train Epoch: 104 [25600/100000 (26%)]\tLoss: 2.966947\n",
        "# Sun Oct 28 18:37:48 2018 Train Epoch: 104 [51200/100000 (51%)]\tLoss: 2.753397\n",
        "# Sun Oct 28 18:39:06 2018 Train Epoch: 104 [76800/100000 (77%)]\tLoss: 2.947322\n",
        "\n",
        "# Test set: Average loss: 3.4240, Accuracy: 2505/10000 (25%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:40:52 2018 Train Epoch: 105 [0/100000 (0%)]\tLoss: 3.074938\n",
        "# Sun Oct 28 18:42:10 2018 Train Epoch: 105 [25600/100000 (26%)]\tLoss: 2.985806\n",
        "# Sun Oct 28 18:43:27 2018 Train Epoch: 105 [51200/100000 (51%)]\tLoss: 2.864056\n",
        "# Sun Oct 28 18:44:46 2018 Train Epoch: 105 [76800/100000 (77%)]\tLoss: 3.023882\n",
        "\n",
        "# Test set: Average loss: 3.3465, Accuracy: 2596/10000 (26%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:46:32 2018 Train Epoch: 106 [0/100000 (0%)]\tLoss: 2.823315"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZjJUIprb4UdV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logs of running TinyImagenetNet\n",
        "# executed this training on another google account, hence copying just the logs over here\n",
        "\n",
        "# Using device cuda\n",
        "# num cpus: 2\n",
        "# Restoring:\n",
        "# conv_1.weight -> \ttorch.Size([128, 3, 5, 5]) = 0MB\n",
        "# conv_1.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_1.weight -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_1.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_1.running_mean -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_1.running_var -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
        "# conv_2.weight -> \ttorch.Size([128, 128, 5, 5]) = 1MB\n",
        "# conv_2.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_2.weight -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_2.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_2.running_mean -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_2.running_var -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
        "# conv_3.weight -> \ttorch.Size([128, 128, 5, 5]) = 1MB\n",
        "# conv_3.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_3.weight -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_3.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_3.running_mean -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_3.running_var -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_3.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
        "# conv_4.weight -> \ttorch.Size([128, 128, 2, 2]) = 0MB\n",
        "# conv_4.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_4.weight -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_4.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_4.running_mean -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_4.running_var -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_4.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
        "# conv_5.weight -> \ttorch.Size([128, 128, 5, 5]) = 1MB\n",
        "# conv_5.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_5.weight -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_5.bias -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_5.running_mean -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_5.running_var -> \ttorch.Size([128]) = 0MB\n",
        "# batch_norm_5.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
        "# fc_1.weight -> \ttorch.Size([1024, 3200]) = 13MB\n",
        "# fc_1.bias -> \ttorch.Size([1024]) = 0MB\n",
        "# batch_norm_6.weight -> \ttorch.Size([1024]) = 0MB\n",
        "# batch_norm_6.bias -> \ttorch.Size([1024]) = 0MB\n",
        "# batch_norm_6.running_mean -> \ttorch.Size([1024]) = 0MB\n",
        "# batch_norm_6.running_var -> \ttorch.Size([1024]) = 0MB\n",
        "# batch_norm_6.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
        "# fc_2.weight -> \ttorch.Size([200, 1024]) = 0MB\n",
        "# fc_2.bias -> \ttorch.Size([200]) = 0MB\n",
        "\n",
        "# Restored all variables\n",
        "# No new variables\n",
        "# Restored /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/000.pt\n",
        "\n",
        "# Test set: Average loss: 5.3155, Accuracy: 90/10000 (1%)\n",
        "\n",
        "\n",
        "\n",
        "# Sun Oct 28 12:30:21 2018 Train Epoch: 0 [0/100000 (0%)]\tLoss: 5.377012\n",
        "# Sun Oct 28 12:32:30 2018 Train Epoch: 0 [25600/100000 (26%)]\tLoss: 4.849088\n",
        "# Sun Oct 28 12:34:37 2018 Train Epoch: 0 [51200/100000 (51%)]\tLoss: 4.834553\n",
        "# Sun Oct 28 12:36:45 2018 Train Epoch: 0 [76800/100000 (77%)]\tLoss: 4.502099\n",
        "\n",
        "# Test set: Average loss: 4.2178, Accuracy: 1153/10000 (12%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([11.5300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/000.pt\n",
        "\n",
        "# Sun Oct 28 12:39:29 2018 Train Epoch: 1 [0/100000 (0%)]\tLoss: 4.415096\n",
        "# Sun Oct 28 12:41:37 2018 Train Epoch: 1 [25600/100000 (26%)]\tLoss: 4.419689\n",
        "# Sun Oct 28 12:43:42 2018 Train Epoch: 1 [51200/100000 (51%)]\tLoss: 4.200725\n",
        "# Sun Oct 28 12:45:48 2018 Train Epoch: 1 [76800/100000 (77%)]\tLoss: 4.165313\n",
        "\n",
        "# Test set: Average loss: 3.8780, Accuracy: 1574/10000 (16%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([15.7400])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/001.pt\n",
        "\n",
        "# Sun Oct 28 12:48:28 2018 Train Epoch: 2 [0/100000 (0%)]\tLoss: 4.246354\n",
        "# Sun Oct 28 12:50:34 2018 Train Epoch: 2 [25600/100000 (26%)]\tLoss: 3.965997\n",
        "# Sun Oct 28 12:52:39 2018 Train Epoch: 2 [51200/100000 (51%)]\tLoss: 4.003307\n",
        "# Sun Oct 28 12:54:42 2018 Train Epoch: 2 [76800/100000 (77%)]\tLoss: 3.904356\n",
        "\n",
        "# Test set: Average loss: 3.6897, Accuracy: 1886/10000 (19%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([18.8600])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/002.pt\n",
        "\n",
        "# Sun Oct 28 12:57:25 2018 Train Epoch: 3 [0/100000 (0%)]\tLoss: 3.985389\n",
        "# Sun Oct 28 12:59:31 2018 Train Epoch: 3 [25600/100000 (26%)]\tLoss: 3.916231\n",
        "# Sun Oct 28 13:01:38 2018 Train Epoch: 3 [51200/100000 (51%)]\tLoss: 4.034729\n",
        "# Sun Oct 28 13:03:43 2018 Train Epoch: 3 [76800/100000 (77%)]\tLoss: 3.799181\n",
        "\n",
        "# Test set: Average loss: 3.5720, Accuracy: 2029/10000 (20%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([20.2900])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/003.pt\n",
        "\n",
        "# Sun Oct 28 13:06:24 2018 Train Epoch: 4 [0/100000 (0%)]\tLoss: 3.825869\n",
        "# Sun Oct 28 13:08:30 2018 Train Epoch: 4 [25600/100000 (26%)]\tLoss: 3.733182\n",
        "# Sun Oct 28 13:10:35 2018 Train Epoch: 4 [51200/100000 (51%)]\tLoss: 3.777595\n",
        "# Sun Oct 28 13:12:41 2018 Train Epoch: 4 [76800/100000 (77%)]\tLoss: 3.810255\n",
        "\n",
        "# Test set: Average loss: 3.5045, Accuracy: 2173/10000 (22%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([21.7300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/004.pt\n",
        "\n",
        "# Sun Oct 28 13:15:23 2018 Train Epoch: 5 [0/100000 (0%)]\tLoss: 3.774880\n",
        "# Sun Oct 28 13:17:28 2018 Train Epoch: 5 [25600/100000 (26%)]\tLoss: 3.549268\n",
        "# Sun Oct 28 13:19:32 2018 Train Epoch: 5 [51200/100000 (51%)]\tLoss: 3.920585\n",
        "# Sun Oct 28 13:21:37 2018 Train Epoch: 5 [76800/100000 (77%)]\tLoss: 3.846957\n",
        "\n",
        "# Test set: Average loss: 3.3461, Accuracy: 2401/10000 (24%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([24.0100])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/005.pt\n",
        "\n",
        "# Sun Oct 28 13:24:17 2018 Train Epoch: 6 [0/100000 (0%)]\tLoss: 3.526892\n",
        "# Sun Oct 28 13:26:22 2018 Train Epoch: 6 [25600/100000 (26%)]\tLoss: 3.639910\n",
        "# Sun Oct 28 13:28:27 2018 Train Epoch: 6 [51200/100000 (51%)]\tLoss: 3.854770\n",
        "# Sun Oct 28 13:30:31 2018 Train Epoch: 6 [76800/100000 (77%)]\tLoss: 3.582989\n",
        "\n",
        "# Test set: Average loss: 3.3250, Accuracy: 2445/10000 (24%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([24.4500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/006.pt\n",
        "\n",
        "# Sun Oct 28 13:33:13 2018 Train Epoch: 7 [0/100000 (0%)]\tLoss: 3.522500\n",
        "# Sun Oct 28 13:35:18 2018 Train Epoch: 7 [25600/100000 (26%)]\tLoss: 3.592361\n",
        "# Sun Oct 28 13:37:23 2018 Train Epoch: 7 [51200/100000 (51%)]\tLoss: 3.585311\n",
        "# Sun Oct 28 13:39:28 2018 Train Epoch: 7 [76800/100000 (77%)]\tLoss: 3.659487\n",
        "\n",
        "# Test set: Average loss: 3.1154, Accuracy: 2835/10000 (28%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([28.3500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/007.pt\n",
        "\n",
        "# Sun Oct 28 13:42:11 2018 Train Epoch: 8 [0/100000 (0%)]\tLoss: 3.515057\n",
        "# Sun Oct 28 13:44:15 2018 Train Epoch: 8 [25600/100000 (26%)]\tLoss: 3.284205\n",
        "# Sun Oct 28 13:46:19 2018 Train Epoch: 8 [51200/100000 (51%)]\tLoss: 3.362457\n",
        "# Sun Oct 28 13:48:25 2018 Train Epoch: 8 [76800/100000 (77%)]\tLoss: 3.420733\n",
        "\n",
        "# Test set: Average loss: 3.1314, Accuracy: 2772/10000 (28%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 13:51:06 2018 Train Epoch: 9 [0/100000 (0%)]\tLoss: 3.372928\n",
        "# Sun Oct 28 13:53:11 2018 Train Epoch: 9 [25600/100000 (26%)]\tLoss: 3.450295\n",
        "# Sun Oct 28 13:55:15 2018 Train Epoch: 9 [51200/100000 (51%)]\tLoss: 3.343666\n",
        "# Sun Oct 28 13:57:21 2018 Train Epoch: 9 [76800/100000 (77%)]\tLoss: 3.507637\n",
        "\n",
        "# Test set: Average loss: 3.0201, Accuracy: 3050/10000 (30%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([30.5000])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/009.pt\n",
        "\n",
        "# Sun Oct 28 14:00:02 2018 Train Epoch: 10 [0/100000 (0%)]\tLoss: 3.423912\n",
        "# Sun Oct 28 14:02:07 2018 Train Epoch: 10 [25600/100000 (26%)]\tLoss: 3.438416\n",
        "# Sun Oct 28 14:04:12 2018 Train Epoch: 10 [51200/100000 (51%)]\tLoss: 3.337837\n",
        "# Sun Oct 28 14:06:17 2018 Train Epoch: 10 [76800/100000 (77%)]\tLoss: 3.488422\n",
        "\n",
        "# Test set: Average loss: 2.9825, Accuracy: 3091/10000 (31%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([30.9100])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/010.pt\n",
        "\n",
        "# Sun Oct 28 14:09:00 2018 Train Epoch: 11 [0/100000 (0%)]\tLoss: 3.415284\n",
        "# Sun Oct 28 14:11:05 2018 Train Epoch: 11 [25600/100000 (26%)]\tLoss: 3.347615\n",
        "# Sun Oct 28 14:13:10 2018 Train Epoch: 11 [51200/100000 (51%)]\tLoss: 3.285580\n",
        "# Sun Oct 28 14:15:15 2018 Train Epoch: 11 [76800/100000 (77%)]\tLoss: 3.335426\n",
        "\n",
        "# Test set: Average loss: 2.9595, Accuracy: 3119/10000 (31%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([31.1900])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/011.pt\n",
        "\n",
        "# Sun Oct 28 14:17:57 2018 Train Epoch: 12 [0/100000 (0%)]\tLoss: 3.349758\n",
        "# Sun Oct 28 14:20:01 2018 Train Epoch: 12 [25600/100000 (26%)]\tLoss: 3.151669\n",
        "# Sun Oct 28 14:22:05 2018 Train Epoch: 12 [51200/100000 (51%)]\tLoss: 3.279536\n",
        "# Sun Oct 28 14:24:10 2018 Train Epoch: 12 [76800/100000 (77%)]\tLoss: 3.358614\n",
        "\n",
        "# Test set: Average loss: 2.8855, Accuracy: 3272/10000 (33%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([32.7200])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/012.pt\n",
        "\n",
        "# Sun Oct 28 14:26:52 2018 Train Epoch: 13 [0/100000 (0%)]\tLoss: 3.169672\n",
        "# Sun Oct 28 14:28:58 2018 Train Epoch: 13 [25600/100000 (26%)]\tLoss: 3.283002\n",
        "# Sun Oct 28 14:31:03 2018 Train Epoch: 13 [51200/100000 (51%)]\tLoss: 3.235801\n",
        "# Sun Oct 28 14:33:06 2018 Train Epoch: 13 [76800/100000 (77%)]\tLoss: 3.370761\n",
        "\n",
        "# Test set: Average loss: 2.8394, Accuracy: 3312/10000 (33%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([33.1200])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/013.pt\n",
        "\n",
        "# Sun Oct 28 14:35:48 2018 Train Epoch: 14 [0/100000 (0%)]\tLoss: 3.132638\n",
        "# Sun Oct 28 14:37:53 2018 Train Epoch: 14 [25600/100000 (26%)]\tLoss: 3.255510\n",
        "# Sun Oct 28 14:39:59 2018 Train Epoch: 14 [51200/100000 (51%)]\tLoss: 3.160418\n",
        "# Sun Oct 28 14:42:03 2018 Train Epoch: 14 [76800/100000 (77%)]\tLoss: 3.250893\n",
        "\n",
        "# Test set: Average loss: 2.8518, Accuracy: 3331/10000 (33%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([33.3100])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/014.pt\n",
        "\n",
        "# Sun Oct 28 14:44:45 2018 Train Epoch: 15 [0/100000 (0%)]\tLoss: 3.185097\n",
        "# Sun Oct 28 14:46:51 2018 Train Epoch: 15 [25600/100000 (26%)]\tLoss: 3.379658\n",
        "# Sun Oct 28 14:48:56 2018 Train Epoch: 15 [51200/100000 (51%)]\tLoss: 3.121918\n",
        "# Sun Oct 28 14:51:01 2018 Train Epoch: 15 [76800/100000 (77%)]\tLoss: 3.241090\n",
        "\n",
        "# Test set: Average loss: 2.8052, Accuracy: 3455/10000 (35%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([34.5500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/015.pt\n",
        "\n",
        "# Sun Oct 28 14:53:43 2018 Train Epoch: 16 [0/100000 (0%)]\tLoss: 3.108033\n",
        "# Sun Oct 28 14:55:49 2018 Train Epoch: 16 [25600/100000 (26%)]\tLoss: 3.364268\n",
        "# Sun Oct 28 14:57:54 2018 Train Epoch: 16 [51200/100000 (51%)]\tLoss: 3.099672\n",
        "# Sun Oct 28 15:00:00 2018 Train Epoch: 16 [76800/100000 (77%)]\tLoss: 2.940580\n",
        "\n",
        "# Test set: Average loss: 2.7472, Accuracy: 3523/10000 (35%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([35.2300])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/016.pt\n",
        "\n",
        "# Sun Oct 28 15:02:42 2018 Train Epoch: 17 [0/100000 (0%)]\tLoss: 2.907229\n",
        "# Sun Oct 28 15:04:47 2018 Train Epoch: 17 [25600/100000 (26%)]\tLoss: 3.096868\n",
        "# Sun Oct 28 15:06:52 2018 Train Epoch: 17 [51200/100000 (51%)]\tLoss: 3.190704\n",
        "# Sun Oct 28 15:08:56 2018 Train Epoch: 17 [76800/100000 (77%)]\tLoss: 2.935968\n",
        "\n",
        "# Test set: Average loss: 2.6831, Accuracy: 3625/10000 (36%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([36.2500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/017.pt\n",
        "\n",
        "# Sun Oct 28 15:11:38 2018 Train Epoch: 18 [0/100000 (0%)]\tLoss: 3.434441\n",
        "# Sun Oct 28 15:13:43 2018 Train Epoch: 18 [25600/100000 (26%)]\tLoss: 3.057197\n",
        "# Sun Oct 28 15:15:51 2018 Train Epoch: 18 [51200/100000 (51%)]\tLoss: 3.145785\n",
        "# Sun Oct 28 15:17:56 2018 Train Epoch: 18 [76800/100000 (77%)]\tLoss: 3.054482\n",
        "\n",
        "# Test set: Average loss: 2.8123, Accuracy: 3404/10000 (34%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:20:40 2018 Train Epoch: 19 [0/100000 (0%)]\tLoss: 3.175818\n",
        "# Sun Oct 28 15:22:44 2018 Train Epoch: 19 [25600/100000 (26%)]\tLoss: 3.205219\n",
        "# Sun Oct 28 15:24:49 2018 Train Epoch: 19 [51200/100000 (51%)]\tLoss: 2.967139\n",
        "# Sun Oct 28 15:26:54 2018 Train Epoch: 19 [76800/100000 (77%)]\tLoss: 3.184585\n",
        "\n",
        "# Test set: Average loss: 2.7056, Accuracy: 3666/10000 (37%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([36.6600])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/019.pt\n",
        "\n",
        "# Sun Oct 28 15:29:35 2018 Train Epoch: 20 [0/100000 (0%)]\tLoss: 3.301793\n",
        "# Sun Oct 28 15:31:40 2018 Train Epoch: 20 [25600/100000 (26%)]\tLoss: 3.189958\n",
        "# Sun Oct 28 15:33:44 2018 Train Epoch: 20 [51200/100000 (51%)]\tLoss: 2.966855\n",
        "# Sun Oct 28 15:35:49 2018 Train Epoch: 20 [76800/100000 (77%)]\tLoss: 3.064985\n",
        "\n",
        "# Test set: Average loss: 2.7822, Accuracy: 3483/10000 (35%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:38:31 2018 Train Epoch: 21 [0/100000 (0%)]\tLoss: 2.925046\n",
        "# Sun Oct 28 15:40:36 2018 Train Epoch: 21 [25600/100000 (26%)]\tLoss: 2.828116\n",
        "# Sun Oct 28 15:42:40 2018 Train Epoch: 21 [51200/100000 (51%)]\tLoss: 3.162532\n",
        "# Sun Oct 28 15:44:44 2018 Train Epoch: 21 [76800/100000 (77%)]\tLoss: 2.930682\n",
        "\n",
        "# Test set: Average loss: 2.6374, Accuracy: 3752/10000 (38%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([37.5200])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/021.pt\n",
        "\n",
        "# Sun Oct 28 15:47:25 2018 Train Epoch: 22 [0/100000 (0%)]\tLoss: 3.030813\n",
        "# Sun Oct 28 15:49:30 2018 Train Epoch: 22 [25600/100000 (26%)]\tLoss: 2.930698\n",
        "# Sun Oct 28 15:51:34 2018 Train Epoch: 22 [51200/100000 (51%)]\tLoss: 3.079339\n",
        "# Sun Oct 28 15:53:42 2018 Train Epoch: 22 [76800/100000 (77%)]\tLoss: 3.193906\n",
        "\n",
        "# Test set: Average loss: 2.6431, Accuracy: 3752/10000 (38%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 15:56:25 2018 Train Epoch: 23 [0/100000 (0%)]\tLoss: 2.975350\n",
        "# Sun Oct 28 15:58:33 2018 Train Epoch: 23 [25600/100000 (26%)]\tLoss: 2.908637\n",
        "# Sun Oct 28 16:00:41 2018 Train Epoch: 23 [51200/100000 (51%)]\tLoss: 3.178688\n",
        "# Sun Oct 28 16:02:50 2018 Train Epoch: 23 [76800/100000 (77%)]\tLoss: 3.022272\n",
        "\n",
        "# Test set: Average loss: 2.6824, Accuracy: 3701/10000 (37%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:05:35 2018 Train Epoch: 24 [0/100000 (0%)]\tLoss: 2.861867\n",
        "# Sun Oct 28 16:07:43 2018 Train Epoch: 24 [25600/100000 (26%)]\tLoss: 3.133802\n",
        "# Sun Oct 28 16:09:49 2018 Train Epoch: 24 [51200/100000 (51%)]\tLoss: 2.909939\n",
        "# Sun Oct 28 16:11:56 2018 Train Epoch: 24 [76800/100000 (77%)]\tLoss: 2.913597\n",
        "\n",
        "# Test set: Average loss: 2.5959, Accuracy: 3855/10000 (39%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([38.5500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/024.pt\n",
        "\n",
        "# Sun Oct 28 16:14:40 2018 Train Epoch: 25 [0/100000 (0%)]\tLoss: 2.948693\n",
        "# Sun Oct 28 16:16:47 2018 Train Epoch: 25 [25600/100000 (26%)]\tLoss: 3.007232\n",
        "# Sun Oct 28 16:18:54 2018 Train Epoch: 25 [51200/100000 (51%)]\tLoss: 2.812789\n",
        "# Sun Oct 28 16:21:00 2018 Train Epoch: 25 [76800/100000 (77%)]\tLoss: 2.956999\n",
        "\n",
        "# Test set: Average loss: 2.6339, Accuracy: 3776/10000 (38%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:23:45 2018 Train Epoch: 26 [0/100000 (0%)]\tLoss: 3.070861\n",
        "# Sun Oct 28 16:25:52 2018 Train Epoch: 26 [25600/100000 (26%)]\tLoss: 2.628300\n",
        "# Sun Oct 28 16:28:00 2018 Train Epoch: 26 [51200/100000 (51%)]\tLoss: 2.866710\n",
        "# Sun Oct 28 16:30:07 2018 Train Epoch: 26 [76800/100000 (77%)]\tLoss: 2.816523\n",
        "\n",
        "# Test set: Average loss: 2.5562, Accuracy: 3915/10000 (39%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([39.1500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/026.pt\n",
        "\n",
        "# Sun Oct 28 16:32:51 2018 Train Epoch: 27 [0/100000 (0%)]\tLoss: 2.873649\n",
        "# Sun Oct 28 16:34:58 2018 Train Epoch: 27 [25600/100000 (26%)]\tLoss: 3.097970\n",
        "# Sun Oct 28 16:37:06 2018 Train Epoch: 27 [51200/100000 (51%)]\tLoss: 2.806521\n",
        "# Sun Oct 28 16:39:16 2018 Train Epoch: 27 [76800/100000 (77%)]\tLoss: 2.870451\n",
        "\n",
        "# Test set: Average loss: 2.4989, Accuracy: 4000/10000 (40%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([40.])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/027.pt\n",
        "\n",
        "# Sun Oct 28 16:42:02 2018 Train Epoch: 28 [0/100000 (0%)]\tLoss: 2.796102\n",
        "# Sun Oct 28 16:44:11 2018 Train Epoch: 28 [25600/100000 (26%)]\tLoss: 2.937736\n",
        "# Sun Oct 28 16:46:19 2018 Train Epoch: 28 [51200/100000 (51%)]\tLoss: 2.819060\n",
        "# Sun Oct 28 16:48:26 2018 Train Epoch: 28 [76800/100000 (77%)]\tLoss: 2.994011\n",
        "\n",
        "# Test set: Average loss: 2.5728, Accuracy: 3873/10000 (39%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 16:51:09 2018 Train Epoch: 29 [0/100000 (0%)]\tLoss: 2.723800\n",
        "# Sun Oct 28 16:53:15 2018 Train Epoch: 29 [25600/100000 (26%)]\tLoss: 2.924855\n",
        "# Sun Oct 28 16:55:46 2018 Train Epoch: 29 [51200/100000 (51%)]\tLoss: 2.687292\n",
        "# Sun Oct 28 16:58:01 2018 Train Epoch: 29 [76800/100000 (77%)]\tLoss: 2.816816\n",
        "\n",
        "# Test set: Average loss: 2.5476, Accuracy: 3931/10000 (39%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:00:42 2018 Train Epoch: 30 [0/100000 (0%)]\tLoss: 2.886770\n",
        "# Sun Oct 28 17:02:48 2018 Train Epoch: 30 [25600/100000 (26%)]\tLoss: 2.773642\n",
        "# Sun Oct 28 17:04:52 2018 Train Epoch: 30 [51200/100000 (51%)]\tLoss: 2.897140\n",
        "# Sun Oct 28 17:06:57 2018 Train Epoch: 30 [76800/100000 (77%)]\tLoss: 2.809257\n",
        "\n",
        "# Test set: Average loss: 2.5423, Accuracy: 3943/10000 (39%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:09:37 2018 Train Epoch: 31 [0/100000 (0%)]\tLoss: 2.539403\n",
        "# Sun Oct 28 17:11:43 2018 Train Epoch: 31 [25600/100000 (26%)]\tLoss: 2.767190\n",
        "# Sun Oct 28 17:13:50 2018 Train Epoch: 31 [51200/100000 (51%)]\tLoss: 2.842101\n",
        "# Sun Oct 28 17:15:55 2018 Train Epoch: 31 [76800/100000 (77%)]\tLoss: 2.989471\n",
        "\n",
        "# Test set: Average loss: 2.4899, Accuracy: 4069/10000 (41%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([40.6900])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/031.pt\n",
        "\n",
        "# Sun Oct 28 17:18:37 2018 Train Epoch: 32 [0/100000 (0%)]\tLoss: 2.662488\n",
        "# Sun Oct 28 17:20:43 2018 Train Epoch: 32 [25600/100000 (26%)]\tLoss: 2.713089\n",
        "# Sun Oct 28 17:22:48 2018 Train Epoch: 32 [51200/100000 (51%)]\tLoss: 2.863564\n",
        "# Sun Oct 28 17:24:53 2018 Train Epoch: 32 [76800/100000 (77%)]\tLoss: 2.854995\n",
        "\n",
        "# Test set: Average loss: 2.4579, Accuracy: 4099/10000 (41%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([40.9900])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/032.pt\n",
        "\n",
        "# Sun Oct 28 17:27:33 2018 Train Epoch: 33 [0/100000 (0%)]\tLoss: 2.899722\n",
        "# Sun Oct 28 17:29:39 2018 Train Epoch: 33 [25600/100000 (26%)]\tLoss: 2.830293\n",
        "# Sun Oct 28 17:31:43 2018 Train Epoch: 33 [51200/100000 (51%)]\tLoss: 2.889915\n",
        "# Sun Oct 28 17:33:47 2018 Train Epoch: 33 [76800/100000 (77%)]\tLoss: 2.621592\n",
        "\n",
        "# Test set: Average loss: 2.4746, Accuracy: 4069/10000 (41%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:36:30 2018 Train Epoch: 34 [0/100000 (0%)]\tLoss: 2.759654\n",
        "# Sun Oct 28 17:38:36 2018 Train Epoch: 34 [25600/100000 (26%)]\tLoss: 2.780350\n",
        "# Sun Oct 28 17:40:43 2018 Train Epoch: 34 [51200/100000 (51%)]\tLoss: 2.803836\n",
        "# Sun Oct 28 17:42:48 2018 Train Epoch: 34 [76800/100000 (77%)]\tLoss: 2.874275\n",
        "\n",
        "# Test set: Average loss: 2.5648, Accuracy: 3979/10000 (40%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:45:29 2018 Train Epoch: 35 [0/100000 (0%)]\tLoss: 2.831063\n",
        "# Sun Oct 28 17:47:35 2018 Train Epoch: 35 [25600/100000 (26%)]\tLoss: 2.836558\n",
        "# Sun Oct 28 17:49:42 2018 Train Epoch: 35 [51200/100000 (51%)]\tLoss: 2.921089\n",
        "# Sun Oct 28 17:51:48 2018 Train Epoch: 35 [76800/100000 (77%)]\tLoss: 3.040183\n",
        "\n",
        "# Test set: Average loss: 2.4820, Accuracy: 4073/10000 (41%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 17:54:33 2018 Train Epoch: 36 [0/100000 (0%)]\tLoss: 2.728272\n",
        "# Sun Oct 28 17:56:41 2018 Train Epoch: 36 [25600/100000 (26%)]\tLoss: 2.662808\n",
        "# Sun Oct 28 17:58:45 2018 Train Epoch: 36 [51200/100000 (51%)]\tLoss: 3.046364\n",
        "# Sun Oct 28 18:00:50 2018 Train Epoch: 36 [76800/100000 (77%)]\tLoss: 2.816983\n",
        "\n",
        "# Test set: Average loss: 2.4631, Accuracy: 4165/10000 (42%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([41.6500])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/036.pt\n",
        "\n",
        "# Sun Oct 28 18:03:32 2018 Train Epoch: 37 [0/100000 (0%)]\tLoss: 2.556520\n",
        "# Sun Oct 28 18:05:38 2018 Train Epoch: 37 [25600/100000 (26%)]\tLoss: 2.777833\n",
        "# Sun Oct 28 18:07:43 2018 Train Epoch: 37 [51200/100000 (51%)]\tLoss: 2.610878\n",
        "# Sun Oct 28 18:09:47 2018 Train Epoch: 37 [76800/100000 (77%)]\tLoss: 2.879117\n",
        "\n",
        "# Test set: Average loss: 2.5877, Accuracy: 3947/10000 (39%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:12:28 2018 Train Epoch: 38 [0/100000 (0%)]\tLoss: 2.798246\n",
        "# Sun Oct 28 18:14:35 2018 Train Epoch: 38 [25600/100000 (26%)]\tLoss: 2.639030\n",
        "# Sun Oct 28 18:16:43 2018 Train Epoch: 38 [51200/100000 (51%)]\tLoss: 2.734604\n",
        "# Sun Oct 28 18:18:49 2018 Train Epoch: 38 [76800/100000 (77%)]\tLoss: 2.964253\n",
        "\n",
        "# Test set: Average loss: 2.4983, Accuracy: 4087/10000 (41%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:21:38 2018 Train Epoch: 39 [0/100000 (0%)]\tLoss: 2.570939\n",
        "# Sun Oct 28 18:23:50 2018 Train Epoch: 39 [25600/100000 (26%)]\tLoss: 2.717344\n",
        "# Sun Oct 28 18:26:00 2018 Train Epoch: 39 [51200/100000 (51%)]\tLoss: 2.722348\n",
        "# Sun Oct 28 18:28:09 2018 Train Epoch: 39 [76800/100000 (77%)]\tLoss: 2.776539\n",
        "\n",
        "# Test set: Average loss: 2.4943, Accuracy: 4070/10000 (41%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:30:55 2018 Train Epoch: 40 [0/100000 (0%)]\tLoss: 2.743117\n",
        "# Sun Oct 28 18:33:03 2018 Train Epoch: 40 [25600/100000 (26%)]\tLoss: 2.699856\n",
        "# Sun Oct 28 18:35:08 2018 Train Epoch: 40 [51200/100000 (51%)]\tLoss: 2.883345\n",
        "# Sun Oct 28 18:37:15 2018 Train Epoch: 40 [76800/100000 (77%)]\tLoss: 2.777556\n",
        "\n",
        "# Test set: Average loss: 2.5009, Accuracy: 4043/10000 (40%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:39:59 2018 Train Epoch: 41 [0/100000 (0%)]\tLoss: 2.729731\n",
        "# Sun Oct 28 18:42:07 2018 Train Epoch: 41 [25600/100000 (26%)]\tLoss: 2.693479\n",
        "# Sun Oct 28 18:44:13 2018 Train Epoch: 41 [51200/100000 (51%)]\tLoss: 2.679828\n",
        "# Sun Oct 28 18:46:19 2018 Train Epoch: 41 [76800/100000 (77%)]\tLoss: 2.845359\n",
        "\n",
        "# Test set: Average loss: 2.4864, Accuracy: 4112/10000 (41%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:49:04 2018 Train Epoch: 42 [0/100000 (0%)]\tLoss: 2.558099\n",
        "# Sun Oct 28 18:51:12 2018 Train Epoch: 42 [25600/100000 (26%)]\tLoss: 2.774149\n",
        "# Sun Oct 28 18:53:19 2018 Train Epoch: 42 [51200/100000 (51%)]\tLoss: 2.714049\n",
        "# Sun Oct 28 18:55:26 2018 Train Epoch: 42 [76800/100000 (77%)]\tLoss: 2.687752\n",
        "\n",
        "# Test set: Average loss: 2.5171, Accuracy: 4030/10000 (40%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 18:58:12 2018 Train Epoch: 43 [0/100000 (0%)]\tLoss: 2.644560\n",
        "# Sun Oct 28 19:00:22 2018 Train Epoch: 43 [25600/100000 (26%)]\tLoss: 2.858551\n",
        "# Sun Oct 28 19:02:32 2018 Train Epoch: 43 [51200/100000 (51%)]\tLoss: 2.799983\n",
        "# Sun Oct 28 19:04:40 2018 Train Epoch: 43 [76800/100000 (77%)]\tLoss: 2.754701\n",
        "\n",
        "# Test set: Average loss: 2.3924, Accuracy: 4291/10000 (43%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([42.9100])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/043.pt\n",
        "\n",
        "# Sun Oct 28 19:07:26 2018 Train Epoch: 44 [0/100000 (0%)]\tLoss: 2.822831\n",
        "# Sun Oct 28 19:09:34 2018 Train Epoch: 44 [25600/100000 (26%)]\tLoss: 2.591827\n",
        "# Sun Oct 28 19:11:43 2018 Train Epoch: 44 [51200/100000 (51%)]\tLoss: 2.751663\n",
        "# Sun Oct 28 19:13:52 2018 Train Epoch: 44 [76800/100000 (77%)]\tLoss: 2.609361\n",
        "\n",
        "# Test set: Average loss: 2.4310, Accuracy: 4251/10000 (43%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 19:16:38 2018 Train Epoch: 45 [0/100000 (0%)]\tLoss: 2.681939\n",
        "# Sun Oct 28 19:18:48 2018 Train Epoch: 45 [25600/100000 (26%)]\tLoss: 2.440070\n",
        "# Sun Oct 28 19:20:58 2018 Train Epoch: 45 [51200/100000 (51%)]\tLoss: 2.780433\n",
        "# Sun Oct 28 19:23:08 2018 Train Epoch: 45 [76800/100000 (77%)]\tLoss: 2.708389\n",
        "\n",
        "# Test set: Average loss: 2.4412, Accuracy: 4184/10000 (42%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 19:25:56 2018 Train Epoch: 46 [0/100000 (0%)]\tLoss: 2.397124\n",
        "# Sun Oct 28 19:28:07 2018 Train Epoch: 46 [25600/100000 (26%)]\tLoss: 2.438269\n",
        "# Sun Oct 28 19:30:16 2018 Train Epoch: 46 [51200/100000 (51%)]\tLoss: 2.716933\n",
        "# Sun Oct 28 19:32:25 2018 Train Epoch: 46 [76800/100000 (77%)]\tLoss: 2.512417\n",
        "\n",
        "# Test set: Average loss: 2.4602, Accuracy: 4175/10000 (42%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 19:35:12 2018 Train Epoch: 47 [0/100000 (0%)]\tLoss: 2.722832\n",
        "# Sun Oct 28 19:37:20 2018 Train Epoch: 47 [25600/100000 (26%)]\tLoss: 2.605356\n",
        "# Sun Oct 28 19:39:31 2018 Train Epoch: 47 [51200/100000 (51%)]\tLoss: 2.784863\n",
        "# Sun Oct 28 19:41:41 2018 Train Epoch: 47 [76800/100000 (77%)]\tLoss: 2.796753\n",
        "\n",
        "# Test set: Average loss: 2.3661, Accuracy: 4368/10000 (44%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([43.6800])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/047.pt\n",
        "\n",
        "# Sun Oct 28 19:44:30 2018 Train Epoch: 48 [0/100000 (0%)]\tLoss: 2.559405\n",
        "# Sun Oct 28 19:46:40 2018 Train Epoch: 48 [25600/100000 (26%)]\tLoss: 2.699735\n",
        "# Sun Oct 28 19:48:47 2018 Train Epoch: 48 [51200/100000 (51%)]\tLoss: 2.484727\n",
        "# Sun Oct 28 19:50:57 2018 Train Epoch: 48 [76800/100000 (77%)]\tLoss: 2.616627\n",
        "\n",
        "# Test set: Average loss: 2.4124, Accuracy: 4174/10000 (42%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 19:53:42 2018 Train Epoch: 49 [0/100000 (0%)]\tLoss: 2.644048\n",
        "# Sun Oct 28 19:55:47 2018 Train Epoch: 49 [25600/100000 (26%)]\tLoss: 2.454347\n",
        "# Sun Oct 28 19:57:51 2018 Train Epoch: 49 [51200/100000 (51%)]\tLoss: 2.509847\n",
        "# Sun Oct 28 19:59:56 2018 Train Epoch: 49 [76800/100000 (77%)]\tLoss: 2.638407\n",
        "\n",
        "# Test set: Average loss: 2.3754, Accuracy: 4312/10000 (43%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 20:02:39 2018 Train Epoch: 50 [0/100000 (0%)]\tLoss: 2.621843\n",
        "# Sun Oct 28 20:04:46 2018 Train Epoch: 50 [25600/100000 (26%)]\tLoss: 2.457928\n",
        "# Sun Oct 28 20:06:52 2018 Train Epoch: 50 [51200/100000 (51%)]\tLoss: 2.728131\n",
        "# Sun Oct 28 20:08:59 2018 Train Epoch: 50 [76800/100000 (77%)]\tLoss: 2.793470\n",
        "\n",
        "# Test set: Average loss: 2.4552, Accuracy: 4214/10000 (42%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 20:11:43 2018 Train Epoch: 51 [0/100000 (0%)]\tLoss: 2.573477\n",
        "# Sun Oct 28 20:13:49 2018 Train Epoch: 51 [25600/100000 (26%)]\tLoss: 2.524499\n",
        "# Sun Oct 28 20:15:54 2018 Train Epoch: 51 [51200/100000 (51%)]\tLoss: 2.684808\n",
        "# Sun Oct 28 20:17:58 2018 Train Epoch: 51 [76800/100000 (77%)]\tLoss: 2.593652\n",
        "\n",
        "# Test set: Average loss: 2.3863, Accuracy: 4319/10000 (43%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 20:20:40 2018 Train Epoch: 52 [0/100000 (0%)]\tLoss: 2.625976\n",
        "# Sun Oct 28 20:22:44 2018 Train Epoch: 52 [25600/100000 (26%)]\tLoss: 2.534035\n",
        "# Sun Oct 28 20:24:48 2018 Train Epoch: 52 [51200/100000 (51%)]\tLoss: 2.863297\n",
        "# Sun Oct 28 20:26:52 2018 Train Epoch: 52 [76800/100000 (77%)]\tLoss: 2.621811\n",
        "\n",
        "# Test set: Average loss: 2.3974, Accuracy: 4275/10000 (43%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 20:29:32 2018 Train Epoch: 53 [0/100000 (0%)]\tLoss: 2.603741\n",
        "# Sun Oct 28 20:31:37 2018 Train Epoch: 53 [25600/100000 (26%)]\tLoss: 2.461010\n",
        "# Sun Oct 28 20:33:40 2018 Train Epoch: 53 [51200/100000 (51%)]\tLoss: 2.844060\n",
        "# Sun Oct 28 20:35:46 2018 Train Epoch: 53 [76800/100000 (77%)]\tLoss: 2.625038\n",
        "\n",
        "# Test set: Average loss: 2.3299, Accuracy: 4426/10000 (44%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([44.2600])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/053.pt\n",
        "\n",
        "# Sun Oct 28 20:38:25 2018 Train Epoch: 54 [0/100000 (0%)]\tLoss: 2.671259\n",
        "# Sun Oct 28 20:40:30 2018 Train Epoch: 54 [25600/100000 (26%)]\tLoss: 2.725747\n",
        "# Sun Oct 28 20:42:33 2018 Train Epoch: 54 [51200/100000 (51%)]\tLoss: 2.609068\n",
        "# Sun Oct 28 20:44:37 2018 Train Epoch: 54 [76800/100000 (77%)]\tLoss: 2.494177\n",
        "\n",
        "# Test set: Average loss: 2.3272, Accuracy: 4417/10000 (44%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 20:47:19 2018 Train Epoch: 55 [0/100000 (0%)]\tLoss: 2.549925\n",
        "# Sun Oct 28 20:49:25 2018 Train Epoch: 55 [25600/100000 (26%)]\tLoss: 2.520359\n",
        "# Sun Oct 28 20:51:30 2018 Train Epoch: 55 [51200/100000 (51%)]\tLoss: 2.657639\n",
        "# Sun Oct 28 20:53:34 2018 Train Epoch: 55 [76800/100000 (77%)]\tLoss: 2.691612\n",
        "\n",
        "# Test set: Average loss: 2.4402, Accuracy: 4272/10000 (43%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 20:56:15 2018 Train Epoch: 56 [0/100000 (0%)]\tLoss: 2.641478\n",
        "# Sun Oct 28 20:58:20 2018 Train Epoch: 56 [25600/100000 (26%)]\tLoss: 2.747093\n",
        "# Sun Oct 28 21:00:25 2018 Train Epoch: 56 [51200/100000 (51%)]\tLoss: 2.656837\n",
        "# Sun Oct 28 21:02:31 2018 Train Epoch: 56 [76800/100000 (77%)]\tLoss: 2.661368\n",
        "\n",
        "# Test set: Average loss: 2.4449, Accuracy: 4226/10000 (42%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 21:05:13 2018 Train Epoch: 57 [0/100000 (0%)]\tLoss: 2.574148\n",
        "# Sun Oct 28 21:07:18 2018 Train Epoch: 57 [25600/100000 (26%)]\tLoss: 2.480488\n",
        "# Sun Oct 28 21:09:23 2018 Train Epoch: 57 [51200/100000 (51%)]\tLoss: 2.557541\n",
        "# Sun Oct 28 21:11:28 2018 Train Epoch: 57 [76800/100000 (77%)]\tLoss: 2.561859\n",
        "\n",
        "# Test set: Average loss: 2.3148, Accuracy: 4474/10000 (45%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([44.7400])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/057.pt\n",
        "\n",
        "# Sun Oct 28 21:14:10 2018 Train Epoch: 58 [0/100000 (0%)]\tLoss: 2.482678\n",
        "# Sun Oct 28 21:16:15 2018 Train Epoch: 58 [25600/100000 (26%)]\tLoss: 2.376153\n",
        "# Sun Oct 28 21:18:20 2018 Train Epoch: 58 [51200/100000 (51%)]\tLoss: 2.437112\n",
        "# Sun Oct 28 21:20:26 2018 Train Epoch: 58 [76800/100000 (77%)]\tLoss: 2.890622\n",
        "\n",
        "# Test set: Average loss: 2.3330, Accuracy: 4434/10000 (44%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 21:23:10 2018 Train Epoch: 59 [0/100000 (0%)]\tLoss: 2.563146\n",
        "# Sun Oct 28 21:25:15 2018 Train Epoch: 59 [25600/100000 (26%)]\tLoss: 2.745726\n",
        "# Sun Oct 28 21:27:20 2018 Train Epoch: 59 [51200/100000 (51%)]\tLoss: 2.476393\n",
        "# Sun Oct 28 21:29:25 2018 Train Epoch: 59 [76800/100000 (77%)]\tLoss: 2.525990\n",
        "\n",
        "# Test set: Average loss: 2.3323, Accuracy: 4405/10000 (44%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 21:32:08 2018 Train Epoch: 60 [0/100000 (0%)]\tLoss: 2.561321\n",
        "# Sun Oct 28 21:34:18 2018 Train Epoch: 60 [25600/100000 (26%)]\tLoss: 2.673620\n",
        "# Sun Oct 28 21:36:26 2018 Train Epoch: 60 [51200/100000 (51%)]\tLoss: 2.613992\n",
        "# Sun Oct 28 21:38:54 2018 Train Epoch: 60 [76800/100000 (77%)]\tLoss: 2.567859\n",
        "\n",
        "# Test set: Average loss: 2.3452, Accuracy: 4382/10000 (44%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 21:41:42 2018 Train Epoch: 61 [0/100000 (0%)]\tLoss: 2.520113\n",
        "# Sun Oct 28 21:43:51 2018 Train Epoch: 61 [25600/100000 (26%)]\tLoss: 2.630699\n",
        "# Sun Oct 28 21:45:58 2018 Train Epoch: 61 [51200/100000 (51%)]\tLoss: 2.525153\n",
        "# Sun Oct 28 21:48:05 2018 Train Epoch: 61 [76800/100000 (77%)]\tLoss: 2.823554\n",
        "\n",
        "# Test set: Average loss: 2.3480, Accuracy: 4432/10000 (44%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 21:50:45 2018 Train Epoch: 62 [0/100000 (0%)]\tLoss: 2.457150\n",
        "# Sun Oct 28 21:52:50 2018 Train Epoch: 62 [25600/100000 (26%)]\tLoss: 2.626017\n",
        "# Sun Oct 28 21:54:54 2018 Train Epoch: 62 [51200/100000 (51%)]\tLoss: 2.661081\n",
        "# Sun Oct 28 21:56:59 2018 Train Epoch: 62 [76800/100000 (77%)]\tLoss: 2.544224\n",
        "\n",
        "# Test set: Average loss: 2.3883, Accuracy: 4347/10000 (43%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 21:59:42 2018 Train Epoch: 63 [0/100000 (0%)]\tLoss: 2.362120\n",
        "# Sun Oct 28 22:01:47 2018 Train Epoch: 63 [25600/100000 (26%)]\tLoss: 2.348460\n",
        "# Sun Oct 28 22:03:53 2018 Train Epoch: 63 [51200/100000 (51%)]\tLoss: 2.697369\n",
        "# Sun Oct 28 22:05:57 2018 Train Epoch: 63 [76800/100000 (77%)]\tLoss: 2.663199\n",
        "\n",
        "# Test set: Average loss: 2.3458, Accuracy: 4410/10000 (44%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 22:08:37 2018 Train Epoch: 64 [0/100000 (0%)]\tLoss: 2.609439\n",
        "# Sun Oct 28 22:10:42 2018 Train Epoch: 64 [25600/100000 (26%)]\tLoss: 2.344103\n",
        "# Sun Oct 28 22:12:47 2018 Train Epoch: 64 [51200/100000 (51%)]\tLoss: 2.736394\n",
        "# Sun Oct 28 22:14:51 2018 Train Epoch: 64 [76800/100000 (77%)]\tLoss: 2.566020\n",
        "\n",
        "# Test set: Average loss: 2.2893, Accuracy: 4528/10000 (45%)\n",
        "\n",
        "# Saving new best checkpoint with accuracy, tensor([45.2800])\n",
        "# Saved /gdrive/My Drive/colab_files/homework1/tiny_imagenet/checkpoints/064.pt\n",
        "\n",
        "# Sun Oct 28 22:17:32 2018 Train Epoch: 65 [0/100000 (0%)]\tLoss: 2.398110\n",
        "# Sun Oct 28 22:19:37 2018 Train Epoch: 65 [25600/100000 (26%)]\tLoss: 2.772782\n",
        "# Sun Oct 28 22:21:41 2018 Train Epoch: 65 [51200/100000 (51%)]\tLoss: 2.581842\n",
        "# Sun Oct 28 22:23:45 2018 Train Epoch: 65 [76800/100000 (77%)]\tLoss: 2.706642\n",
        "\n",
        "# Test set: Average loss: 2.3579, Accuracy: 4389/10000 (44%)\n",
        "\n",
        "# Test accuracy did not improve\n",
        "# Sun Oct 28 22:26:25 2018 Train Epoch: 66 [0/100000 (0%)]\tLoss: 2.622486"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FGXjLjtFbHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 6: ImageNet!\n",
        "ImageNet is the big granddaddy of supervised vision datasets. It's the dataset that got the whole deep learning train running. We thought it would be fun for you guys to try your hand at it.\n",
        "\n",
        "A few notes:\n",
        "- Downloading ImageNet should take about 7 minutues. Extracting should take another 7 minutes. So run this code, then go have a coffee or whatever.\n",
        "- The data will be removed after 24 hours or so. That's just how Colab works. Each time you reset the environment, you will also have to redownload ImageNet. \n",
        "    - I would recommend against storing it on your on Google Drive though as that is way slower to read during training.\n",
        "    - Through extensive testing, we have found that the fastest way to get the data is the code we provide (faster than saving to google drive, uploading by hand, etc.) \n",
        "- Reading images directly with PIL seems to have issues on Colab. Instead try cv2.imread.\n",
        "    - cv2.imread returns images in bgr order. To go from bgr -> rgb, use    `im = im[:, :, ::-1]`\n",
        "- The images are provided such that the minimum side length is at least 128 unless the original image was too small for it to be shrunk.\n",
        "    - transforms.Resize will be your friend\n",
        "- It seams you cannot use a batch size > 64 x 128 x 128 x 3 while still using multiprocessing due to Colab limitations. You can use larger batches if you set num_workers = 0, but it will be much slower.\n",
        "- This dataset has 1000 classes, whereas Tiny ImageNet only had 200.\n",
        "- You will probably want to save your results more than once every epoch since Colab may spurriously shut down your session.\n",
        "- We know this takes a long time to run. Try leaving it overnight. We don't expect you to get state-of-the-art performance.\n",
        "- `glob.glob` is a good Python function for reading lots of folder/file names, but it does not return a sorted list.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "68XKUg9hF5Io",
        "colab_type": "code",
        "outputId": "0feb2f98-891f-41fd-8997-b4e0b47206c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloads and extracts the data\n",
        "if not os.path.exists('imagenet128'):\n",
        "    !echo $(date +%x_%r)\n",
        "    !gsutil cp gs://imagenet-cropped/imagenet128.tar .\n",
        "    !echo $(date +%x_%r)\n",
        "    !tar -xf imagenet128.tar\n",
        "    !echo $(date +%x_%r)\n",
        "    !ls imagenet128/train | head -4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/31/2018_07:01:32 AM\n",
            "Copying gs://imagenet-cropped/imagenet128.tar...\n",
            "\\ [1 files][ 23.7 GiB/ 23.7 GiB]   29.8 MiB/s                                   \n",
            "Operation completed over 1 objects/23.7 GiB.                                     \n",
            "10/31/2018_07:08:05 AM\n",
            "10/31/2018_07:13:49 AM\n",
            "n01440764\n",
            "n01443537\n",
            "n01484850\n",
            "n01491361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H1xnyidgF5p5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sets up synset data. Have a look at what is in these dictionaries as they may help you with debugging.\n",
        "import json\n",
        "imagenet_synset_data = json.load(open('imagenet128/imagenet_synsets.json'))\n",
        "synset_id_to_cls = {val['id']: int(key) for key, val in imagenet_synset_data.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mZGFUkQt3HV",
        "colab_type": "code",
        "outputId": "89db42b0-835b-4718-e3af-afdff4352cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ls imagenet128/train/n01440764/ | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YadzpVs-uTDd",
        "colab_type": "code",
        "outputId": "91a58777-12e4-43e1-fcff-fc9520685b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "hello = glob.glob('imagenet128/train/n01440764/*.JPEG')[0]\n",
        "hello.split('/')[-2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'n01440764'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "0bUNNoLlGWgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data loader\n",
        "class ImageFolderLoader(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        #raise NotImplementedError\n",
        "        self.folder = folder\n",
        "        self.transform = transform\n",
        "        self.file_names = glob.glob(folder + '/*/*.JPEG')\n",
        "        \n",
        "       \n",
        "    def __len__(self):\n",
        "        #raise NotImplementedError\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #raise NotImplementedError\n",
        "        file_name = self.file_names[idx]\n",
        "        file_label_id = file_name.split('/')[-2]\n",
        "        label = torch.tensor([synset_id_to_cls[file_label_id]])\n",
        "        im = cv2.imread(file_name)\n",
        "        im = im[:, : , ::-1]\n",
        "        if self.transform:\n",
        "            im = self.transform(im)\n",
        "        return (im, label)\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IwBV9dcTGYqx",
        "colab_type": "code",
        "outputId": "191f5be6-4e05-4e19-c692-fa74912252b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# train_transforms = None\n",
        "# test_transforms = None\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomSizedCrop(128),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(18),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "data_train = ImageFolderLoader('imagenet128/train', transform=train_transforms)\n",
        "assert(len(data_train) == 1281167)\n",
        "data_test = ImageFolderLoader('imagenet128/val', transform=test_transforms)\n",
        "assert(len(data_test) == 50000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
            "  \"please use transforms.RandomResizedCrop instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZpZ9QsO3XwfB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullImagenetNet(TinyImagenetNet):\n",
        "    def __init__(self):\n",
        "        super(FullImagenetNet, self).__init__()\n",
        "        #raise NotImplementedError\n",
        "        self.conv_1 = nn.Conv2d(3, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_1 = nn.ReLU(True);\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(128);\n",
        "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "\n",
        "        self.conv_2 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_2 = nn.ReLU(True);\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(128);\n",
        "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv_3 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_3 = nn.ReLU(True);\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(128);\n",
        "        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv_4 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_4 = nn.ReLU(True);\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(128);\n",
        "        self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "       \n",
        "        self.conv_5 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu_5 = nn.ReLU(True);\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(128);\n",
        "\n",
        "        self.fc_1 = nn.Linear(8192, 2048);\n",
        "        self.relu_6 = nn.Softmax(1);\n",
        "        self.batch_norm_6 = nn.BatchNorm1d(2048);\n",
        "        self.dropout_1 = nn.Dropout(p=0.5);\n",
        "\n",
        "        # Softmax or SVM\n",
        "        self.fc_2 = nn.Linear(2048, 1000);\n",
        "\n",
        "\n",
        "    def forward(self, y):\n",
        "        #raise NotImplementedError\n",
        "        y = self.conv_1(y)\n",
        "        y = self.relu_1(y)\n",
        "        y = self.batch_norm_1(y)\n",
        "        y = self.pool_1(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_2(y)\n",
        "        y = self.relu_2(y)\n",
        "        y = self.batch_norm_2(y)\n",
        "        y = self.pool_2(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_3(y)\n",
        "        y = self.relu_3(y)\n",
        "        y = self.batch_norm_3(y)\n",
        "        y = self.pool_3(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_4(y)\n",
        "        y = self.relu_4(y)\n",
        "        y = self.batch_norm_4(y)\n",
        "        y = self.pool_4(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.conv_5(y)\n",
        "        y = self.relu_5(y)\n",
        "        y = self.batch_norm_5(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = y.view(y.size(0), -1)\n",
        "#         print (y.shape)\n",
        "        y = self.fc_1(y)\n",
        "        y = self.relu_6(y)\n",
        "        y = self.batch_norm_6(y)\n",
        "        y = self.dropout_1(y)\n",
        "#         print (y.shape)\n",
        "        \n",
        "        y = self.fc_2(y)\n",
        "#         print (y.shape)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vcpfOG6BzSfj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# code taken from pytorch resnet architecture definition\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def bn(planes, init_zero=False):\n",
        "    m = nn.BatchNorm2d(planes)\n",
        "    m.weight.data.fill_(0 if init_zero else 1)\n",
        "    m.bias.data.zero_()\n",
        "    return m\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = bn(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = bn(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BottleneckFinal(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = bn(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = bn(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = bn(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.downsample is not None: residual = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BottleneckZero(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = bn(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = bn(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = bn(planes * 4, init_zero=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.downsample is not None: residual = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = bn(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = bn(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = bn(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.downsample is not None: residual = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, k=1):\n",
        "        self.inplanes = 64\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = bn(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, int(64*k), layers[0])\n",
        "        self.layer2 = self._make_layer(block, int(128*k), layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, int(256*k), layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, int(512*k), layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(int(512*k) * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                bn(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, is_best, num_to_keep=1):\n",
        "        # TODO save the model if it is the best\n",
        "        if is_best:\n",
        "          print(\"Saving new best checkpoint with accuracy,\", accuracy)\n",
        "          pt_util.save(self, file_path, num_to_keep)\n",
        "        else:\n",
        "          print (\"Test accuracy did not improve\")\n",
        "        #raise NotImplementedError('Need to implement save_best_model')\n",
        "           \n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model\n",
        "\n",
        "def bnf_resnet50  (pretrained=False): return ResNet(BottleneckFinal, [3, 4, 6, 3])\n",
        "def bnz_resnet50  (pretrained=False): return ResNet(BottleneckZero, [3, 4, 6, 3])\n",
        "def w15_resnet50  (pretrained=False): return ResNet(Bottleneck, [2, 3, 3, 2], k=1.5)\n",
        "def w125_resnet50 (pretrained=False): return ResNet(Bottleneck, [3, 4, 4, 3], k=1.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ww8Th8ow0fYZ",
        "colab_type": "code",
        "outputId": "cc081d4b-8f7b-4e42-f160-17a0454f6c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 27642
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "best_accuracy = torch.FloatTensor([0])\n",
        "TEST_BATCH_SIZE = 50\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0.0005\n",
        "CHECKPOINT_PATH = BASE_PATH + 'imagenet_full/checkpoints1'\n",
        "LOG_PATH = BASE_PATH + 'imagenet_full/' + 'log.pkl'\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "\n",
        "class_names = sorted([(int(key), val['label'].split(',')[0]) for key, val in imagenet_synset_data.items()])\n",
        "name_to_class = {line[1]: line[0] for line in class_names}\n",
        "class_names = [line[1] for line in class_names]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = resnet18().to(device)\n",
        "# model = FullImagenetNet().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(CHECKPOINT_PATH)\n",
        "\n",
        "log_data = pt_util.read_log(LOG_PATH, [])\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\n",
        "        _,_,_,_,_,test_loss, acc = test(model, device, test_loader, True)\n",
        "        acc = torch.FloatTensor([acc])\n",
        "        is_best = bool(acc.numpy() > best_accuracy.numpy())\n",
        "        best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n",
        "        model.save_best_model(acc, CHECKPOINT_PATH + '/%03d.pt' % epoch, is_best=is_best)\n",
        "        data2dump = [epoch, loss.item(), test_loss, acc.item()]\n",
        "        print (data2dump)\n",
        "        log_data.append(data2dump)\n",
        "        \n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model(CHECKPOINT_PATH + '/%03d.pt' % epoch, 0)\n",
        "    pt_util.write_log(LOG_PATH, log_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n",
            "Restoring:\n",
            "conv1.weight -> \ttorch.Size([64, 3, 7, 7]) = 0MB\n",
            "bn1.weight -> \ttorch.Size([64]) = 0MB\n",
            "bn1.bias -> \ttorch.Size([64]) = 0MB\n",
            "bn1.running_mean -> \ttorch.Size([64]) = 0MB\n",
            "bn1.running_var -> \ttorch.Size([64]) = 0MB\n",
            "bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer1.0.conv1.weight -> \ttorch.Size([64, 64, 3, 3]) = 0MB\n",
            "layer1.0.bn1.weight -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn1.bias -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn1.running_mean -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn1.running_var -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer1.0.conv2.weight -> \ttorch.Size([64, 64, 3, 3]) = 0MB\n",
            "layer1.0.bn2.weight -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn2.bias -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn2.running_mean -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn2.running_var -> \ttorch.Size([64]) = 0MB\n",
            "layer1.0.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer1.1.conv1.weight -> \ttorch.Size([64, 64, 3, 3]) = 0MB\n",
            "layer1.1.bn1.weight -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn1.bias -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn1.running_mean -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn1.running_var -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer1.1.conv2.weight -> \ttorch.Size([64, 64, 3, 3]) = 0MB\n",
            "layer1.1.bn2.weight -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn2.bias -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn2.running_mean -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn2.running_var -> \ttorch.Size([64]) = 0MB\n",
            "layer1.1.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer2.0.conv1.weight -> \ttorch.Size([128, 64, 3, 3]) = 0MB\n",
            "layer2.0.bn1.weight -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn1.bias -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn1.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn1.running_var -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer2.0.conv2.weight -> \ttorch.Size([128, 128, 3, 3]) = 0MB\n",
            "layer2.0.bn2.weight -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn2.bias -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn2.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn2.running_var -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer2.0.downsample.0.weight -> \ttorch.Size([128, 64, 1, 1]) = 0MB\n",
            "layer2.0.downsample.1.weight -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.downsample.1.bias -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.downsample.1.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.downsample.1.running_var -> \ttorch.Size([128]) = 0MB\n",
            "layer2.0.downsample.1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer2.1.conv1.weight -> \ttorch.Size([128, 128, 3, 3]) = 0MB\n",
            "layer2.1.bn1.weight -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn1.bias -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn1.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn1.running_var -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer2.1.conv2.weight -> \ttorch.Size([128, 128, 3, 3]) = 0MB\n",
            "layer2.1.bn2.weight -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn2.bias -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn2.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn2.running_var -> \ttorch.Size([128]) = 0MB\n",
            "layer2.1.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer3.0.conv1.weight -> \ttorch.Size([256, 128, 3, 3]) = 1MB\n",
            "layer3.0.bn1.weight -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn1.bias -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn1.running_mean -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn1.running_var -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer3.0.conv2.weight -> \ttorch.Size([256, 256, 3, 3]) = 2MB\n",
            "layer3.0.bn2.weight -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn2.bias -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn2.running_mean -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn2.running_var -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer3.0.downsample.0.weight -> \ttorch.Size([256, 128, 1, 1]) = 0MB\n",
            "layer3.0.downsample.1.weight -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.downsample.1.bias -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.downsample.1.running_mean -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.downsample.1.running_var -> \ttorch.Size([256]) = 0MB\n",
            "layer3.0.downsample.1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer3.1.conv1.weight -> \ttorch.Size([256, 256, 3, 3]) = 2MB\n",
            "layer3.1.bn1.weight -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn1.bias -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn1.running_mean -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn1.running_var -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer3.1.conv2.weight -> \ttorch.Size([256, 256, 3, 3]) = 2MB\n",
            "layer3.1.bn2.weight -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn2.bias -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn2.running_mean -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn2.running_var -> \ttorch.Size([256]) = 0MB\n",
            "layer3.1.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer4.0.conv1.weight -> \ttorch.Size([512, 256, 3, 3]) = 4MB\n",
            "layer4.0.bn1.weight -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn1.bias -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn1.running_mean -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn1.running_var -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer4.0.conv2.weight -> \ttorch.Size([512, 512, 3, 3]) = 9MB\n",
            "layer4.0.bn2.weight -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn2.bias -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn2.running_mean -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn2.running_var -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer4.0.downsample.0.weight -> \ttorch.Size([512, 256, 1, 1]) = 0MB\n",
            "layer4.0.downsample.1.weight -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.downsample.1.bias -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.downsample.1.running_mean -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.downsample.1.running_var -> \ttorch.Size([512]) = 0MB\n",
            "layer4.0.downsample.1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer4.1.conv1.weight -> \ttorch.Size([512, 512, 3, 3]) = 9MB\n",
            "layer4.1.bn1.weight -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn1.bias -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn1.running_mean -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn1.running_var -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "layer4.1.conv2.weight -> \ttorch.Size([512, 512, 3, 3]) = 9MB\n",
            "layer4.1.bn2.weight -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn2.bias -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn2.running_mean -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn2.running_var -> \ttorch.Size([512]) = 0MB\n",
            "layer4.1.bn2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "fc.weight -> \ttorch.Size([1000, 512]) = 2MB\n",
            "fc.bias -> \ttorch.Size([1000]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/005.pt\n",
            "Wed Oct 31 07:31:02 2018 Train Epoch: 5 [0/1281167 (0%)]\tLoss: 4.058011\n",
            "Wed Oct 31 07:31:29 2018 Train Epoch: 5 [6400/1281167 (0%)]\tLoss: 3.587623\n",
            "Wed Oct 31 07:31:56 2018 Train Epoch: 5 [12800/1281167 (1%)]\tLoss: 3.692526\n",
            "Wed Oct 31 07:32:24 2018 Train Epoch: 5 [19200/1281167 (1%)]\tLoss: 3.920380\n",
            "Wed Oct 31 07:32:51 2018 Train Epoch: 5 [25600/1281167 (2%)]\tLoss: 3.611817\n",
            "Wed Oct 31 07:33:19 2018 Train Epoch: 5 [32000/1281167 (2%)]\tLoss: 3.840326\n",
            "Wed Oct 31 07:33:46 2018 Train Epoch: 5 [38400/1281167 (3%)]\tLoss: 4.066532\n",
            "Wed Oct 31 07:34:14 2018 Train Epoch: 5 [44800/1281167 (3%)]\tLoss: 3.759220\n",
            "Wed Oct 31 07:34:41 2018 Train Epoch: 5 [51200/1281167 (4%)]\tLoss: 3.787779\n",
            "Wed Oct 31 07:35:09 2018 Train Epoch: 5 [57600/1281167 (4%)]\tLoss: 3.346452\n",
            "Wed Oct 31 07:35:37 2018 Train Epoch: 5 [64000/1281167 (5%)]\tLoss: 3.653705\n",
            "Wed Oct 31 07:36:04 2018 Train Epoch: 5 [70400/1281167 (5%)]\tLoss: 3.689728\n",
            "Wed Oct 31 07:36:32 2018 Train Epoch: 5 [76800/1281167 (6%)]\tLoss: 3.720217\n",
            "Wed Oct 31 07:36:59 2018 Train Epoch: 5 [83200/1281167 (6%)]\tLoss: 3.702187\n",
            "Wed Oct 31 07:37:27 2018 Train Epoch: 5 [89600/1281167 (7%)]\tLoss: 3.228512\n",
            "Wed Oct 31 07:37:54 2018 Train Epoch: 5 [96000/1281167 (7%)]\tLoss: 3.182351\n",
            "Wed Oct 31 07:38:22 2018 Train Epoch: 5 [102400/1281167 (8%)]\tLoss: 3.860069\n",
            "Wed Oct 31 07:38:50 2018 Train Epoch: 5 [108800/1281167 (8%)]\tLoss: 3.217318\n",
            "Wed Oct 31 07:39:17 2018 Train Epoch: 5 [115200/1281167 (9%)]\tLoss: 3.600095\n",
            "Wed Oct 31 07:39:45 2018 Train Epoch: 5 [121600/1281167 (9%)]\tLoss: 3.498777\n",
            "Wed Oct 31 07:40:12 2018 Train Epoch: 5 [128000/1281167 (10%)]\tLoss: 3.653640\n",
            "Wed Oct 31 07:40:40 2018 Train Epoch: 5 [134400/1281167 (10%)]\tLoss: 3.510711\n",
            "Wed Oct 31 07:41:08 2018 Train Epoch: 5 [140800/1281167 (11%)]\tLoss: 3.363939\n",
            "Wed Oct 31 07:41:35 2018 Train Epoch: 5 [147200/1281167 (11%)]\tLoss: 3.473051\n",
            "Wed Oct 31 07:42:03 2018 Train Epoch: 5 [153600/1281167 (12%)]\tLoss: 4.052924\n",
            "Wed Oct 31 07:42:30 2018 Train Epoch: 5 [160000/1281167 (12%)]\tLoss: 3.677474\n",
            "Wed Oct 31 07:42:58 2018 Train Epoch: 5 [166400/1281167 (13%)]\tLoss: 3.429228\n",
            "Wed Oct 31 07:43:26 2018 Train Epoch: 5 [172800/1281167 (13%)]\tLoss: 3.073106\n",
            "Wed Oct 31 07:43:53 2018 Train Epoch: 5 [179200/1281167 (14%)]\tLoss: 4.013768\n",
            "Wed Oct 31 07:44:21 2018 Train Epoch: 5 [185600/1281167 (14%)]\tLoss: 3.712402\n",
            "Wed Oct 31 07:44:48 2018 Train Epoch: 5 [192000/1281167 (15%)]\tLoss: 3.749051\n",
            "Wed Oct 31 07:45:16 2018 Train Epoch: 5 [198400/1281167 (15%)]\tLoss: 3.231971\n",
            "Wed Oct 31 07:45:44 2018 Train Epoch: 5 [204800/1281167 (16%)]\tLoss: 3.542559\n",
            "Wed Oct 31 07:46:11 2018 Train Epoch: 5 [211200/1281167 (16%)]\tLoss: 3.088151\n",
            "Wed Oct 31 07:46:39 2018 Train Epoch: 5 [217600/1281167 (17%)]\tLoss: 3.250340\n",
            "Wed Oct 31 07:47:06 2018 Train Epoch: 5 [224000/1281167 (17%)]\tLoss: 4.071111\n",
            "Wed Oct 31 07:47:34 2018 Train Epoch: 5 [230400/1281167 (18%)]\tLoss: 2.932119\n",
            "Wed Oct 31 07:48:01 2018 Train Epoch: 5 [236800/1281167 (18%)]\tLoss: 4.024656\n",
            "Wed Oct 31 07:48:29 2018 Train Epoch: 5 [243200/1281167 (19%)]\tLoss: 3.581292\n",
            "Wed Oct 31 07:48:57 2018 Train Epoch: 5 [249600/1281167 (19%)]\tLoss: 4.002925\n",
            "Wed Oct 31 07:49:24 2018 Train Epoch: 5 [256000/1281167 (20%)]\tLoss: 3.433008\n",
            "Wed Oct 31 07:49:52 2018 Train Epoch: 5 [262400/1281167 (20%)]\tLoss: 3.878928\n",
            "Wed Oct 31 07:50:19 2018 Train Epoch: 5 [268800/1281167 (21%)]\tLoss: 3.460163\n",
            "Wed Oct 31 07:50:47 2018 Train Epoch: 5 [275200/1281167 (21%)]\tLoss: 3.960477\n",
            "Wed Oct 31 07:51:15 2018 Train Epoch: 5 [281600/1281167 (22%)]\tLoss: 3.684520\n",
            "Wed Oct 31 07:51:42 2018 Train Epoch: 5 [288000/1281167 (22%)]\tLoss: 4.053861\n",
            "Wed Oct 31 07:52:10 2018 Train Epoch: 5 [294400/1281167 (23%)]\tLoss: 3.290817\n",
            "Wed Oct 31 07:52:37 2018 Train Epoch: 5 [300800/1281167 (23%)]\tLoss: 3.957287\n",
            "Wed Oct 31 07:53:04 2018 Train Epoch: 5 [307200/1281167 (24%)]\tLoss: 3.254795\n",
            "Wed Oct 31 07:53:32 2018 Train Epoch: 5 [313600/1281167 (24%)]\tLoss: 3.703368\n",
            "Wed Oct 31 07:53:59 2018 Train Epoch: 5 [320000/1281167 (25%)]\tLoss: 3.469972\n",
            "Wed Oct 31 07:54:26 2018 Train Epoch: 5 [326400/1281167 (25%)]\tLoss: 3.715039\n",
            "Wed Oct 31 07:54:53 2018 Train Epoch: 5 [332800/1281167 (26%)]\tLoss: 3.679844\n",
            "Wed Oct 31 07:55:21 2018 Train Epoch: 5 [339200/1281167 (26%)]\tLoss: 3.622820\n",
            "Wed Oct 31 07:55:48 2018 Train Epoch: 5 [345600/1281167 (27%)]\tLoss: 3.496108\n",
            "Wed Oct 31 07:56:15 2018 Train Epoch: 5 [352000/1281167 (27%)]\tLoss: 3.082460\n",
            "Wed Oct 31 07:56:42 2018 Train Epoch: 5 [358400/1281167 (28%)]\tLoss: 3.424255\n",
            "Wed Oct 31 07:57:09 2018 Train Epoch: 5 [364800/1281167 (28%)]\tLoss: 4.017190\n",
            "Wed Oct 31 07:57:37 2018 Train Epoch: 5 [371200/1281167 (29%)]\tLoss: 3.779017\n",
            "Wed Oct 31 07:58:04 2018 Train Epoch: 5 [377600/1281167 (29%)]\tLoss: 3.489846\n",
            "Wed Oct 31 07:58:31 2018 Train Epoch: 5 [384000/1281167 (30%)]\tLoss: 3.237533\n",
            "Wed Oct 31 07:58:58 2018 Train Epoch: 5 [390400/1281167 (30%)]\tLoss: 3.843164\n",
            "Wed Oct 31 07:59:25 2018 Train Epoch: 5 [396800/1281167 (31%)]\tLoss: 3.471278\n",
            "Wed Oct 31 07:59:53 2018 Train Epoch: 5 [403200/1281167 (31%)]\tLoss: 3.142713\n",
            "Wed Oct 31 08:00:20 2018 Train Epoch: 5 [409600/1281167 (32%)]\tLoss: 3.475248\n",
            "Wed Oct 31 08:00:47 2018 Train Epoch: 5 [416000/1281167 (32%)]\tLoss: 3.835055\n",
            "Wed Oct 31 08:01:14 2018 Train Epoch: 5 [422400/1281167 (33%)]\tLoss: 4.234496\n",
            "Wed Oct 31 08:01:42 2018 Train Epoch: 5 [428800/1281167 (33%)]\tLoss: 3.427750\n",
            "Wed Oct 31 08:02:09 2018 Train Epoch: 5 [435200/1281167 (34%)]\tLoss: 3.635866\n",
            "Wed Oct 31 08:02:36 2018 Train Epoch: 5 [441600/1281167 (34%)]\tLoss: 3.850441\n",
            "Wed Oct 31 08:03:03 2018 Train Epoch: 5 [448000/1281167 (35%)]\tLoss: 3.660751\n",
            "Wed Oct 31 08:03:30 2018 Train Epoch: 5 [454400/1281167 (35%)]\tLoss: 3.552867\n",
            "Wed Oct 31 08:03:58 2018 Train Epoch: 5 [460800/1281167 (36%)]\tLoss: 3.980240\n",
            "Wed Oct 31 08:04:25 2018 Train Epoch: 5 [467200/1281167 (36%)]\tLoss: 3.806351\n",
            "Wed Oct 31 08:04:52 2018 Train Epoch: 5 [473600/1281167 (37%)]\tLoss: 3.785655\n",
            "Wed Oct 31 08:05:19 2018 Train Epoch: 5 [480000/1281167 (37%)]\tLoss: 3.062633\n",
            "Wed Oct 31 08:05:46 2018 Train Epoch: 5 [486400/1281167 (38%)]\tLoss: 4.067804\n",
            "Wed Oct 31 08:06:14 2018 Train Epoch: 5 [492800/1281167 (38%)]\tLoss: 3.682819\n",
            "Wed Oct 31 08:06:41 2018 Train Epoch: 5 [499200/1281167 (39%)]\tLoss: 3.820234\n",
            "Wed Oct 31 08:07:08 2018 Train Epoch: 5 [505600/1281167 (39%)]\tLoss: 3.657043\n",
            "Wed Oct 31 08:07:35 2018 Train Epoch: 5 [512000/1281167 (40%)]\tLoss: 3.428146\n",
            "Wed Oct 31 08:08:03 2018 Train Epoch: 5 [518400/1281167 (40%)]\tLoss: 3.463760\n",
            "Wed Oct 31 08:08:30 2018 Train Epoch: 5 [524800/1281167 (41%)]\tLoss: 3.851542\n",
            "Wed Oct 31 08:08:57 2018 Train Epoch: 5 [531200/1281167 (41%)]\tLoss: 3.642380\n",
            "Wed Oct 31 08:09:24 2018 Train Epoch: 5 [537600/1281167 (42%)]\tLoss: 4.110217\n",
            "Wed Oct 31 08:09:51 2018 Train Epoch: 5 [544000/1281167 (42%)]\tLoss: 3.630615\n",
            "Wed Oct 31 08:10:19 2018 Train Epoch: 5 [550400/1281167 (43%)]\tLoss: 3.470161\n",
            "Wed Oct 31 08:10:46 2018 Train Epoch: 5 [556800/1281167 (43%)]\tLoss: 3.673079\n",
            "Wed Oct 31 08:11:13 2018 Train Epoch: 5 [563200/1281167 (44%)]\tLoss: 3.619609\n",
            "Wed Oct 31 08:11:40 2018 Train Epoch: 5 [569600/1281167 (44%)]\tLoss: 3.494541\n",
            "Wed Oct 31 08:12:08 2018 Train Epoch: 5 [576000/1281167 (45%)]\tLoss: 4.142061\n",
            "Wed Oct 31 08:12:35 2018 Train Epoch: 5 [582400/1281167 (45%)]\tLoss: 4.372660\n",
            "Wed Oct 31 08:13:02 2018 Train Epoch: 5 [588800/1281167 (46%)]\tLoss: 3.749257\n",
            "Wed Oct 31 08:13:29 2018 Train Epoch: 5 [595200/1281167 (46%)]\tLoss: 3.959718\n",
            "Wed Oct 31 08:13:56 2018 Train Epoch: 5 [601600/1281167 (47%)]\tLoss: 3.652872\n",
            "Wed Oct 31 08:14:24 2018 Train Epoch: 5 [608000/1281167 (47%)]\tLoss: 3.553887\n",
            "Wed Oct 31 08:14:51 2018 Train Epoch: 5 [614400/1281167 (48%)]\tLoss: 3.449818\n",
            "Wed Oct 31 08:15:18 2018 Train Epoch: 5 [620800/1281167 (48%)]\tLoss: 3.424174\n",
            "Wed Oct 31 08:15:45 2018 Train Epoch: 5 [627200/1281167 (49%)]\tLoss: 3.780434\n",
            "Wed Oct 31 08:16:13 2018 Train Epoch: 5 [633600/1281167 (49%)]\tLoss: 3.271124\n",
            "Wed Oct 31 08:16:40 2018 Train Epoch: 5 [640000/1281167 (50%)]\tLoss: 3.689869\n",
            "Wed Oct 31 08:17:07 2018 Train Epoch: 5 [646400/1281167 (50%)]\tLoss: 3.754938\n",
            "Wed Oct 31 08:17:34 2018 Train Epoch: 5 [652800/1281167 (51%)]\tLoss: 3.659804\n",
            "Wed Oct 31 08:18:01 2018 Train Epoch: 5 [659200/1281167 (51%)]\tLoss: 4.145591\n",
            "Wed Oct 31 08:18:29 2018 Train Epoch: 5 [665600/1281167 (52%)]\tLoss: 3.941767\n",
            "Wed Oct 31 08:18:56 2018 Train Epoch: 5 [672000/1281167 (52%)]\tLoss: 3.784144\n",
            "Wed Oct 31 08:19:23 2018 Train Epoch: 5 [678400/1281167 (53%)]\tLoss: 3.913230\n",
            "Wed Oct 31 08:19:50 2018 Train Epoch: 5 [684800/1281167 (53%)]\tLoss: 2.995116\n",
            "Wed Oct 31 08:20:17 2018 Train Epoch: 5 [691200/1281167 (54%)]\tLoss: 3.412389\n",
            "Wed Oct 31 08:20:45 2018 Train Epoch: 5 [697600/1281167 (54%)]\tLoss: 4.192901\n",
            "Wed Oct 31 08:21:12 2018 Train Epoch: 5 [704000/1281167 (55%)]\tLoss: 3.593061\n",
            "Wed Oct 31 08:21:39 2018 Train Epoch: 5 [710400/1281167 (55%)]\tLoss: 3.933022\n",
            "Wed Oct 31 08:22:06 2018 Train Epoch: 5 [716800/1281167 (56%)]\tLoss: 3.706971\n",
            "Wed Oct 31 08:22:33 2018 Train Epoch: 5 [723200/1281167 (56%)]\tLoss: 3.620215\n",
            "Wed Oct 31 08:23:01 2018 Train Epoch: 5 [729600/1281167 (57%)]\tLoss: 3.308594\n",
            "Wed Oct 31 08:23:28 2018 Train Epoch: 5 [736000/1281167 (57%)]\tLoss: 3.419663\n",
            "Wed Oct 31 08:23:55 2018 Train Epoch: 5 [742400/1281167 (58%)]\tLoss: 3.846511\n",
            "Wed Oct 31 08:24:22 2018 Train Epoch: 5 [748800/1281167 (58%)]\tLoss: 4.089630\n",
            "Wed Oct 31 08:24:50 2018 Train Epoch: 5 [755200/1281167 (59%)]\tLoss: 3.675190\n",
            "Wed Oct 31 08:25:17 2018 Train Epoch: 5 [761600/1281167 (59%)]\tLoss: 3.759145\n",
            "Wed Oct 31 08:25:44 2018 Train Epoch: 5 [768000/1281167 (60%)]\tLoss: 3.894335\n",
            "Wed Oct 31 08:26:11 2018 Train Epoch: 5 [774400/1281167 (60%)]\tLoss: 4.088552\n",
            "Wed Oct 31 08:26:39 2018 Train Epoch: 5 [780800/1281167 (61%)]\tLoss: 3.268409\n",
            "Wed Oct 31 08:27:06 2018 Train Epoch: 5 [787200/1281167 (61%)]\tLoss: 3.513729\n",
            "Wed Oct 31 08:27:33 2018 Train Epoch: 5 [793600/1281167 (62%)]\tLoss: 3.813978\n",
            "Wed Oct 31 08:28:00 2018 Train Epoch: 5 [800000/1281167 (62%)]\tLoss: 3.566543\n",
            "Wed Oct 31 08:28:27 2018 Train Epoch: 5 [806400/1281167 (63%)]\tLoss: 3.416295\n",
            "Wed Oct 31 08:28:55 2018 Train Epoch: 5 [812800/1281167 (63%)]\tLoss: 3.644776\n",
            "Wed Oct 31 08:29:22 2018 Train Epoch: 5 [819200/1281167 (64%)]\tLoss: 3.146053\n",
            "Wed Oct 31 08:29:49 2018 Train Epoch: 5 [825600/1281167 (64%)]\tLoss: 3.528570\n",
            "Wed Oct 31 08:30:16 2018 Train Epoch: 5 [832000/1281167 (65%)]\tLoss: 3.456335\n",
            "Wed Oct 31 08:30:43 2018 Train Epoch: 5 [838400/1281167 (65%)]\tLoss: 3.524977\n",
            "Wed Oct 31 08:31:11 2018 Train Epoch: 5 [844800/1281167 (66%)]\tLoss: 2.861196\n",
            "Wed Oct 31 08:31:38 2018 Train Epoch: 5 [851200/1281167 (66%)]\tLoss: 3.828671\n",
            "Wed Oct 31 08:32:05 2018 Train Epoch: 5 [857600/1281167 (67%)]\tLoss: 3.501945\n",
            "Wed Oct 31 08:32:32 2018 Train Epoch: 5 [864000/1281167 (67%)]\tLoss: 3.405988\n",
            "Wed Oct 31 08:33:00 2018 Train Epoch: 5 [870400/1281167 (68%)]\tLoss: 3.601685\n",
            "Wed Oct 31 08:33:27 2018 Train Epoch: 5 [876800/1281167 (68%)]\tLoss: 3.763107\n",
            "Wed Oct 31 08:33:54 2018 Train Epoch: 5 [883200/1281167 (69%)]\tLoss: 3.456821\n",
            "Wed Oct 31 08:34:21 2018 Train Epoch: 5 [889600/1281167 (69%)]\tLoss: 4.074180\n",
            "Wed Oct 31 08:34:48 2018 Train Epoch: 5 [896000/1281167 (70%)]\tLoss: 3.647475\n",
            "Wed Oct 31 08:35:16 2018 Train Epoch: 5 [902400/1281167 (70%)]\tLoss: 3.852766\n",
            "Wed Oct 31 08:35:43 2018 Train Epoch: 5 [908800/1281167 (71%)]\tLoss: 3.686987\n",
            "Wed Oct 31 08:36:10 2018 Train Epoch: 5 [915200/1281167 (71%)]\tLoss: 3.842345\n",
            "Wed Oct 31 08:36:37 2018 Train Epoch: 5 [921600/1281167 (72%)]\tLoss: 3.333383\n",
            "Wed Oct 31 08:37:05 2018 Train Epoch: 5 [928000/1281167 (72%)]\tLoss: 3.394755\n",
            "Wed Oct 31 08:37:32 2018 Train Epoch: 5 [934400/1281167 (73%)]\tLoss: 4.036407\n",
            "Wed Oct 31 08:37:59 2018 Train Epoch: 5 [940800/1281167 (73%)]\tLoss: 3.716068\n",
            "Wed Oct 31 08:38:26 2018 Train Epoch: 5 [947200/1281167 (74%)]\tLoss: 3.720029\n",
            "Wed Oct 31 08:38:54 2018 Train Epoch: 5 [953600/1281167 (74%)]\tLoss: 3.472674\n",
            "Wed Oct 31 08:39:21 2018 Train Epoch: 5 [960000/1281167 (75%)]\tLoss: 3.483199\n",
            "Wed Oct 31 08:39:48 2018 Train Epoch: 5 [966400/1281167 (75%)]\tLoss: 3.655325\n",
            "Wed Oct 31 08:40:15 2018 Train Epoch: 5 [972800/1281167 (76%)]\tLoss: 3.811238\n",
            "Wed Oct 31 08:40:42 2018 Train Epoch: 5 [979200/1281167 (76%)]\tLoss: 3.449160\n",
            "Wed Oct 31 08:41:10 2018 Train Epoch: 5 [985600/1281167 (77%)]\tLoss: 3.963132\n",
            "Wed Oct 31 08:41:37 2018 Train Epoch: 5 [992000/1281167 (77%)]\tLoss: 3.783826\n",
            "Wed Oct 31 08:42:04 2018 Train Epoch: 5 [998400/1281167 (78%)]\tLoss: 4.191797\n",
            "Wed Oct 31 08:42:31 2018 Train Epoch: 5 [1004800/1281167 (78%)]\tLoss: 3.504066\n",
            "Wed Oct 31 08:42:58 2018 Train Epoch: 5 [1011200/1281167 (79%)]\tLoss: 3.705927\n",
            "Wed Oct 31 08:43:26 2018 Train Epoch: 5 [1017600/1281167 (79%)]\tLoss: 2.936022\n",
            "Wed Oct 31 08:43:53 2018 Train Epoch: 5 [1024000/1281167 (80%)]\tLoss: 3.477963\n",
            "Wed Oct 31 08:44:20 2018 Train Epoch: 5 [1030400/1281167 (80%)]\tLoss: 3.576715\n",
            "Wed Oct 31 08:44:47 2018 Train Epoch: 5 [1036800/1281167 (81%)]\tLoss: 3.527113\n",
            "Wed Oct 31 08:45:15 2018 Train Epoch: 5 [1043200/1281167 (81%)]\tLoss: 3.766308\n",
            "Wed Oct 31 08:45:42 2018 Train Epoch: 5 [1049600/1281167 (82%)]\tLoss: 3.608317\n",
            "Wed Oct 31 08:46:09 2018 Train Epoch: 5 [1056000/1281167 (82%)]\tLoss: 3.569336\n",
            "Wed Oct 31 08:46:36 2018 Train Epoch: 5 [1062400/1281167 (83%)]\tLoss: 3.747402\n",
            "Wed Oct 31 08:47:03 2018 Train Epoch: 5 [1068800/1281167 (83%)]\tLoss: 3.379204\n",
            "Wed Oct 31 08:47:31 2018 Train Epoch: 5 [1075200/1281167 (84%)]\tLoss: 3.811474\n",
            "Wed Oct 31 08:47:58 2018 Train Epoch: 5 [1081600/1281167 (84%)]\tLoss: 4.018148\n",
            "Wed Oct 31 08:48:25 2018 Train Epoch: 5 [1088000/1281167 (85%)]\tLoss: 3.429703\n",
            "Wed Oct 31 08:48:52 2018 Train Epoch: 5 [1094400/1281167 (85%)]\tLoss: 3.702506\n",
            "Wed Oct 31 08:49:20 2018 Train Epoch: 5 [1100800/1281167 (86%)]\tLoss: 3.684099\n",
            "Wed Oct 31 08:49:47 2018 Train Epoch: 5 [1107200/1281167 (86%)]\tLoss: 3.468730\n",
            "Wed Oct 31 08:50:14 2018 Train Epoch: 5 [1113600/1281167 (87%)]\tLoss: 3.538084\n",
            "Wed Oct 31 08:50:41 2018 Train Epoch: 5 [1120000/1281167 (87%)]\tLoss: 3.689895\n",
            "Wed Oct 31 08:51:08 2018 Train Epoch: 5 [1126400/1281167 (88%)]\tLoss: 3.691160\n",
            "Wed Oct 31 08:51:36 2018 Train Epoch: 5 [1132800/1281167 (88%)]\tLoss: 3.616948\n",
            "Wed Oct 31 08:52:03 2018 Train Epoch: 5 [1139200/1281167 (89%)]\tLoss: 3.516117\n",
            "Wed Oct 31 08:52:30 2018 Train Epoch: 5 [1145600/1281167 (89%)]\tLoss: 4.240053\n",
            "Wed Oct 31 08:52:57 2018 Train Epoch: 5 [1152000/1281167 (90%)]\tLoss: 3.553508\n",
            "Wed Oct 31 08:53:25 2018 Train Epoch: 5 [1158400/1281167 (90%)]\tLoss: 3.405334\n",
            "Wed Oct 31 08:53:52 2018 Train Epoch: 5 [1164800/1281167 (91%)]\tLoss: 3.383955\n",
            "Wed Oct 31 08:54:19 2018 Train Epoch: 5 [1171200/1281167 (91%)]\tLoss: 3.197826\n",
            "Wed Oct 31 08:54:46 2018 Train Epoch: 5 [1177600/1281167 (92%)]\tLoss: 3.334543\n",
            "Wed Oct 31 08:55:13 2018 Train Epoch: 5 [1184000/1281167 (92%)]\tLoss: 3.893160\n",
            "Wed Oct 31 08:55:41 2018 Train Epoch: 5 [1190400/1281167 (93%)]\tLoss: 3.879976\n",
            "Wed Oct 31 08:56:08 2018 Train Epoch: 5 [1196800/1281167 (93%)]\tLoss: 3.670784\n",
            "Wed Oct 31 08:56:35 2018 Train Epoch: 5 [1203200/1281167 (94%)]\tLoss: 4.152830\n",
            "Wed Oct 31 08:57:02 2018 Train Epoch: 5 [1209600/1281167 (94%)]\tLoss: 3.666236\n",
            "Wed Oct 31 08:57:29 2018 Train Epoch: 5 [1216000/1281167 (95%)]\tLoss: 3.838167\n",
            "Wed Oct 31 08:57:57 2018 Train Epoch: 5 [1222400/1281167 (95%)]\tLoss: 3.681051\n",
            "Wed Oct 31 08:58:24 2018 Train Epoch: 5 [1228800/1281167 (96%)]\tLoss: 3.028848\n",
            "Wed Oct 31 08:58:51 2018 Train Epoch: 5 [1235200/1281167 (96%)]\tLoss: 3.480066\n",
            "Wed Oct 31 08:59:18 2018 Train Epoch: 5 [1241600/1281167 (97%)]\tLoss: 3.711172\n",
            "Wed Oct 31 08:59:46 2018 Train Epoch: 5 [1248000/1281167 (97%)]\tLoss: 2.886681\n",
            "Wed Oct 31 09:00:13 2018 Train Epoch: 5 [1254400/1281167 (98%)]\tLoss: 3.566924\n",
            "Wed Oct 31 09:00:40 2018 Train Epoch: 5 [1260800/1281167 (98%)]\tLoss: 3.375828\n",
            "Wed Oct 31 09:01:07 2018 Train Epoch: 5 [1267200/1281167 (99%)]\tLoss: 3.702514\n",
            "Wed Oct 31 09:01:34 2018 Train Epoch: 5 [1273600/1281167 (99%)]\tLoss: 4.195517\n",
            "Wed Oct 31 09:02:02 2018 Train Epoch: 5 [1280000/1281167 (100%)]\tLoss: 3.175907\n",
            "\n",
            "Test set: Average loss: 3.5294, Accuracy: 13493/50000 (27%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/005.pt\n",
            "\n",
            "Wed Oct 31 09:05:59 2018 Train Epoch: 6 [0/1281167 (0%)]\tLoss: 3.229624\n",
            "Wed Oct 31 09:06:26 2018 Train Epoch: 6 [6400/1281167 (0%)]\tLoss: 3.579745\n",
            "Wed Oct 31 09:06:53 2018 Train Epoch: 6 [12800/1281167 (1%)]\tLoss: 3.767463\n",
            "Wed Oct 31 09:07:21 2018 Train Epoch: 6 [19200/1281167 (1%)]\tLoss: 3.886413\n",
            "Wed Oct 31 09:07:48 2018 Train Epoch: 6 [25600/1281167 (2%)]\tLoss: 3.187989\n",
            "Wed Oct 31 09:08:15 2018 Train Epoch: 6 [32000/1281167 (2%)]\tLoss: 3.181020\n",
            "Wed Oct 31 09:08:42 2018 Train Epoch: 6 [38400/1281167 (3%)]\tLoss: 3.923687\n",
            "Wed Oct 31 09:09:09 2018 Train Epoch: 6 [44800/1281167 (3%)]\tLoss: 3.884410\n",
            "Wed Oct 31 09:09:37 2018 Train Epoch: 6 [51200/1281167 (4%)]\tLoss: 3.983097\n",
            "Wed Oct 31 09:10:04 2018 Train Epoch: 6 [57600/1281167 (4%)]\tLoss: 3.435335\n",
            "Wed Oct 31 09:10:31 2018 Train Epoch: 6 [64000/1281167 (5%)]\tLoss: 3.895551\n",
            "Wed Oct 31 09:10:58 2018 Train Epoch: 6 [70400/1281167 (5%)]\tLoss: 3.559556\n",
            "Wed Oct 31 09:11:25 2018 Train Epoch: 6 [76800/1281167 (6%)]\tLoss: 4.124943\n",
            "Wed Oct 31 09:11:53 2018 Train Epoch: 6 [83200/1281167 (6%)]\tLoss: 3.502359\n",
            "Wed Oct 31 09:12:20 2018 Train Epoch: 6 [89600/1281167 (7%)]\tLoss: 3.323475\n",
            "Wed Oct 31 09:12:47 2018 Train Epoch: 6 [96000/1281167 (7%)]\tLoss: 3.812198\n",
            "Wed Oct 31 09:13:14 2018 Train Epoch: 6 [102400/1281167 (8%)]\tLoss: 3.361775\n",
            "Wed Oct 31 09:13:42 2018 Train Epoch: 6 [108800/1281167 (8%)]\tLoss: 3.549704\n",
            "Wed Oct 31 09:14:09 2018 Train Epoch: 6 [115200/1281167 (9%)]\tLoss: 3.547339\n",
            "Wed Oct 31 09:14:36 2018 Train Epoch: 6 [121600/1281167 (9%)]\tLoss: 3.615996\n",
            "Wed Oct 31 09:15:03 2018 Train Epoch: 6 [128000/1281167 (10%)]\tLoss: 3.795989\n",
            "Wed Oct 31 09:15:31 2018 Train Epoch: 6 [134400/1281167 (10%)]\tLoss: 3.438323\n",
            "Wed Oct 31 09:15:58 2018 Train Epoch: 6 [140800/1281167 (11%)]\tLoss: 3.438891\n",
            "Wed Oct 31 09:16:25 2018 Train Epoch: 6 [147200/1281167 (11%)]\tLoss: 3.226418\n",
            "Wed Oct 31 09:16:52 2018 Train Epoch: 6 [153600/1281167 (12%)]\tLoss: 4.004684\n",
            "Wed Oct 31 09:17:20 2018 Train Epoch: 6 [160000/1281167 (12%)]\tLoss: 3.342772\n",
            "Wed Oct 31 09:17:47 2018 Train Epoch: 6 [166400/1281167 (13%)]\tLoss: 3.651286\n",
            "Wed Oct 31 09:18:14 2018 Train Epoch: 6 [172800/1281167 (13%)]\tLoss: 3.565635\n",
            "Wed Oct 31 09:18:41 2018 Train Epoch: 6 [179200/1281167 (14%)]\tLoss: 3.701150\n",
            "Wed Oct 31 09:19:08 2018 Train Epoch: 6 [185600/1281167 (14%)]\tLoss: 4.055166\n",
            "Wed Oct 31 09:19:36 2018 Train Epoch: 6 [192000/1281167 (15%)]\tLoss: 3.282292\n",
            "Wed Oct 31 09:20:03 2018 Train Epoch: 6 [198400/1281167 (15%)]\tLoss: 3.604694\n",
            "Wed Oct 31 09:20:30 2018 Train Epoch: 6 [204800/1281167 (16%)]\tLoss: 3.716573\n",
            "Wed Oct 31 09:20:57 2018 Train Epoch: 6 [211200/1281167 (16%)]\tLoss: 3.500867\n",
            "Wed Oct 31 09:21:25 2018 Train Epoch: 6 [217600/1281167 (17%)]\tLoss: 3.625528\n",
            "Wed Oct 31 09:21:52 2018 Train Epoch: 6 [224000/1281167 (17%)]\tLoss: 3.548027\n",
            "Wed Oct 31 09:22:19 2018 Train Epoch: 6 [230400/1281167 (18%)]\tLoss: 3.913979\n",
            "Wed Oct 31 09:22:46 2018 Train Epoch: 6 [236800/1281167 (18%)]\tLoss: 3.873807\n",
            "Wed Oct 31 09:23:13 2018 Train Epoch: 6 [243200/1281167 (19%)]\tLoss: 2.978422\n",
            "Wed Oct 31 09:23:41 2018 Train Epoch: 6 [249600/1281167 (19%)]\tLoss: 3.919194\n",
            "Wed Oct 31 09:24:08 2018 Train Epoch: 6 [256000/1281167 (20%)]\tLoss: 3.208000\n",
            "Wed Oct 31 09:24:35 2018 Train Epoch: 6 [262400/1281167 (20%)]\tLoss: 3.822938\n",
            "Wed Oct 31 09:25:02 2018 Train Epoch: 6 [268800/1281167 (21%)]\tLoss: 3.608070\n",
            "Wed Oct 31 09:25:30 2018 Train Epoch: 6 [275200/1281167 (21%)]\tLoss: 3.234947\n",
            "Wed Oct 31 09:25:57 2018 Train Epoch: 6 [281600/1281167 (22%)]\tLoss: 3.789605\n",
            "Wed Oct 31 09:26:24 2018 Train Epoch: 6 [288000/1281167 (22%)]\tLoss: 3.824780\n",
            "Wed Oct 31 09:26:51 2018 Train Epoch: 6 [294400/1281167 (23%)]\tLoss: 4.068612\n",
            "Wed Oct 31 09:27:18 2018 Train Epoch: 6 [300800/1281167 (23%)]\tLoss: 3.653069\n",
            "Wed Oct 31 09:27:46 2018 Train Epoch: 6 [307200/1281167 (24%)]\tLoss: 3.609340\n",
            "Wed Oct 31 09:28:13 2018 Train Epoch: 6 [313600/1281167 (24%)]\tLoss: 3.852462\n",
            "Wed Oct 31 09:28:40 2018 Train Epoch: 6 [320000/1281167 (25%)]\tLoss: 3.590703\n",
            "Wed Oct 31 09:29:07 2018 Train Epoch: 6 [326400/1281167 (25%)]\tLoss: 3.677120\n",
            "Wed Oct 31 09:29:35 2018 Train Epoch: 6 [332800/1281167 (26%)]\tLoss: 3.725976\n",
            "Wed Oct 31 09:30:02 2018 Train Epoch: 6 [339200/1281167 (26%)]\tLoss: 3.351391\n",
            "Wed Oct 31 09:30:29 2018 Train Epoch: 6 [345600/1281167 (27%)]\tLoss: 3.774094\n",
            "Wed Oct 31 09:30:56 2018 Train Epoch: 6 [352000/1281167 (27%)]\tLoss: 3.897582\n",
            "Wed Oct 31 09:31:24 2018 Train Epoch: 6 [358400/1281167 (28%)]\tLoss: 3.718173\n",
            "Wed Oct 31 09:31:51 2018 Train Epoch: 6 [364800/1281167 (28%)]\tLoss: 3.899631\n",
            "Wed Oct 31 09:32:18 2018 Train Epoch: 6 [371200/1281167 (29%)]\tLoss: 3.892999\n",
            "Wed Oct 31 09:32:45 2018 Train Epoch: 6 [377600/1281167 (29%)]\tLoss: 3.953700\n",
            "Wed Oct 31 09:33:13 2018 Train Epoch: 6 [384000/1281167 (30%)]\tLoss: 3.268744\n",
            "Wed Oct 31 09:33:40 2018 Train Epoch: 6 [390400/1281167 (30%)]\tLoss: 3.480588\n",
            "Wed Oct 31 09:34:07 2018 Train Epoch: 6 [396800/1281167 (31%)]\tLoss: 3.643031\n",
            "Wed Oct 31 09:34:34 2018 Train Epoch: 6 [403200/1281167 (31%)]\tLoss: 3.884360\n",
            "Wed Oct 31 09:35:02 2018 Train Epoch: 6 [409600/1281167 (32%)]\tLoss: 4.098322\n",
            "Wed Oct 31 09:35:29 2018 Train Epoch: 6 [416000/1281167 (32%)]\tLoss: 3.284799\n",
            "Wed Oct 31 09:35:56 2018 Train Epoch: 6 [422400/1281167 (33%)]\tLoss: 3.590051\n",
            "Wed Oct 31 09:36:23 2018 Train Epoch: 6 [428800/1281167 (33%)]\tLoss: 3.863134\n",
            "Wed Oct 31 09:36:51 2018 Train Epoch: 6 [435200/1281167 (34%)]\tLoss: 3.794927\n",
            "Wed Oct 31 09:37:18 2018 Train Epoch: 6 [441600/1281167 (34%)]\tLoss: 3.443143\n",
            "Wed Oct 31 09:37:45 2018 Train Epoch: 6 [448000/1281167 (35%)]\tLoss: 3.422443\n",
            "Wed Oct 31 09:38:12 2018 Train Epoch: 6 [454400/1281167 (35%)]\tLoss: 3.701305\n",
            "Wed Oct 31 09:38:40 2018 Train Epoch: 6 [460800/1281167 (36%)]\tLoss: 4.175907\n",
            "Wed Oct 31 09:39:07 2018 Train Epoch: 6 [467200/1281167 (36%)]\tLoss: 3.840529\n",
            "Wed Oct 31 09:39:34 2018 Train Epoch: 6 [473600/1281167 (37%)]\tLoss: 3.406824\n",
            "Wed Oct 31 09:40:01 2018 Train Epoch: 6 [480000/1281167 (37%)]\tLoss: 3.528995\n",
            "Wed Oct 31 09:40:29 2018 Train Epoch: 6 [486400/1281167 (38%)]\tLoss: 4.082585\n",
            "Wed Oct 31 09:40:56 2018 Train Epoch: 6 [492800/1281167 (38%)]\tLoss: 3.965349\n",
            "Wed Oct 31 09:41:23 2018 Train Epoch: 6 [499200/1281167 (39%)]\tLoss: 3.497454\n",
            "Wed Oct 31 09:41:50 2018 Train Epoch: 6 [505600/1281167 (39%)]\tLoss: 4.107153\n",
            "Wed Oct 31 09:42:18 2018 Train Epoch: 6 [512000/1281167 (40%)]\tLoss: 3.495643\n",
            "Wed Oct 31 09:42:45 2018 Train Epoch: 6 [518400/1281167 (40%)]\tLoss: 3.940609\n",
            "Wed Oct 31 09:43:12 2018 Train Epoch: 6 [524800/1281167 (41%)]\tLoss: 3.630313\n",
            "Wed Oct 31 09:43:39 2018 Train Epoch: 6 [531200/1281167 (41%)]\tLoss: 3.723855\n",
            "Wed Oct 31 09:44:06 2018 Train Epoch: 6 [537600/1281167 (42%)]\tLoss: 3.981381\n",
            "Wed Oct 31 09:44:34 2018 Train Epoch: 6 [544000/1281167 (42%)]\tLoss: 3.691033\n",
            "Wed Oct 31 09:45:01 2018 Train Epoch: 6 [550400/1281167 (43%)]\tLoss: 3.518087\n",
            "Wed Oct 31 09:45:28 2018 Train Epoch: 6 [556800/1281167 (43%)]\tLoss: 3.685529\n",
            "Wed Oct 31 09:45:55 2018 Train Epoch: 6 [563200/1281167 (44%)]\tLoss: 3.710716\n",
            "Wed Oct 31 09:46:23 2018 Train Epoch: 6 [569600/1281167 (44%)]\tLoss: 3.892761\n",
            "Wed Oct 31 09:46:50 2018 Train Epoch: 6 [576000/1281167 (45%)]\tLoss: 3.388467\n",
            "Wed Oct 31 09:47:17 2018 Train Epoch: 6 [582400/1281167 (45%)]\tLoss: 3.968388\n",
            "Wed Oct 31 09:47:44 2018 Train Epoch: 6 [588800/1281167 (46%)]\tLoss: 3.365240\n",
            "Wed Oct 31 09:48:11 2018 Train Epoch: 6 [595200/1281167 (46%)]\tLoss: 3.562561\n",
            "Wed Oct 31 09:48:39 2018 Train Epoch: 6 [601600/1281167 (47%)]\tLoss: 3.711284\n",
            "Wed Oct 31 09:49:06 2018 Train Epoch: 6 [608000/1281167 (47%)]\tLoss: 3.604193\n",
            "Wed Oct 31 09:49:33 2018 Train Epoch: 6 [614400/1281167 (48%)]\tLoss: 3.843020\n",
            "Wed Oct 31 09:50:00 2018 Train Epoch: 6 [620800/1281167 (48%)]\tLoss: 3.046591\n",
            "Wed Oct 31 09:50:28 2018 Train Epoch: 6 [627200/1281167 (49%)]\tLoss: 3.756298\n",
            "Wed Oct 31 09:50:55 2018 Train Epoch: 6 [633600/1281167 (49%)]\tLoss: 3.910006\n",
            "Wed Oct 31 09:51:22 2018 Train Epoch: 6 [640000/1281167 (50%)]\tLoss: 3.879124\n",
            "Wed Oct 31 09:51:49 2018 Train Epoch: 6 [646400/1281167 (50%)]\tLoss: 3.899254\n",
            "Wed Oct 31 09:52:17 2018 Train Epoch: 6 [652800/1281167 (51%)]\tLoss: 3.433361\n",
            "Wed Oct 31 09:52:44 2018 Train Epoch: 6 [659200/1281167 (51%)]\tLoss: 3.321560\n",
            "Wed Oct 31 09:53:11 2018 Train Epoch: 6 [665600/1281167 (52%)]\tLoss: 3.304922\n",
            "Wed Oct 31 09:53:38 2018 Train Epoch: 6 [672000/1281167 (52%)]\tLoss: 3.677128\n",
            "Wed Oct 31 09:54:05 2018 Train Epoch: 6 [678400/1281167 (53%)]\tLoss: 3.671202\n",
            "Wed Oct 31 09:54:33 2018 Train Epoch: 6 [684800/1281167 (53%)]\tLoss: 4.023563\n",
            "Wed Oct 31 09:55:00 2018 Train Epoch: 6 [691200/1281167 (54%)]\tLoss: 3.238153\n",
            "Wed Oct 31 09:55:27 2018 Train Epoch: 6 [697600/1281167 (54%)]\tLoss: 3.638137\n",
            "Wed Oct 31 09:55:54 2018 Train Epoch: 6 [704000/1281167 (55%)]\tLoss: 4.217826\n",
            "Wed Oct 31 09:56:21 2018 Train Epoch: 6 [710400/1281167 (55%)]\tLoss: 3.591017\n",
            "Wed Oct 31 09:56:49 2018 Train Epoch: 6 [716800/1281167 (56%)]\tLoss: 3.880085\n",
            "Wed Oct 31 09:57:16 2018 Train Epoch: 6 [723200/1281167 (56%)]\tLoss: 3.425686\n",
            "Wed Oct 31 09:57:43 2018 Train Epoch: 6 [729600/1281167 (57%)]\tLoss: 3.792477\n",
            "Wed Oct 31 09:58:10 2018 Train Epoch: 6 [736000/1281167 (57%)]\tLoss: 3.226887\n",
            "Wed Oct 31 09:58:38 2018 Train Epoch: 6 [742400/1281167 (58%)]\tLoss: 3.981929\n",
            "Wed Oct 31 09:59:05 2018 Train Epoch: 6 [748800/1281167 (58%)]\tLoss: 3.914528\n",
            "Wed Oct 31 09:59:32 2018 Train Epoch: 6 [755200/1281167 (59%)]\tLoss: 3.584363\n",
            "Wed Oct 31 09:59:59 2018 Train Epoch: 6 [761600/1281167 (59%)]\tLoss: 3.935156\n",
            "Wed Oct 31 10:00:27 2018 Train Epoch: 6 [768000/1281167 (60%)]\tLoss: 3.086626\n",
            "Wed Oct 31 10:00:54 2018 Train Epoch: 6 [774400/1281167 (60%)]\tLoss: 3.694550\n",
            "Wed Oct 31 10:01:21 2018 Train Epoch: 6 [780800/1281167 (61%)]\tLoss: 3.546818\n",
            "Wed Oct 31 10:01:48 2018 Train Epoch: 6 [787200/1281167 (61%)]\tLoss: 3.763178\n",
            "Wed Oct 31 10:02:15 2018 Train Epoch: 6 [793600/1281167 (62%)]\tLoss: 3.280631\n",
            "Wed Oct 31 10:02:43 2018 Train Epoch: 6 [800000/1281167 (62%)]\tLoss: 3.936458\n",
            "Wed Oct 31 10:03:10 2018 Train Epoch: 6 [806400/1281167 (63%)]\tLoss: 3.427894\n",
            "Wed Oct 31 10:03:37 2018 Train Epoch: 6 [812800/1281167 (63%)]\tLoss: 3.130563\n",
            "Wed Oct 31 10:04:04 2018 Train Epoch: 6 [819200/1281167 (64%)]\tLoss: 3.811279\n",
            "Wed Oct 31 10:04:32 2018 Train Epoch: 6 [825600/1281167 (64%)]\tLoss: 3.150766\n",
            "Wed Oct 31 10:04:59 2018 Train Epoch: 6 [832000/1281167 (65%)]\tLoss: 3.662810\n",
            "Wed Oct 31 10:05:26 2018 Train Epoch: 6 [838400/1281167 (65%)]\tLoss: 3.251112\n",
            "Wed Oct 31 10:05:53 2018 Train Epoch: 6 [844800/1281167 (66%)]\tLoss: 3.957698\n",
            "Wed Oct 31 10:06:21 2018 Train Epoch: 6 [851200/1281167 (66%)]\tLoss: 3.525456\n",
            "Wed Oct 31 10:06:48 2018 Train Epoch: 6 [857600/1281167 (67%)]\tLoss: 3.376877\n",
            "Wed Oct 31 10:07:15 2018 Train Epoch: 6 [864000/1281167 (67%)]\tLoss: 3.459262\n",
            "Wed Oct 31 10:07:42 2018 Train Epoch: 6 [870400/1281167 (68%)]\tLoss: 3.732215\n",
            "Wed Oct 31 10:08:10 2018 Train Epoch: 6 [876800/1281167 (68%)]\tLoss: 3.400580\n",
            "Wed Oct 31 10:08:37 2018 Train Epoch: 6 [883200/1281167 (69%)]\tLoss: 3.642145\n",
            "Wed Oct 31 10:09:04 2018 Train Epoch: 6 [889600/1281167 (69%)]\tLoss: 4.493048\n",
            "Wed Oct 31 10:09:31 2018 Train Epoch: 6 [896000/1281167 (70%)]\tLoss: 3.009690\n",
            "Wed Oct 31 10:09:59 2018 Train Epoch: 6 [902400/1281167 (70%)]\tLoss: 3.346170\n",
            "Wed Oct 31 10:10:26 2018 Train Epoch: 6 [908800/1281167 (71%)]\tLoss: 3.641034\n",
            "Wed Oct 31 10:10:53 2018 Train Epoch: 6 [915200/1281167 (71%)]\tLoss: 4.052330\n",
            "Wed Oct 31 10:11:20 2018 Train Epoch: 6 [921600/1281167 (72%)]\tLoss: 3.055544\n",
            "Wed Oct 31 10:11:48 2018 Train Epoch: 6 [928000/1281167 (72%)]\tLoss: 3.133114\n",
            "Wed Oct 31 10:12:15 2018 Train Epoch: 6 [934400/1281167 (73%)]\tLoss: 3.529876\n",
            "Wed Oct 31 10:12:42 2018 Train Epoch: 6 [940800/1281167 (73%)]\tLoss: 3.523365\n",
            "Wed Oct 31 10:13:09 2018 Train Epoch: 6 [947200/1281167 (74%)]\tLoss: 3.142388\n",
            "Wed Oct 31 10:13:36 2018 Train Epoch: 6 [953600/1281167 (74%)]\tLoss: 3.596748\n",
            "Wed Oct 31 10:14:04 2018 Train Epoch: 6 [960000/1281167 (75%)]\tLoss: 3.811194\n",
            "Wed Oct 31 10:14:31 2018 Train Epoch: 6 [966400/1281167 (75%)]\tLoss: 3.495731\n",
            "Wed Oct 31 10:14:58 2018 Train Epoch: 6 [972800/1281167 (76%)]\tLoss: 3.287536\n",
            "Wed Oct 31 10:15:25 2018 Train Epoch: 6 [979200/1281167 (76%)]\tLoss: 3.559608\n",
            "Wed Oct 31 10:15:53 2018 Train Epoch: 6 [985600/1281167 (77%)]\tLoss: 3.342974\n",
            "Wed Oct 31 10:16:20 2018 Train Epoch: 6 [992000/1281167 (77%)]\tLoss: 3.750095\n",
            "Wed Oct 31 10:16:47 2018 Train Epoch: 6 [998400/1281167 (78%)]\tLoss: 3.578892\n",
            "Wed Oct 31 10:17:14 2018 Train Epoch: 6 [1004800/1281167 (78%)]\tLoss: 3.618154\n",
            "Wed Oct 31 10:17:42 2018 Train Epoch: 6 [1011200/1281167 (79%)]\tLoss: 3.539620\n",
            "Wed Oct 31 10:18:09 2018 Train Epoch: 6 [1017600/1281167 (79%)]\tLoss: 3.375284\n",
            "Wed Oct 31 10:18:36 2018 Train Epoch: 6 [1024000/1281167 (80%)]\tLoss: 3.569076\n",
            "Wed Oct 31 10:19:04 2018 Train Epoch: 6 [1030400/1281167 (80%)]\tLoss: 3.734175\n",
            "Wed Oct 31 10:19:31 2018 Train Epoch: 6 [1036800/1281167 (81%)]\tLoss: 3.643645\n",
            "Wed Oct 31 10:19:58 2018 Train Epoch: 6 [1043200/1281167 (81%)]\tLoss: 3.361102\n",
            "Wed Oct 31 10:20:25 2018 Train Epoch: 6 [1049600/1281167 (82%)]\tLoss: 3.561334\n",
            "Wed Oct 31 10:20:52 2018 Train Epoch: 6 [1056000/1281167 (82%)]\tLoss: 3.778902\n",
            "Wed Oct 31 10:21:20 2018 Train Epoch: 6 [1062400/1281167 (83%)]\tLoss: 2.984997\n",
            "Wed Oct 31 10:21:47 2018 Train Epoch: 6 [1068800/1281167 (83%)]\tLoss: 3.611741\n",
            "Wed Oct 31 10:22:14 2018 Train Epoch: 6 [1075200/1281167 (84%)]\tLoss: 3.630272\n",
            "Wed Oct 31 10:22:41 2018 Train Epoch: 6 [1081600/1281167 (84%)]\tLoss: 3.772247\n",
            "Wed Oct 31 10:23:09 2018 Train Epoch: 6 [1088000/1281167 (85%)]\tLoss: 3.400781\n",
            "Wed Oct 31 10:23:36 2018 Train Epoch: 6 [1094400/1281167 (85%)]\tLoss: 3.059166\n",
            "Wed Oct 31 10:24:03 2018 Train Epoch: 6 [1100800/1281167 (86%)]\tLoss: 3.127288\n",
            "Wed Oct 31 10:24:30 2018 Train Epoch: 6 [1107200/1281167 (86%)]\tLoss: 3.634907\n",
            "Wed Oct 31 10:24:58 2018 Train Epoch: 6 [1113600/1281167 (87%)]\tLoss: 3.472731\n",
            "Wed Oct 31 10:25:25 2018 Train Epoch: 6 [1120000/1281167 (87%)]\tLoss: 3.549888\n",
            "Wed Oct 31 10:25:52 2018 Train Epoch: 6 [1126400/1281167 (88%)]\tLoss: 3.617376\n",
            "Wed Oct 31 10:26:19 2018 Train Epoch: 6 [1132800/1281167 (88%)]\tLoss: 3.358054\n",
            "Wed Oct 31 10:26:47 2018 Train Epoch: 6 [1139200/1281167 (89%)]\tLoss: 3.719317\n",
            "Wed Oct 31 10:27:14 2018 Train Epoch: 6 [1145600/1281167 (89%)]\tLoss: 3.143483\n",
            "Wed Oct 31 10:27:41 2018 Train Epoch: 6 [1152000/1281167 (90%)]\tLoss: 3.347802\n",
            "Wed Oct 31 10:28:08 2018 Train Epoch: 6 [1158400/1281167 (90%)]\tLoss: 3.716458\n",
            "Wed Oct 31 10:28:36 2018 Train Epoch: 6 [1164800/1281167 (91%)]\tLoss: 3.077083\n",
            "Wed Oct 31 10:29:03 2018 Train Epoch: 6 [1171200/1281167 (91%)]\tLoss: 3.930660\n",
            "Wed Oct 31 10:29:30 2018 Train Epoch: 6 [1177600/1281167 (92%)]\tLoss: 3.522063\n",
            "Wed Oct 31 10:29:57 2018 Train Epoch: 6 [1184000/1281167 (92%)]\tLoss: 4.505267\n",
            "Wed Oct 31 10:30:25 2018 Train Epoch: 6 [1190400/1281167 (93%)]\tLoss: 3.539620\n",
            "Wed Oct 31 10:30:52 2018 Train Epoch: 6 [1196800/1281167 (93%)]\tLoss: 3.798122\n",
            "Wed Oct 31 10:31:19 2018 Train Epoch: 6 [1203200/1281167 (94%)]\tLoss: 3.688102\n",
            "Wed Oct 31 10:31:46 2018 Train Epoch: 6 [1209600/1281167 (94%)]\tLoss: 3.503125\n",
            "Wed Oct 31 10:32:14 2018 Train Epoch: 6 [1216000/1281167 (95%)]\tLoss: 3.871724\n",
            "Wed Oct 31 10:32:41 2018 Train Epoch: 6 [1222400/1281167 (95%)]\tLoss: 3.285133\n",
            "Wed Oct 31 10:33:08 2018 Train Epoch: 6 [1228800/1281167 (96%)]\tLoss: 3.857512\n",
            "Wed Oct 31 10:33:35 2018 Train Epoch: 6 [1235200/1281167 (96%)]\tLoss: 3.682172\n",
            "Wed Oct 31 10:34:03 2018 Train Epoch: 6 [1241600/1281167 (97%)]\tLoss: 4.101961\n",
            "Wed Oct 31 10:34:30 2018 Train Epoch: 6 [1248000/1281167 (97%)]\tLoss: 3.677637\n",
            "Wed Oct 31 10:34:57 2018 Train Epoch: 6 [1254400/1281167 (98%)]\tLoss: 3.673348\n",
            "Wed Oct 31 10:35:24 2018 Train Epoch: 6 [1260800/1281167 (98%)]\tLoss: 3.422267\n",
            "Wed Oct 31 10:35:52 2018 Train Epoch: 6 [1267200/1281167 (99%)]\tLoss: 4.124959\n",
            "Wed Oct 31 10:36:19 2018 Train Epoch: 6 [1273600/1281167 (99%)]\tLoss: 3.194051\n",
            "Wed Oct 31 10:36:46 2018 Train Epoch: 6 [1280000/1281167 (100%)]\tLoss: 3.162050\n",
            "\n",
            "Test set: Average loss: 3.4220, Accuracy: 14233/50000 (28%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/006.pt\n",
            "\n",
            "Wed Oct 31 10:40:42 2018 Train Epoch: 7 [0/1281167 (0%)]\tLoss: 3.574609\n",
            "Wed Oct 31 10:41:09 2018 Train Epoch: 7 [6400/1281167 (0%)]\tLoss: 3.782685\n",
            "Wed Oct 31 10:41:36 2018 Train Epoch: 7 [12800/1281167 (1%)]\tLoss: 3.605440\n",
            "Wed Oct 31 10:42:03 2018 Train Epoch: 7 [19200/1281167 (1%)]\tLoss: 3.296835\n",
            "Wed Oct 31 10:42:31 2018 Train Epoch: 7 [25600/1281167 (2%)]\tLoss: 4.014095\n",
            "Wed Oct 31 10:42:58 2018 Train Epoch: 7 [32000/1281167 (2%)]\tLoss: 3.521125\n",
            "Wed Oct 31 10:43:25 2018 Train Epoch: 7 [38400/1281167 (3%)]\tLoss: 3.592564\n",
            "Wed Oct 31 10:43:52 2018 Train Epoch: 7 [44800/1281167 (3%)]\tLoss: 3.418850\n",
            "Wed Oct 31 10:44:20 2018 Train Epoch: 7 [51200/1281167 (4%)]\tLoss: 3.639596\n",
            "Wed Oct 31 10:44:47 2018 Train Epoch: 7 [57600/1281167 (4%)]\tLoss: 3.402408\n",
            "Wed Oct 31 10:45:14 2018 Train Epoch: 7 [64000/1281167 (5%)]\tLoss: 3.653500\n",
            "Wed Oct 31 10:45:41 2018 Train Epoch: 7 [70400/1281167 (5%)]\tLoss: 3.943861\n",
            "Wed Oct 31 10:46:09 2018 Train Epoch: 7 [76800/1281167 (6%)]\tLoss: 3.402269\n",
            "Wed Oct 31 10:46:36 2018 Train Epoch: 7 [83200/1281167 (6%)]\tLoss: 3.391071\n",
            "Wed Oct 31 10:47:03 2018 Train Epoch: 7 [89600/1281167 (7%)]\tLoss: 3.214161\n",
            "Wed Oct 31 10:47:30 2018 Train Epoch: 7 [96000/1281167 (7%)]\tLoss: 3.409972\n",
            "Wed Oct 31 10:47:58 2018 Train Epoch: 7 [102400/1281167 (8%)]\tLoss: 3.782243\n",
            "Wed Oct 31 10:48:25 2018 Train Epoch: 7 [108800/1281167 (8%)]\tLoss: 3.168607\n",
            "Wed Oct 31 10:48:52 2018 Train Epoch: 7 [115200/1281167 (9%)]\tLoss: 3.540640\n",
            "Wed Oct 31 10:49:19 2018 Train Epoch: 7 [121600/1281167 (9%)]\tLoss: 3.695754\n",
            "Wed Oct 31 10:49:47 2018 Train Epoch: 7 [128000/1281167 (10%)]\tLoss: 3.729088\n",
            "Wed Oct 31 10:50:14 2018 Train Epoch: 7 [134400/1281167 (10%)]\tLoss: 3.744215\n",
            "Wed Oct 31 10:50:41 2018 Train Epoch: 7 [140800/1281167 (11%)]\tLoss: 3.417870\n",
            "Wed Oct 31 10:51:08 2018 Train Epoch: 7 [147200/1281167 (11%)]\tLoss: 4.056567\n",
            "Wed Oct 31 10:51:36 2018 Train Epoch: 7 [153600/1281167 (12%)]\tLoss: 3.627891\n",
            "Wed Oct 31 10:52:03 2018 Train Epoch: 7 [160000/1281167 (12%)]\tLoss: 3.570171\n",
            "Wed Oct 31 10:52:30 2018 Train Epoch: 7 [166400/1281167 (13%)]\tLoss: 3.523852\n",
            "Wed Oct 31 10:52:57 2018 Train Epoch: 7 [172800/1281167 (13%)]\tLoss: 3.667150\n",
            "Wed Oct 31 10:53:24 2018 Train Epoch: 7 [179200/1281167 (14%)]\tLoss: 3.658774\n",
            "Wed Oct 31 10:53:52 2018 Train Epoch: 7 [185600/1281167 (14%)]\tLoss: 3.524496\n",
            "Wed Oct 31 10:54:19 2018 Train Epoch: 7 [192000/1281167 (15%)]\tLoss: 3.854326\n",
            "Wed Oct 31 10:54:46 2018 Train Epoch: 7 [198400/1281167 (15%)]\tLoss: 3.721354\n",
            "Wed Oct 31 10:55:13 2018 Train Epoch: 7 [204800/1281167 (16%)]\tLoss: 3.640320\n",
            "Wed Oct 31 10:55:40 2018 Train Epoch: 7 [211200/1281167 (16%)]\tLoss: 3.710178\n",
            "Wed Oct 31 10:56:08 2018 Train Epoch: 7 [217600/1281167 (17%)]\tLoss: 4.121526\n",
            "Wed Oct 31 10:56:35 2018 Train Epoch: 7 [224000/1281167 (17%)]\tLoss: 3.082476\n",
            "Wed Oct 31 10:57:02 2018 Train Epoch: 7 [230400/1281167 (18%)]\tLoss: 3.863782\n",
            "Wed Oct 31 10:57:29 2018 Train Epoch: 7 [236800/1281167 (18%)]\tLoss: 3.914286\n",
            "Wed Oct 31 10:57:57 2018 Train Epoch: 7 [243200/1281167 (19%)]\tLoss: 3.373304\n",
            "Wed Oct 31 10:58:24 2018 Train Epoch: 7 [249600/1281167 (19%)]\tLoss: 3.353774\n",
            "Wed Oct 31 10:58:51 2018 Train Epoch: 7 [256000/1281167 (20%)]\tLoss: 3.446486\n",
            "Wed Oct 31 10:59:18 2018 Train Epoch: 7 [262400/1281167 (20%)]\tLoss: 3.760094\n",
            "Wed Oct 31 10:59:45 2018 Train Epoch: 7 [268800/1281167 (21%)]\tLoss: 3.703568\n",
            "Wed Oct 31 11:00:13 2018 Train Epoch: 7 [275200/1281167 (21%)]\tLoss: 3.482350\n",
            "Wed Oct 31 11:00:40 2018 Train Epoch: 7 [281600/1281167 (22%)]\tLoss: 3.679634\n",
            "Wed Oct 31 11:01:07 2018 Train Epoch: 7 [288000/1281167 (22%)]\tLoss: 3.649610\n",
            "Wed Oct 31 11:01:34 2018 Train Epoch: 7 [294400/1281167 (23%)]\tLoss: 3.550124\n",
            "Wed Oct 31 11:02:01 2018 Train Epoch: 7 [300800/1281167 (23%)]\tLoss: 3.827312\n",
            "Wed Oct 31 11:02:29 2018 Train Epoch: 7 [307200/1281167 (24%)]\tLoss: 3.111854\n",
            "Wed Oct 31 11:02:56 2018 Train Epoch: 7 [313600/1281167 (24%)]\tLoss: 3.872138\n",
            "Wed Oct 31 11:03:23 2018 Train Epoch: 7 [320000/1281167 (25%)]\tLoss: 3.465274\n",
            "Wed Oct 31 11:03:50 2018 Train Epoch: 7 [326400/1281167 (25%)]\tLoss: 3.795572\n",
            "Wed Oct 31 11:04:18 2018 Train Epoch: 7 [332800/1281167 (26%)]\tLoss: 3.808808\n",
            "Wed Oct 31 11:04:45 2018 Train Epoch: 7 [339200/1281167 (26%)]\tLoss: 3.687703\n",
            "Wed Oct 31 11:05:12 2018 Train Epoch: 7 [345600/1281167 (27%)]\tLoss: 3.757665\n",
            "Wed Oct 31 11:05:39 2018 Train Epoch: 7 [352000/1281167 (27%)]\tLoss: 3.828658\n",
            "Wed Oct 31 11:06:07 2018 Train Epoch: 7 [358400/1281167 (28%)]\tLoss: 3.703007\n",
            "Wed Oct 31 11:06:34 2018 Train Epoch: 7 [364800/1281167 (28%)]\tLoss: 3.722648\n",
            "Wed Oct 31 11:07:01 2018 Train Epoch: 7 [371200/1281167 (29%)]\tLoss: 3.682805\n",
            "Wed Oct 31 11:07:28 2018 Train Epoch: 7 [377600/1281167 (29%)]\tLoss: 4.186110\n",
            "Wed Oct 31 11:07:56 2018 Train Epoch: 7 [384000/1281167 (30%)]\tLoss: 3.683579\n",
            "Wed Oct 31 11:08:23 2018 Train Epoch: 7 [390400/1281167 (30%)]\tLoss: 3.985221\n",
            "Wed Oct 31 11:08:50 2018 Train Epoch: 7 [396800/1281167 (31%)]\tLoss: 3.615074\n",
            "Wed Oct 31 11:09:17 2018 Train Epoch: 7 [403200/1281167 (31%)]\tLoss: 3.762092\n",
            "Wed Oct 31 11:09:44 2018 Train Epoch: 7 [409600/1281167 (32%)]\tLoss: 3.786581\n",
            "Wed Oct 31 11:10:12 2018 Train Epoch: 7 [416000/1281167 (32%)]\tLoss: 3.587226\n",
            "Wed Oct 31 11:10:39 2018 Train Epoch: 7 [422400/1281167 (33%)]\tLoss: 3.914892\n",
            "Wed Oct 31 11:11:06 2018 Train Epoch: 7 [428800/1281167 (33%)]\tLoss: 3.932277\n",
            "Wed Oct 31 11:11:33 2018 Train Epoch: 7 [435200/1281167 (34%)]\tLoss: 3.559196\n",
            "Wed Oct 31 11:12:00 2018 Train Epoch: 7 [441600/1281167 (34%)]\tLoss: 3.426256\n",
            "Wed Oct 31 11:12:28 2018 Train Epoch: 7 [448000/1281167 (35%)]\tLoss: 3.575000\n",
            "Wed Oct 31 11:12:55 2018 Train Epoch: 7 [454400/1281167 (35%)]\tLoss: 3.882372\n",
            "Wed Oct 31 11:13:22 2018 Train Epoch: 7 [460800/1281167 (36%)]\tLoss: 3.689487\n",
            "Wed Oct 31 11:13:49 2018 Train Epoch: 7 [467200/1281167 (36%)]\tLoss: 3.477748\n",
            "Wed Oct 31 11:14:17 2018 Train Epoch: 7 [473600/1281167 (37%)]\tLoss: 3.463614\n",
            "Wed Oct 31 11:14:44 2018 Train Epoch: 7 [480000/1281167 (37%)]\tLoss: 3.343728\n",
            "Wed Oct 31 11:15:11 2018 Train Epoch: 7 [486400/1281167 (38%)]\tLoss: 3.579619\n",
            "Wed Oct 31 11:15:38 2018 Train Epoch: 7 [492800/1281167 (38%)]\tLoss: 3.521527\n",
            "Wed Oct 31 11:16:06 2018 Train Epoch: 7 [499200/1281167 (39%)]\tLoss: 3.653267\n",
            "Wed Oct 31 11:16:33 2018 Train Epoch: 7 [505600/1281167 (39%)]\tLoss: 3.674407\n",
            "Wed Oct 31 11:17:00 2018 Train Epoch: 7 [512000/1281167 (40%)]\tLoss: 3.380055\n",
            "Wed Oct 31 11:17:27 2018 Train Epoch: 7 [518400/1281167 (40%)]\tLoss: 3.630976\n",
            "Wed Oct 31 11:17:55 2018 Train Epoch: 7 [524800/1281167 (41%)]\tLoss: 3.511951\n",
            "Wed Oct 31 11:18:22 2018 Train Epoch: 7 [531200/1281167 (41%)]\tLoss: 3.412867\n",
            "Wed Oct 31 11:18:49 2018 Train Epoch: 7 [537600/1281167 (42%)]\tLoss: 3.774465\n",
            "Wed Oct 31 11:19:16 2018 Train Epoch: 7 [544000/1281167 (42%)]\tLoss: 3.678882\n",
            "Wed Oct 31 11:19:43 2018 Train Epoch: 7 [550400/1281167 (43%)]\tLoss: 3.570873\n",
            "Wed Oct 31 11:20:11 2018 Train Epoch: 7 [556800/1281167 (43%)]\tLoss: 3.356631\n",
            "Wed Oct 31 11:20:38 2018 Train Epoch: 7 [563200/1281167 (44%)]\tLoss: 3.283382\n",
            "Wed Oct 31 11:21:05 2018 Train Epoch: 7 [569600/1281167 (44%)]\tLoss: 3.872715\n",
            "Wed Oct 31 11:21:32 2018 Train Epoch: 7 [576000/1281167 (45%)]\tLoss: 3.841480\n",
            "Wed Oct 31 11:22:00 2018 Train Epoch: 7 [582400/1281167 (45%)]\tLoss: 3.375894\n",
            "Wed Oct 31 11:22:27 2018 Train Epoch: 7 [588800/1281167 (46%)]\tLoss: 3.390359\n",
            "Wed Oct 31 11:22:54 2018 Train Epoch: 7 [595200/1281167 (46%)]\tLoss: 3.466522\n",
            "Wed Oct 31 11:23:21 2018 Train Epoch: 7 [601600/1281167 (47%)]\tLoss: 3.879114\n",
            "Wed Oct 31 11:23:48 2018 Train Epoch: 7 [608000/1281167 (47%)]\tLoss: 2.936462\n",
            "Wed Oct 31 11:24:16 2018 Train Epoch: 7 [614400/1281167 (48%)]\tLoss: 4.006282\n",
            "Wed Oct 31 11:24:43 2018 Train Epoch: 7 [620800/1281167 (48%)]\tLoss: 3.299981\n",
            "Wed Oct 31 11:25:10 2018 Train Epoch: 7 [627200/1281167 (49%)]\tLoss: 3.553138\n",
            "Wed Oct 31 11:25:37 2018 Train Epoch: 7 [633600/1281167 (49%)]\tLoss: 3.815198\n",
            "Wed Oct 31 11:26:05 2018 Train Epoch: 7 [640000/1281167 (50%)]\tLoss: 3.875936\n",
            "Wed Oct 31 11:26:32 2018 Train Epoch: 7 [646400/1281167 (50%)]\tLoss: 3.536117\n",
            "Wed Oct 31 11:26:59 2018 Train Epoch: 7 [652800/1281167 (51%)]\tLoss: 3.653536\n",
            "Wed Oct 31 11:27:26 2018 Train Epoch: 7 [659200/1281167 (51%)]\tLoss: 3.420323\n",
            "Wed Oct 31 11:27:53 2018 Train Epoch: 7 [665600/1281167 (52%)]\tLoss: 3.061448\n",
            "Wed Oct 31 11:28:21 2018 Train Epoch: 7 [672000/1281167 (52%)]\tLoss: 3.671613\n",
            "Wed Oct 31 11:28:48 2018 Train Epoch: 7 [678400/1281167 (53%)]\tLoss: 3.396163\n",
            "Wed Oct 31 11:29:15 2018 Train Epoch: 7 [684800/1281167 (53%)]\tLoss: 3.868356\n",
            "Wed Oct 31 11:29:42 2018 Train Epoch: 7 [691200/1281167 (54%)]\tLoss: 4.410626\n",
            "Wed Oct 31 11:30:10 2018 Train Epoch: 7 [697600/1281167 (54%)]\tLoss: 4.024504\n",
            "Wed Oct 31 11:30:37 2018 Train Epoch: 7 [704000/1281167 (55%)]\tLoss: 3.350546\n",
            "Wed Oct 31 11:31:04 2018 Train Epoch: 7 [710400/1281167 (55%)]\tLoss: 3.212697\n",
            "Wed Oct 31 11:31:31 2018 Train Epoch: 7 [716800/1281167 (56%)]\tLoss: 3.705352\n",
            "Wed Oct 31 11:31:58 2018 Train Epoch: 7 [723200/1281167 (56%)]\tLoss: 3.594148\n",
            "Wed Oct 31 11:32:26 2018 Train Epoch: 7 [729600/1281167 (57%)]\tLoss: 3.963394\n",
            "Wed Oct 31 11:32:53 2018 Train Epoch: 7 [736000/1281167 (57%)]\tLoss: 3.714083\n",
            "Wed Oct 31 11:33:20 2018 Train Epoch: 7 [742400/1281167 (58%)]\tLoss: 3.734881\n",
            "Wed Oct 31 11:33:47 2018 Train Epoch: 7 [748800/1281167 (58%)]\tLoss: 3.541002\n",
            "Wed Oct 31 11:34:15 2018 Train Epoch: 7 [755200/1281167 (59%)]\tLoss: 3.707800\n",
            "Wed Oct 31 11:34:42 2018 Train Epoch: 7 [761600/1281167 (59%)]\tLoss: 3.773157\n",
            "Wed Oct 31 11:35:09 2018 Train Epoch: 7 [768000/1281167 (60%)]\tLoss: 3.910067\n",
            "Wed Oct 31 11:35:36 2018 Train Epoch: 7 [774400/1281167 (60%)]\tLoss: 3.990435\n",
            "Wed Oct 31 11:36:04 2018 Train Epoch: 7 [780800/1281167 (61%)]\tLoss: 3.344418\n",
            "Wed Oct 31 11:36:31 2018 Train Epoch: 7 [787200/1281167 (61%)]\tLoss: 3.439010\n",
            "Wed Oct 31 11:36:58 2018 Train Epoch: 7 [793600/1281167 (62%)]\tLoss: 3.773558\n",
            "Wed Oct 31 11:37:25 2018 Train Epoch: 7 [800000/1281167 (62%)]\tLoss: 3.328609\n",
            "Wed Oct 31 11:37:53 2018 Train Epoch: 7 [806400/1281167 (63%)]\tLoss: 3.526011\n",
            "Wed Oct 31 11:38:20 2018 Train Epoch: 7 [812800/1281167 (63%)]\tLoss: 3.845032\n",
            "Wed Oct 31 11:38:47 2018 Train Epoch: 7 [819200/1281167 (64%)]\tLoss: 3.719517\n",
            "Wed Oct 31 11:39:14 2018 Train Epoch: 7 [825600/1281167 (64%)]\tLoss: 3.769753\n",
            "Wed Oct 31 11:39:41 2018 Train Epoch: 7 [832000/1281167 (65%)]\tLoss: 3.628669\n",
            "Wed Oct 31 11:40:09 2018 Train Epoch: 7 [838400/1281167 (65%)]\tLoss: 3.531739\n",
            "Wed Oct 31 11:40:36 2018 Train Epoch: 7 [844800/1281167 (66%)]\tLoss: 3.824209\n",
            "Wed Oct 31 11:41:03 2018 Train Epoch: 7 [851200/1281167 (66%)]\tLoss: 3.532016\n",
            "Wed Oct 31 11:41:30 2018 Train Epoch: 7 [857600/1281167 (67%)]\tLoss: 3.855236\n",
            "Wed Oct 31 11:41:57 2018 Train Epoch: 7 [864000/1281167 (67%)]\tLoss: 4.007735\n",
            "Wed Oct 31 11:42:25 2018 Train Epoch: 7 [870400/1281167 (68%)]\tLoss: 3.882183\n",
            "Wed Oct 31 11:42:52 2018 Train Epoch: 7 [876800/1281167 (68%)]\tLoss: 3.161464\n",
            "Wed Oct 31 11:43:19 2018 Train Epoch: 7 [883200/1281167 (69%)]\tLoss: 3.655802\n",
            "Wed Oct 31 11:43:46 2018 Train Epoch: 7 [889600/1281167 (69%)]\tLoss: 2.971472\n",
            "Wed Oct 31 11:44:14 2018 Train Epoch: 7 [896000/1281167 (70%)]\tLoss: 3.228123\n",
            "Wed Oct 31 11:44:41 2018 Train Epoch: 7 [902400/1281167 (70%)]\tLoss: 3.580725\n",
            "Wed Oct 31 11:45:08 2018 Train Epoch: 7 [908800/1281167 (71%)]\tLoss: 3.775481\n",
            "Wed Oct 31 11:45:35 2018 Train Epoch: 7 [915200/1281167 (71%)]\tLoss: 3.009262\n",
            "Wed Oct 31 11:46:03 2018 Train Epoch: 7 [921600/1281167 (72%)]\tLoss: 3.793839\n",
            "Wed Oct 31 11:46:30 2018 Train Epoch: 7 [928000/1281167 (72%)]\tLoss: 3.259990\n",
            "Wed Oct 31 11:46:57 2018 Train Epoch: 7 [934400/1281167 (73%)]\tLoss: 3.489555\n",
            "Wed Oct 31 11:47:24 2018 Train Epoch: 7 [940800/1281167 (73%)]\tLoss: 4.154449\n",
            "Wed Oct 31 11:47:52 2018 Train Epoch: 7 [947200/1281167 (74%)]\tLoss: 3.924188\n",
            "Wed Oct 31 11:48:19 2018 Train Epoch: 7 [953600/1281167 (74%)]\tLoss: 3.597032\n",
            "Wed Oct 31 11:48:46 2018 Train Epoch: 7 [960000/1281167 (75%)]\tLoss: 3.505804\n",
            "Wed Oct 31 11:49:13 2018 Train Epoch: 7 [966400/1281167 (75%)]\tLoss: 3.530318\n",
            "Wed Oct 31 11:49:40 2018 Train Epoch: 7 [972800/1281167 (76%)]\tLoss: 3.586216\n",
            "Wed Oct 31 11:50:08 2018 Train Epoch: 7 [979200/1281167 (76%)]\tLoss: 3.683745\n",
            "Wed Oct 31 11:50:35 2018 Train Epoch: 7 [985600/1281167 (77%)]\tLoss: 3.946539\n",
            "Wed Oct 31 11:51:02 2018 Train Epoch: 7 [992000/1281167 (77%)]\tLoss: 3.857852\n",
            "Wed Oct 31 11:51:29 2018 Train Epoch: 7 [998400/1281167 (78%)]\tLoss: 3.269187\n",
            "Wed Oct 31 11:51:57 2018 Train Epoch: 7 [1004800/1281167 (78%)]\tLoss: 3.582433\n",
            "Wed Oct 31 11:52:24 2018 Train Epoch: 7 [1011200/1281167 (79%)]\tLoss: 3.469680\n",
            "Wed Oct 31 11:52:51 2018 Train Epoch: 7 [1017600/1281167 (79%)]\tLoss: 3.532643\n",
            "Wed Oct 31 11:53:18 2018 Train Epoch: 7 [1024000/1281167 (80%)]\tLoss: 3.638423\n",
            "Wed Oct 31 11:53:46 2018 Train Epoch: 7 [1030400/1281167 (80%)]\tLoss: 3.568405\n",
            "Wed Oct 31 11:54:13 2018 Train Epoch: 7 [1036800/1281167 (81%)]\tLoss: 4.017618\n",
            "Wed Oct 31 11:54:40 2018 Train Epoch: 7 [1043200/1281167 (81%)]\tLoss: 3.274173\n",
            "Wed Oct 31 11:55:07 2018 Train Epoch: 7 [1049600/1281167 (82%)]\tLoss: 3.685265\n",
            "Wed Oct 31 11:55:34 2018 Train Epoch: 7 [1056000/1281167 (82%)]\tLoss: 4.000566\n",
            "Wed Oct 31 11:56:02 2018 Train Epoch: 7 [1062400/1281167 (83%)]\tLoss: 3.283645\n",
            "Wed Oct 31 11:56:29 2018 Train Epoch: 7 [1068800/1281167 (83%)]\tLoss: 3.297830\n",
            "Wed Oct 31 11:56:56 2018 Train Epoch: 7 [1075200/1281167 (84%)]\tLoss: 3.812000\n",
            "Wed Oct 31 11:57:23 2018 Train Epoch: 7 [1081600/1281167 (84%)]\tLoss: 3.394925\n",
            "Wed Oct 31 11:57:51 2018 Train Epoch: 7 [1088000/1281167 (85%)]\tLoss: 3.358253\n",
            "Wed Oct 31 11:58:18 2018 Train Epoch: 7 [1094400/1281167 (85%)]\tLoss: 4.108812\n",
            "Wed Oct 31 11:58:45 2018 Train Epoch: 7 [1100800/1281167 (86%)]\tLoss: 3.376091\n",
            "Wed Oct 31 11:59:12 2018 Train Epoch: 7 [1107200/1281167 (86%)]\tLoss: 4.419929\n",
            "Wed Oct 31 11:59:40 2018 Train Epoch: 7 [1113600/1281167 (87%)]\tLoss: 3.194337\n",
            "Wed Oct 31 12:00:07 2018 Train Epoch: 7 [1120000/1281167 (87%)]\tLoss: 4.055726\n",
            "Wed Oct 31 12:00:34 2018 Train Epoch: 7 [1126400/1281167 (88%)]\tLoss: 3.964301\n",
            "Wed Oct 31 12:01:01 2018 Train Epoch: 7 [1132800/1281167 (88%)]\tLoss: 3.449169\n",
            "Wed Oct 31 12:01:29 2018 Train Epoch: 7 [1139200/1281167 (89%)]\tLoss: 3.501608\n",
            "Wed Oct 31 12:01:56 2018 Train Epoch: 7 [1145600/1281167 (89%)]\tLoss: 3.626977\n",
            "Wed Oct 31 12:02:23 2018 Train Epoch: 7 [1152000/1281167 (90%)]\tLoss: 4.159761\n",
            "Wed Oct 31 12:02:50 2018 Train Epoch: 7 [1158400/1281167 (90%)]\tLoss: 3.754753\n",
            "Wed Oct 31 12:03:17 2018 Train Epoch: 7 [1164800/1281167 (91%)]\tLoss: 3.901968\n",
            "Wed Oct 31 12:03:45 2018 Train Epoch: 7 [1171200/1281167 (91%)]\tLoss: 3.665755\n",
            "Wed Oct 31 12:04:12 2018 Train Epoch: 7 [1177600/1281167 (92%)]\tLoss: 3.842886\n",
            "Wed Oct 31 12:04:39 2018 Train Epoch: 7 [1184000/1281167 (92%)]\tLoss: 3.700233\n",
            "Wed Oct 31 12:05:06 2018 Train Epoch: 7 [1190400/1281167 (93%)]\tLoss: 3.439334\n",
            "Wed Oct 31 12:05:34 2018 Train Epoch: 7 [1196800/1281167 (93%)]\tLoss: 3.747371\n",
            "Wed Oct 31 12:06:01 2018 Train Epoch: 7 [1203200/1281167 (94%)]\tLoss: 3.712352\n",
            "Wed Oct 31 12:06:28 2018 Train Epoch: 7 [1209600/1281167 (94%)]\tLoss: 3.562436\n",
            "Wed Oct 31 12:06:55 2018 Train Epoch: 7 [1216000/1281167 (95%)]\tLoss: 3.183436\n",
            "Wed Oct 31 12:07:22 2018 Train Epoch: 7 [1222400/1281167 (95%)]\tLoss: 3.279993\n",
            "Wed Oct 31 12:07:50 2018 Train Epoch: 7 [1228800/1281167 (96%)]\tLoss: 3.822311\n",
            "Wed Oct 31 12:08:17 2018 Train Epoch: 7 [1235200/1281167 (96%)]\tLoss: 3.449222\n",
            "Wed Oct 31 12:08:44 2018 Train Epoch: 7 [1241600/1281167 (97%)]\tLoss: 3.959206\n",
            "Wed Oct 31 12:09:11 2018 Train Epoch: 7 [1248000/1281167 (97%)]\tLoss: 3.411711\n",
            "Wed Oct 31 12:09:39 2018 Train Epoch: 7 [1254400/1281167 (98%)]\tLoss: 3.142269\n",
            "Wed Oct 31 12:10:06 2018 Train Epoch: 7 [1260800/1281167 (98%)]\tLoss: 2.950963\n",
            "Wed Oct 31 12:10:33 2018 Train Epoch: 7 [1267200/1281167 (99%)]\tLoss: 3.428784\n",
            "Wed Oct 31 12:11:00 2018 Train Epoch: 7 [1273600/1281167 (99%)]\tLoss: 3.567738\n",
            "Wed Oct 31 12:11:28 2018 Train Epoch: 7 [1280000/1281167 (100%)]\tLoss: 3.466752\n",
            "\n",
            "Test set: Average loss: 3.3835, Accuracy: 14430/50000 (29%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/007.pt\n",
            "\n",
            "Wed Oct 31 12:15:25 2018 Train Epoch: 8 [0/1281167 (0%)]\tLoss: 3.737363\n",
            "Wed Oct 31 12:15:52 2018 Train Epoch: 8 [6400/1281167 (0%)]\tLoss: 3.733587\n",
            "Wed Oct 31 12:16:19 2018 Train Epoch: 8 [12800/1281167 (1%)]\tLoss: 3.630140\n",
            "Wed Oct 31 12:16:47 2018 Train Epoch: 8 [19200/1281167 (1%)]\tLoss: 3.886681\n",
            "Wed Oct 31 12:17:14 2018 Train Epoch: 8 [25600/1281167 (2%)]\tLoss: 3.734164\n",
            "Wed Oct 31 12:17:41 2018 Train Epoch: 8 [32000/1281167 (2%)]\tLoss: 3.708700\n",
            "Wed Oct 31 12:18:08 2018 Train Epoch: 8 [38400/1281167 (3%)]\tLoss: 3.373974\n",
            "Wed Oct 31 12:18:35 2018 Train Epoch: 8 [44800/1281167 (3%)]\tLoss: 3.634437\n",
            "Wed Oct 31 12:19:03 2018 Train Epoch: 8 [51200/1281167 (4%)]\tLoss: 3.469412\n",
            "Wed Oct 31 12:19:30 2018 Train Epoch: 8 [57600/1281167 (4%)]\tLoss: 3.748014\n",
            "Wed Oct 31 12:19:57 2018 Train Epoch: 8 [64000/1281167 (5%)]\tLoss: 3.474844\n",
            "Wed Oct 31 12:20:24 2018 Train Epoch: 8 [70400/1281167 (5%)]\tLoss: 3.583719\n",
            "Wed Oct 31 12:20:51 2018 Train Epoch: 8 [76800/1281167 (6%)]\tLoss: 3.802248\n",
            "Wed Oct 31 12:21:19 2018 Train Epoch: 8 [83200/1281167 (6%)]\tLoss: 3.513398\n",
            "Wed Oct 31 12:21:46 2018 Train Epoch: 8 [89600/1281167 (7%)]\tLoss: 4.024164\n",
            "Wed Oct 31 12:22:13 2018 Train Epoch: 8 [96000/1281167 (7%)]\tLoss: 3.170544\n",
            "Wed Oct 31 12:22:40 2018 Train Epoch: 8 [102400/1281167 (8%)]\tLoss: 3.757900\n",
            "Wed Oct 31 12:23:08 2018 Train Epoch: 8 [108800/1281167 (8%)]\tLoss: 3.765274\n",
            "Wed Oct 31 12:23:35 2018 Train Epoch: 8 [115200/1281167 (9%)]\tLoss: 2.993195\n",
            "Wed Oct 31 12:24:02 2018 Train Epoch: 8 [121600/1281167 (9%)]\tLoss: 3.466378\n",
            "Wed Oct 31 12:24:29 2018 Train Epoch: 8 [128000/1281167 (10%)]\tLoss: 3.677050\n",
            "Wed Oct 31 12:24:57 2018 Train Epoch: 8 [134400/1281167 (10%)]\tLoss: 3.239987\n",
            "Wed Oct 31 12:25:24 2018 Train Epoch: 8 [140800/1281167 (11%)]\tLoss: 3.210284\n",
            "Wed Oct 31 12:25:51 2018 Train Epoch: 8 [147200/1281167 (11%)]\tLoss: 3.517451\n",
            "Wed Oct 31 12:26:18 2018 Train Epoch: 8 [153600/1281167 (12%)]\tLoss: 3.499113\n",
            "Wed Oct 31 12:26:46 2018 Train Epoch: 8 [160000/1281167 (12%)]\tLoss: 3.596209\n",
            "Wed Oct 31 12:27:13 2018 Train Epoch: 8 [166400/1281167 (13%)]\tLoss: 3.291896\n",
            "Wed Oct 31 12:27:40 2018 Train Epoch: 8 [172800/1281167 (13%)]\tLoss: 3.994270\n",
            "Wed Oct 31 12:28:07 2018 Train Epoch: 8 [179200/1281167 (14%)]\tLoss: 3.573087\n",
            "Wed Oct 31 12:28:35 2018 Train Epoch: 8 [185600/1281167 (14%)]\tLoss: 3.299636\n",
            "Wed Oct 31 12:29:02 2018 Train Epoch: 8 [192000/1281167 (15%)]\tLoss: 3.579211\n",
            "Wed Oct 31 12:29:29 2018 Train Epoch: 8 [198400/1281167 (15%)]\tLoss: 3.638140\n",
            "Wed Oct 31 12:29:56 2018 Train Epoch: 8 [204800/1281167 (16%)]\tLoss: 3.104428\n",
            "Wed Oct 31 12:30:23 2018 Train Epoch: 8 [211200/1281167 (16%)]\tLoss: 3.945719\n",
            "Wed Oct 31 12:30:51 2018 Train Epoch: 8 [217600/1281167 (17%)]\tLoss: 3.148211\n",
            "Wed Oct 31 12:31:18 2018 Train Epoch: 8 [224000/1281167 (17%)]\tLoss: 3.928091\n",
            "Wed Oct 31 12:31:45 2018 Train Epoch: 8 [230400/1281167 (18%)]\tLoss: 3.032047\n",
            "Wed Oct 31 12:32:13 2018 Train Epoch: 8 [236800/1281167 (18%)]\tLoss: 3.402684\n",
            "Wed Oct 31 12:32:40 2018 Train Epoch: 8 [243200/1281167 (19%)]\tLoss: 3.236664\n",
            "Wed Oct 31 12:33:07 2018 Train Epoch: 8 [249600/1281167 (19%)]\tLoss: 3.445678\n",
            "Wed Oct 31 12:33:34 2018 Train Epoch: 8 [256000/1281167 (20%)]\tLoss: 3.559760\n",
            "Wed Oct 31 12:34:01 2018 Train Epoch: 8 [262400/1281167 (20%)]\tLoss: 3.694156\n",
            "Wed Oct 31 12:34:29 2018 Train Epoch: 8 [268800/1281167 (21%)]\tLoss: 3.071342\n",
            "Wed Oct 31 12:34:56 2018 Train Epoch: 8 [275200/1281167 (21%)]\tLoss: 3.868527\n",
            "Wed Oct 31 12:35:23 2018 Train Epoch: 8 [281600/1281167 (22%)]\tLoss: 3.265389\n",
            "Wed Oct 31 12:35:50 2018 Train Epoch: 8 [288000/1281167 (22%)]\tLoss: 3.189526\n",
            "Wed Oct 31 12:36:17 2018 Train Epoch: 8 [294400/1281167 (23%)]\tLoss: 3.893357\n",
            "Wed Oct 31 12:36:45 2018 Train Epoch: 8 [300800/1281167 (23%)]\tLoss: 3.646183\n",
            "Wed Oct 31 12:37:12 2018 Train Epoch: 8 [307200/1281167 (24%)]\tLoss: 3.759669\n",
            "Wed Oct 31 12:37:39 2018 Train Epoch: 8 [313600/1281167 (24%)]\tLoss: 3.470523\n",
            "Wed Oct 31 12:38:06 2018 Train Epoch: 8 [320000/1281167 (25%)]\tLoss: 3.274925\n",
            "Wed Oct 31 12:38:34 2018 Train Epoch: 8 [326400/1281167 (25%)]\tLoss: 3.565066\n",
            "Wed Oct 31 12:39:01 2018 Train Epoch: 8 [332800/1281167 (26%)]\tLoss: 3.276562\n",
            "Wed Oct 31 12:39:28 2018 Train Epoch: 8 [339200/1281167 (26%)]\tLoss: 3.698687\n",
            "Wed Oct 31 12:39:55 2018 Train Epoch: 8 [345600/1281167 (27%)]\tLoss: 3.782419\n",
            "Wed Oct 31 12:40:23 2018 Train Epoch: 8 [352000/1281167 (27%)]\tLoss: 3.683795\n",
            "Wed Oct 31 12:40:50 2018 Train Epoch: 8 [358400/1281167 (28%)]\tLoss: 3.701018\n",
            "Wed Oct 31 12:41:17 2018 Train Epoch: 8 [364800/1281167 (28%)]\tLoss: 3.748542\n",
            "Wed Oct 31 12:41:44 2018 Train Epoch: 8 [371200/1281167 (29%)]\tLoss: 2.971621\n",
            "Wed Oct 31 12:42:11 2018 Train Epoch: 8 [377600/1281167 (29%)]\tLoss: 3.829355\n",
            "Wed Oct 31 12:42:39 2018 Train Epoch: 8 [384000/1281167 (30%)]\tLoss: 3.813539\n",
            "Wed Oct 31 12:43:06 2018 Train Epoch: 8 [390400/1281167 (30%)]\tLoss: 3.544357\n",
            "Wed Oct 31 12:43:33 2018 Train Epoch: 8 [396800/1281167 (31%)]\tLoss: 3.077547\n",
            "Wed Oct 31 12:44:00 2018 Train Epoch: 8 [403200/1281167 (31%)]\tLoss: 3.386972\n",
            "Wed Oct 31 12:44:28 2018 Train Epoch: 8 [409600/1281167 (32%)]\tLoss: 3.854846\n",
            "Wed Oct 31 12:44:55 2018 Train Epoch: 8 [416000/1281167 (32%)]\tLoss: 4.085452\n",
            "Wed Oct 31 12:45:22 2018 Train Epoch: 8 [422400/1281167 (33%)]\tLoss: 3.514010\n",
            "Wed Oct 31 12:45:49 2018 Train Epoch: 8 [428800/1281167 (33%)]\tLoss: 3.616383\n",
            "Wed Oct 31 12:46:17 2018 Train Epoch: 8 [435200/1281167 (34%)]\tLoss: 3.420271\n",
            "Wed Oct 31 12:46:44 2018 Train Epoch: 8 [441600/1281167 (34%)]\tLoss: 3.742723\n",
            "Wed Oct 31 12:47:11 2018 Train Epoch: 8 [448000/1281167 (35%)]\tLoss: 3.731620\n",
            "Wed Oct 31 12:47:38 2018 Train Epoch: 8 [454400/1281167 (35%)]\tLoss: 3.551843\n",
            "Wed Oct 31 12:48:05 2018 Train Epoch: 8 [460800/1281167 (36%)]\tLoss: 3.548500\n",
            "Wed Oct 31 12:48:33 2018 Train Epoch: 8 [467200/1281167 (36%)]\tLoss: 3.588120\n",
            "Wed Oct 31 12:49:00 2018 Train Epoch: 8 [473600/1281167 (37%)]\tLoss: 3.189636\n",
            "Wed Oct 31 12:49:27 2018 Train Epoch: 8 [480000/1281167 (37%)]\tLoss: 3.281397\n",
            "Wed Oct 31 12:49:54 2018 Train Epoch: 8 [486400/1281167 (38%)]\tLoss: 3.795831\n",
            "Wed Oct 31 12:50:21 2018 Train Epoch: 8 [492800/1281167 (38%)]\tLoss: 3.562910\n",
            "Wed Oct 31 12:50:49 2018 Train Epoch: 8 [499200/1281167 (39%)]\tLoss: 3.524140\n",
            "Wed Oct 31 12:51:16 2018 Train Epoch: 8 [505600/1281167 (39%)]\tLoss: 3.454101\n",
            "Wed Oct 31 12:51:43 2018 Train Epoch: 8 [512000/1281167 (40%)]\tLoss: 3.884461\n",
            "Wed Oct 31 12:52:10 2018 Train Epoch: 8 [518400/1281167 (40%)]\tLoss: 3.828115\n",
            "Wed Oct 31 12:52:38 2018 Train Epoch: 8 [524800/1281167 (41%)]\tLoss: 3.793053\n",
            "Wed Oct 31 12:53:05 2018 Train Epoch: 8 [531200/1281167 (41%)]\tLoss: 3.538155\n",
            "Wed Oct 31 12:53:32 2018 Train Epoch: 8 [537600/1281167 (42%)]\tLoss: 3.687970\n",
            "Wed Oct 31 12:53:59 2018 Train Epoch: 8 [544000/1281167 (42%)]\tLoss: 3.399436\n",
            "Wed Oct 31 12:54:26 2018 Train Epoch: 8 [550400/1281167 (43%)]\tLoss: 3.780601\n",
            "Wed Oct 31 12:54:54 2018 Train Epoch: 8 [556800/1281167 (43%)]\tLoss: 3.989034\n",
            "Wed Oct 31 12:55:21 2018 Train Epoch: 8 [563200/1281167 (44%)]\tLoss: 3.277373\n",
            "Wed Oct 31 12:55:48 2018 Train Epoch: 8 [569600/1281167 (44%)]\tLoss: 3.463347\n",
            "Wed Oct 31 12:56:15 2018 Train Epoch: 8 [576000/1281167 (45%)]\tLoss: 4.025879\n",
            "Wed Oct 31 12:56:42 2018 Train Epoch: 8 [582400/1281167 (45%)]\tLoss: 3.979733\n",
            "Wed Oct 31 12:57:10 2018 Train Epoch: 8 [588800/1281167 (46%)]\tLoss: 3.357146\n",
            "Wed Oct 31 12:57:37 2018 Train Epoch: 8 [595200/1281167 (46%)]\tLoss: 3.422197\n",
            "Wed Oct 31 12:58:04 2018 Train Epoch: 8 [601600/1281167 (47%)]\tLoss: 3.519221\n",
            "Wed Oct 31 12:58:31 2018 Train Epoch: 8 [608000/1281167 (47%)]\tLoss: 3.474626\n",
            "Wed Oct 31 12:58:59 2018 Train Epoch: 8 [614400/1281167 (48%)]\tLoss: 3.613152\n",
            "Wed Oct 31 12:59:26 2018 Train Epoch: 8 [620800/1281167 (48%)]\tLoss: 3.496252\n",
            "Wed Oct 31 12:59:53 2018 Train Epoch: 8 [627200/1281167 (49%)]\tLoss: 4.209213\n",
            "Wed Oct 31 13:00:20 2018 Train Epoch: 8 [633600/1281167 (49%)]\tLoss: 3.773643\n",
            "Wed Oct 31 13:00:47 2018 Train Epoch: 8 [640000/1281167 (50%)]\tLoss: 3.335627\n",
            "Wed Oct 31 13:01:15 2018 Train Epoch: 8 [646400/1281167 (50%)]\tLoss: 3.573685\n",
            "Wed Oct 31 13:01:42 2018 Train Epoch: 8 [652800/1281167 (51%)]\tLoss: 3.460032\n",
            "Wed Oct 31 13:02:09 2018 Train Epoch: 8 [659200/1281167 (51%)]\tLoss: 3.479499\n",
            "Wed Oct 31 13:02:36 2018 Train Epoch: 8 [665600/1281167 (52%)]\tLoss: 3.829679\n",
            "Wed Oct 31 13:03:04 2018 Train Epoch: 8 [672000/1281167 (52%)]\tLoss: 3.430608\n",
            "Wed Oct 31 13:03:31 2018 Train Epoch: 8 [678400/1281167 (53%)]\tLoss: 3.620179\n",
            "Wed Oct 31 13:03:58 2018 Train Epoch: 8 [684800/1281167 (53%)]\tLoss: 3.897817\n",
            "Wed Oct 31 13:04:25 2018 Train Epoch: 8 [691200/1281167 (54%)]\tLoss: 3.650243\n",
            "Wed Oct 31 13:04:53 2018 Train Epoch: 8 [697600/1281167 (54%)]\tLoss: 4.034455\n",
            "Wed Oct 31 13:05:20 2018 Train Epoch: 8 [704000/1281167 (55%)]\tLoss: 3.660172\n",
            "Wed Oct 31 13:05:47 2018 Train Epoch: 8 [710400/1281167 (55%)]\tLoss: 3.525534\n",
            "Wed Oct 31 13:06:14 2018 Train Epoch: 8 [716800/1281167 (56%)]\tLoss: 3.216414\n",
            "Wed Oct 31 13:06:41 2018 Train Epoch: 8 [723200/1281167 (56%)]\tLoss: 3.427549\n",
            "Wed Oct 31 13:07:09 2018 Train Epoch: 8 [729600/1281167 (57%)]\tLoss: 3.702326\n",
            "Wed Oct 31 13:07:36 2018 Train Epoch: 8 [736000/1281167 (57%)]\tLoss: 3.607352\n",
            "Wed Oct 31 13:08:03 2018 Train Epoch: 8 [742400/1281167 (58%)]\tLoss: 3.787109\n",
            "Wed Oct 31 13:08:30 2018 Train Epoch: 8 [748800/1281167 (58%)]\tLoss: 4.019223\n",
            "Wed Oct 31 13:08:58 2018 Train Epoch: 8 [755200/1281167 (59%)]\tLoss: 3.478194\n",
            "Wed Oct 31 13:09:25 2018 Train Epoch: 8 [761600/1281167 (59%)]\tLoss: 4.197608\n",
            "Wed Oct 31 13:09:52 2018 Train Epoch: 8 [768000/1281167 (60%)]\tLoss: 3.775490\n",
            "Wed Oct 31 13:10:19 2018 Train Epoch: 8 [774400/1281167 (60%)]\tLoss: 3.282127\n",
            "Wed Oct 31 13:10:46 2018 Train Epoch: 8 [780800/1281167 (61%)]\tLoss: 4.027617\n",
            "Wed Oct 31 13:11:14 2018 Train Epoch: 8 [787200/1281167 (61%)]\tLoss: 3.909986\n",
            "Wed Oct 31 13:11:41 2018 Train Epoch: 8 [793600/1281167 (62%)]\tLoss: 3.815366\n",
            "Wed Oct 31 13:12:08 2018 Train Epoch: 8 [800000/1281167 (62%)]\tLoss: 3.658398\n",
            "Wed Oct 31 13:12:35 2018 Train Epoch: 8 [806400/1281167 (63%)]\tLoss: 3.313839\n",
            "Wed Oct 31 13:13:03 2018 Train Epoch: 8 [812800/1281167 (63%)]\tLoss: 3.304148\n",
            "Wed Oct 31 13:13:30 2018 Train Epoch: 8 [819200/1281167 (64%)]\tLoss: 3.764498\n",
            "Wed Oct 31 13:13:57 2018 Train Epoch: 8 [825600/1281167 (64%)]\tLoss: 3.942754\n",
            "Wed Oct 31 13:14:24 2018 Train Epoch: 8 [832000/1281167 (65%)]\tLoss: 3.616452\n",
            "Wed Oct 31 13:14:51 2018 Train Epoch: 8 [838400/1281167 (65%)]\tLoss: 3.724316\n",
            "Wed Oct 31 13:15:19 2018 Train Epoch: 8 [844800/1281167 (66%)]\tLoss: 4.045497\n",
            "Wed Oct 31 13:15:46 2018 Train Epoch: 8 [851200/1281167 (66%)]\tLoss: 3.396178\n",
            "Wed Oct 31 13:16:13 2018 Train Epoch: 8 [857600/1281167 (67%)]\tLoss: 3.453108\n",
            "Wed Oct 31 13:16:40 2018 Train Epoch: 8 [864000/1281167 (67%)]\tLoss: 3.874006\n",
            "Wed Oct 31 13:17:08 2018 Train Epoch: 8 [870400/1281167 (68%)]\tLoss: 3.465713\n",
            "Wed Oct 31 13:17:35 2018 Train Epoch: 8 [876800/1281167 (68%)]\tLoss: 3.178729\n",
            "Wed Oct 31 13:18:02 2018 Train Epoch: 8 [883200/1281167 (69%)]\tLoss: 3.713712\n",
            "Wed Oct 31 13:18:29 2018 Train Epoch: 8 [889600/1281167 (69%)]\tLoss: 3.705451\n",
            "Wed Oct 31 13:18:57 2018 Train Epoch: 8 [896000/1281167 (70%)]\tLoss: 3.501116\n",
            "Wed Oct 31 13:19:24 2018 Train Epoch: 8 [902400/1281167 (70%)]\tLoss: 3.428234\n",
            "Wed Oct 31 13:19:51 2018 Train Epoch: 8 [908800/1281167 (71%)]\tLoss: 3.411188\n",
            "Wed Oct 31 13:20:18 2018 Train Epoch: 8 [915200/1281167 (71%)]\tLoss: 3.693676\n",
            "Wed Oct 31 13:20:46 2018 Train Epoch: 8 [921600/1281167 (72%)]\tLoss: 3.372714\n",
            "Wed Oct 31 13:21:13 2018 Train Epoch: 8 [928000/1281167 (72%)]\tLoss: 4.105135\n",
            "Wed Oct 31 13:21:40 2018 Train Epoch: 8 [934400/1281167 (73%)]\tLoss: 3.588060\n",
            "Wed Oct 31 13:22:07 2018 Train Epoch: 8 [940800/1281167 (73%)]\tLoss: 4.132788\n",
            "Wed Oct 31 13:22:34 2018 Train Epoch: 8 [947200/1281167 (74%)]\tLoss: 3.142781\n",
            "Wed Oct 31 13:23:02 2018 Train Epoch: 8 [953600/1281167 (74%)]\tLoss: 3.933754\n",
            "Wed Oct 31 13:23:29 2018 Train Epoch: 8 [960000/1281167 (75%)]\tLoss: 3.604786\n",
            "Wed Oct 31 13:23:56 2018 Train Epoch: 8 [966400/1281167 (75%)]\tLoss: 3.724273\n",
            "Wed Oct 31 13:24:23 2018 Train Epoch: 8 [972800/1281167 (76%)]\tLoss: 3.649904\n",
            "Wed Oct 31 13:24:51 2018 Train Epoch: 8 [979200/1281167 (76%)]\tLoss: 3.978320\n",
            "Wed Oct 31 13:25:18 2018 Train Epoch: 8 [985600/1281167 (77%)]\tLoss: 3.518332\n",
            "Wed Oct 31 13:25:45 2018 Train Epoch: 8 [992000/1281167 (77%)]\tLoss: 3.767516\n",
            "Wed Oct 31 13:26:12 2018 Train Epoch: 8 [998400/1281167 (78%)]\tLoss: 3.077162\n",
            "Wed Oct 31 13:26:40 2018 Train Epoch: 8 [1004800/1281167 (78%)]\tLoss: 3.498440\n",
            "Wed Oct 31 13:27:07 2018 Train Epoch: 8 [1011200/1281167 (79%)]\tLoss: 3.605635\n",
            "Wed Oct 31 13:27:34 2018 Train Epoch: 8 [1017600/1281167 (79%)]\tLoss: 3.229366\n",
            "Wed Oct 31 13:28:01 2018 Train Epoch: 8 [1024000/1281167 (80%)]\tLoss: 3.374758\n",
            "Wed Oct 31 13:28:28 2018 Train Epoch: 8 [1030400/1281167 (80%)]\tLoss: 3.570305\n",
            "Wed Oct 31 13:28:56 2018 Train Epoch: 8 [1036800/1281167 (81%)]\tLoss: 3.798537\n",
            "Wed Oct 31 13:29:23 2018 Train Epoch: 8 [1043200/1281167 (81%)]\tLoss: 3.766612\n",
            "Wed Oct 31 13:29:50 2018 Train Epoch: 8 [1049600/1281167 (82%)]\tLoss: 3.538825\n",
            "Wed Oct 31 13:30:17 2018 Train Epoch: 8 [1056000/1281167 (82%)]\tLoss: 3.981923\n",
            "Wed Oct 31 13:30:45 2018 Train Epoch: 8 [1062400/1281167 (83%)]\tLoss: 3.897625\n",
            "Wed Oct 31 13:31:12 2018 Train Epoch: 8 [1068800/1281167 (83%)]\tLoss: 3.567201\n",
            "Wed Oct 31 13:31:39 2018 Train Epoch: 8 [1075200/1281167 (84%)]\tLoss: 3.326015\n",
            "Wed Oct 31 13:32:06 2018 Train Epoch: 8 [1081600/1281167 (84%)]\tLoss: 3.792155\n",
            "Wed Oct 31 13:32:34 2018 Train Epoch: 8 [1088000/1281167 (85%)]\tLoss: 3.826586\n",
            "Wed Oct 31 13:33:01 2018 Train Epoch: 8 [1094400/1281167 (85%)]\tLoss: 3.508014\n",
            "Wed Oct 31 13:33:28 2018 Train Epoch: 8 [1100800/1281167 (86%)]\tLoss: 3.581897\n",
            "Wed Oct 31 13:33:55 2018 Train Epoch: 8 [1107200/1281167 (86%)]\tLoss: 3.538157\n",
            "Wed Oct 31 13:34:23 2018 Train Epoch: 8 [1113600/1281167 (87%)]\tLoss: 3.574284\n",
            "Wed Oct 31 13:34:50 2018 Train Epoch: 8 [1120000/1281167 (87%)]\tLoss: 3.310498\n",
            "Wed Oct 31 13:35:17 2018 Train Epoch: 8 [1126400/1281167 (88%)]\tLoss: 3.363113\n",
            "Wed Oct 31 13:35:44 2018 Train Epoch: 8 [1132800/1281167 (88%)]\tLoss: 2.969195\n",
            "Wed Oct 31 13:36:12 2018 Train Epoch: 8 [1139200/1281167 (89%)]\tLoss: 3.738309\n",
            "Wed Oct 31 13:36:39 2018 Train Epoch: 8 [1145600/1281167 (89%)]\tLoss: 3.618231\n",
            "Wed Oct 31 13:37:06 2018 Train Epoch: 8 [1152000/1281167 (90%)]\tLoss: 3.081979\n",
            "Wed Oct 31 13:37:33 2018 Train Epoch: 8 [1158400/1281167 (90%)]\tLoss: 3.386641\n",
            "Wed Oct 31 13:38:00 2018 Train Epoch: 8 [1164800/1281167 (91%)]\tLoss: 3.656253\n",
            "Wed Oct 31 13:38:28 2018 Train Epoch: 8 [1171200/1281167 (91%)]\tLoss: 3.753469\n",
            "Wed Oct 31 13:38:55 2018 Train Epoch: 8 [1177600/1281167 (92%)]\tLoss: 3.247977\n",
            "Wed Oct 31 13:39:22 2018 Train Epoch: 8 [1184000/1281167 (92%)]\tLoss: 3.511730\n",
            "Wed Oct 31 13:39:49 2018 Train Epoch: 8 [1190400/1281167 (93%)]\tLoss: 3.591310\n",
            "Wed Oct 31 13:40:17 2018 Train Epoch: 8 [1196800/1281167 (93%)]\tLoss: 3.697085\n",
            "Wed Oct 31 13:40:44 2018 Train Epoch: 8 [1203200/1281167 (94%)]\tLoss: 3.482793\n",
            "Wed Oct 31 13:41:11 2018 Train Epoch: 8 [1209600/1281167 (94%)]\tLoss: 3.728759\n",
            "Wed Oct 31 13:41:38 2018 Train Epoch: 8 [1216000/1281167 (95%)]\tLoss: 3.611197\n",
            "Wed Oct 31 13:42:06 2018 Train Epoch: 8 [1222400/1281167 (95%)]\tLoss: 3.910232\n",
            "Wed Oct 31 13:42:33 2018 Train Epoch: 8 [1228800/1281167 (96%)]\tLoss: 3.489426\n",
            "Wed Oct 31 13:43:00 2018 Train Epoch: 8 [1235200/1281167 (96%)]\tLoss: 3.052620\n",
            "Wed Oct 31 13:43:27 2018 Train Epoch: 8 [1241600/1281167 (97%)]\tLoss: 3.421117\n",
            "Wed Oct 31 13:43:55 2018 Train Epoch: 8 [1248000/1281167 (97%)]\tLoss: 3.599774\n",
            "Wed Oct 31 13:44:22 2018 Train Epoch: 8 [1254400/1281167 (98%)]\tLoss: 3.331768\n",
            "Wed Oct 31 13:44:49 2018 Train Epoch: 8 [1260800/1281167 (98%)]\tLoss: 3.375023\n",
            "Wed Oct 31 13:45:16 2018 Train Epoch: 8 [1267200/1281167 (99%)]\tLoss: 3.933290\n",
            "Wed Oct 31 13:45:43 2018 Train Epoch: 8 [1273600/1281167 (99%)]\tLoss: 3.016366\n",
            "Wed Oct 31 13:46:11 2018 Train Epoch: 8 [1280000/1281167 (100%)]\tLoss: 3.228100\n",
            "\n",
            "Test set: Average loss: 3.4009, Accuracy: 14408/50000 (29%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/008.pt\n",
            "\n",
            "Wed Oct 31 13:50:09 2018 Train Epoch: 9 [0/1281167 (0%)]\tLoss: 3.660979\n",
            "Wed Oct 31 13:50:37 2018 Train Epoch: 9 [6400/1281167 (0%)]\tLoss: 3.656921\n",
            "Wed Oct 31 13:51:04 2018 Train Epoch: 9 [12800/1281167 (1%)]\tLoss: 3.429473\n",
            "Wed Oct 31 13:51:31 2018 Train Epoch: 9 [19200/1281167 (1%)]\tLoss: 3.276580\n",
            "Wed Oct 31 13:51:58 2018 Train Epoch: 9 [25600/1281167 (2%)]\tLoss: 3.470520\n",
            "Wed Oct 31 13:52:25 2018 Train Epoch: 9 [32000/1281167 (2%)]\tLoss: 3.827112\n",
            "Wed Oct 31 13:52:52 2018 Train Epoch: 9 [38400/1281167 (3%)]\tLoss: 3.497342\n",
            "Wed Oct 31 13:53:20 2018 Train Epoch: 9 [44800/1281167 (3%)]\tLoss: 3.170450\n",
            "Wed Oct 31 13:53:47 2018 Train Epoch: 9 [51200/1281167 (4%)]\tLoss: 3.400897\n",
            "Wed Oct 31 13:54:14 2018 Train Epoch: 9 [57600/1281167 (4%)]\tLoss: 4.015463\n",
            "Wed Oct 31 13:54:41 2018 Train Epoch: 9 [64000/1281167 (5%)]\tLoss: 3.658921\n",
            "Wed Oct 31 13:55:09 2018 Train Epoch: 9 [70400/1281167 (5%)]\tLoss: 4.129721\n",
            "Wed Oct 31 13:55:36 2018 Train Epoch: 9 [76800/1281167 (6%)]\tLoss: 3.581439\n",
            "Wed Oct 31 13:56:03 2018 Train Epoch: 9 [83200/1281167 (6%)]\tLoss: 3.468051\n",
            "Wed Oct 31 13:56:30 2018 Train Epoch: 9 [89600/1281167 (7%)]\tLoss: 3.386809\n",
            "Wed Oct 31 13:56:57 2018 Train Epoch: 9 [96000/1281167 (7%)]\tLoss: 3.401358\n",
            "Wed Oct 31 13:57:25 2018 Train Epoch: 9 [102400/1281167 (8%)]\tLoss: 3.510551\n",
            "Wed Oct 31 13:57:52 2018 Train Epoch: 9 [108800/1281167 (8%)]\tLoss: 3.745298\n",
            "Wed Oct 31 13:58:19 2018 Train Epoch: 9 [115200/1281167 (9%)]\tLoss: 3.308842\n",
            "Wed Oct 31 13:58:46 2018 Train Epoch: 9 [121600/1281167 (9%)]\tLoss: 3.449956\n",
            "Wed Oct 31 13:59:14 2018 Train Epoch: 9 [128000/1281167 (10%)]\tLoss: 4.090616\n",
            "Wed Oct 31 13:59:41 2018 Train Epoch: 9 [134400/1281167 (10%)]\tLoss: 3.419357\n",
            "Wed Oct 31 14:00:08 2018 Train Epoch: 9 [140800/1281167 (11%)]\tLoss: 3.005099\n",
            "Wed Oct 31 14:00:35 2018 Train Epoch: 9 [147200/1281167 (11%)]\tLoss: 3.988994\n",
            "Wed Oct 31 14:01:03 2018 Train Epoch: 9 [153600/1281167 (12%)]\tLoss: 3.410434\n",
            "Wed Oct 31 14:01:30 2018 Train Epoch: 9 [160000/1281167 (12%)]\tLoss: 3.445092\n",
            "Wed Oct 31 14:01:57 2018 Train Epoch: 9 [166400/1281167 (13%)]\tLoss: 3.424895\n",
            "Wed Oct 31 14:02:24 2018 Train Epoch: 9 [172800/1281167 (13%)]\tLoss: 4.019896\n",
            "Wed Oct 31 14:02:51 2018 Train Epoch: 9 [179200/1281167 (14%)]\tLoss: 3.664437\n",
            "Wed Oct 31 14:03:19 2018 Train Epoch: 9 [185600/1281167 (14%)]\tLoss: 3.652858\n",
            "Wed Oct 31 14:03:46 2018 Train Epoch: 9 [192000/1281167 (15%)]\tLoss: 3.519569\n",
            "Wed Oct 31 14:04:13 2018 Train Epoch: 9 [198400/1281167 (15%)]\tLoss: 3.344579\n",
            "Wed Oct 31 14:04:40 2018 Train Epoch: 9 [204800/1281167 (16%)]\tLoss: 3.560551\n",
            "Wed Oct 31 14:05:08 2018 Train Epoch: 9 [211200/1281167 (16%)]\tLoss: 3.494840\n",
            "Wed Oct 31 14:05:35 2018 Train Epoch: 9 [217600/1281167 (17%)]\tLoss: 3.562531\n",
            "Wed Oct 31 14:06:02 2018 Train Epoch: 9 [224000/1281167 (17%)]\tLoss: 3.682193\n",
            "Wed Oct 31 14:06:29 2018 Train Epoch: 9 [230400/1281167 (18%)]\tLoss: 3.310160\n",
            "Wed Oct 31 14:06:57 2018 Train Epoch: 9 [236800/1281167 (18%)]\tLoss: 3.724109\n",
            "Wed Oct 31 14:07:24 2018 Train Epoch: 9 [243200/1281167 (19%)]\tLoss: 3.561240\n",
            "Wed Oct 31 14:07:51 2018 Train Epoch: 9 [249600/1281167 (19%)]\tLoss: 4.006739\n",
            "Wed Oct 31 14:08:18 2018 Train Epoch: 9 [256000/1281167 (20%)]\tLoss: 3.326810\n",
            "Wed Oct 31 14:08:46 2018 Train Epoch: 9 [262400/1281167 (20%)]\tLoss: 3.547712\n",
            "Wed Oct 31 14:09:13 2018 Train Epoch: 9 [268800/1281167 (21%)]\tLoss: 3.426262\n",
            "Wed Oct 31 14:09:40 2018 Train Epoch: 9 [275200/1281167 (21%)]\tLoss: 3.666218\n",
            "Wed Oct 31 14:10:07 2018 Train Epoch: 9 [281600/1281167 (22%)]\tLoss: 3.492472\n",
            "Wed Oct 31 14:10:34 2018 Train Epoch: 9 [288000/1281167 (22%)]\tLoss: 3.831323\n",
            "Wed Oct 31 14:11:02 2018 Train Epoch: 9 [294400/1281167 (23%)]\tLoss: 3.720970\n",
            "Wed Oct 31 14:11:29 2018 Train Epoch: 9 [300800/1281167 (23%)]\tLoss: 3.100150\n",
            "Wed Oct 31 14:11:56 2018 Train Epoch: 9 [307200/1281167 (24%)]\tLoss: 4.046505\n",
            "Wed Oct 31 14:12:23 2018 Train Epoch: 9 [313600/1281167 (24%)]\tLoss: 3.491602\n",
            "Wed Oct 31 14:12:50 2018 Train Epoch: 9 [320000/1281167 (25%)]\tLoss: 3.553778\n",
            "Wed Oct 31 14:13:18 2018 Train Epoch: 9 [326400/1281167 (25%)]\tLoss: 3.170305\n",
            "Wed Oct 31 14:13:45 2018 Train Epoch: 9 [332800/1281167 (26%)]\tLoss: 3.629321\n",
            "Wed Oct 31 14:14:12 2018 Train Epoch: 9 [339200/1281167 (26%)]\tLoss: 3.606266\n",
            "Wed Oct 31 14:14:39 2018 Train Epoch: 9 [345600/1281167 (27%)]\tLoss: 3.651254\n",
            "Wed Oct 31 14:15:07 2018 Train Epoch: 9 [352000/1281167 (27%)]\tLoss: 3.993028\n",
            "Wed Oct 31 14:15:34 2018 Train Epoch: 9 [358400/1281167 (28%)]\tLoss: 3.879368\n",
            "Wed Oct 31 14:16:01 2018 Train Epoch: 9 [364800/1281167 (28%)]\tLoss: 4.064241\n",
            "Wed Oct 31 14:16:28 2018 Train Epoch: 9 [371200/1281167 (29%)]\tLoss: 3.202996\n",
            "Wed Oct 31 14:16:56 2018 Train Epoch: 9 [377600/1281167 (29%)]\tLoss: 3.668208\n",
            "Wed Oct 31 14:17:23 2018 Train Epoch: 9 [384000/1281167 (30%)]\tLoss: 3.987633\n",
            "Wed Oct 31 14:17:50 2018 Train Epoch: 9 [390400/1281167 (30%)]\tLoss: 3.798820\n",
            "Wed Oct 31 14:18:17 2018 Train Epoch: 9 [396800/1281167 (31%)]\tLoss: 3.849359\n",
            "Wed Oct 31 14:18:45 2018 Train Epoch: 9 [403200/1281167 (31%)]\tLoss: 3.374753\n",
            "Wed Oct 31 14:19:12 2018 Train Epoch: 9 [409600/1281167 (32%)]\tLoss: 3.208414\n",
            "Wed Oct 31 14:19:39 2018 Train Epoch: 9 [416000/1281167 (32%)]\tLoss: 3.324359\n",
            "Wed Oct 31 14:20:06 2018 Train Epoch: 9 [422400/1281167 (33%)]\tLoss: 3.762458\n",
            "Wed Oct 31 14:20:34 2018 Train Epoch: 9 [428800/1281167 (33%)]\tLoss: 3.034872\n",
            "Wed Oct 31 14:21:01 2018 Train Epoch: 9 [435200/1281167 (34%)]\tLoss: 3.806596\n",
            "Wed Oct 31 14:21:28 2018 Train Epoch: 9 [441600/1281167 (34%)]\tLoss: 3.600699\n",
            "Wed Oct 31 14:21:55 2018 Train Epoch: 9 [448000/1281167 (35%)]\tLoss: 3.315572\n",
            "Wed Oct 31 14:22:23 2018 Train Epoch: 9 [454400/1281167 (35%)]\tLoss: 3.243399\n",
            "Wed Oct 31 14:22:50 2018 Train Epoch: 9 [460800/1281167 (36%)]\tLoss: 3.666919\n",
            "Wed Oct 31 14:23:17 2018 Train Epoch: 9 [467200/1281167 (36%)]\tLoss: 3.317087\n",
            "Wed Oct 31 14:23:44 2018 Train Epoch: 9 [473600/1281167 (37%)]\tLoss: 3.388122\n",
            "Wed Oct 31 14:24:12 2018 Train Epoch: 9 [480000/1281167 (37%)]\tLoss: 3.901931\n",
            "Wed Oct 31 14:24:39 2018 Train Epoch: 9 [486400/1281167 (38%)]\tLoss: 3.904552\n",
            "Wed Oct 31 14:25:06 2018 Train Epoch: 9 [492800/1281167 (38%)]\tLoss: 3.537084\n",
            "Wed Oct 31 14:25:33 2018 Train Epoch: 9 [499200/1281167 (39%)]\tLoss: 3.541980\n",
            "Wed Oct 31 14:26:00 2018 Train Epoch: 9 [505600/1281167 (39%)]\tLoss: 3.680335\n",
            "Wed Oct 31 14:26:28 2018 Train Epoch: 9 [512000/1281167 (40%)]\tLoss: 3.964377\n",
            "Wed Oct 31 14:26:55 2018 Train Epoch: 9 [518400/1281167 (40%)]\tLoss: 3.710736\n",
            "Wed Oct 31 14:27:22 2018 Train Epoch: 9 [524800/1281167 (41%)]\tLoss: 3.226000\n",
            "Wed Oct 31 14:27:49 2018 Train Epoch: 9 [531200/1281167 (41%)]\tLoss: 3.450813\n",
            "Wed Oct 31 14:28:17 2018 Train Epoch: 9 [537600/1281167 (42%)]\tLoss: 3.659927\n",
            "Wed Oct 31 14:28:44 2018 Train Epoch: 9 [544000/1281167 (42%)]\tLoss: 3.665730\n",
            "Wed Oct 31 14:29:11 2018 Train Epoch: 9 [550400/1281167 (43%)]\tLoss: 3.461826\n",
            "Wed Oct 31 14:29:38 2018 Train Epoch: 9 [556800/1281167 (43%)]\tLoss: 3.772392\n",
            "Wed Oct 31 14:30:06 2018 Train Epoch: 9 [563200/1281167 (44%)]\tLoss: 3.562777\n",
            "Wed Oct 31 14:30:33 2018 Train Epoch: 9 [569600/1281167 (44%)]\tLoss: 3.547368\n",
            "Wed Oct 31 14:31:00 2018 Train Epoch: 9 [576000/1281167 (45%)]\tLoss: 3.448975\n",
            "Wed Oct 31 14:31:27 2018 Train Epoch: 9 [582400/1281167 (45%)]\tLoss: 3.579515\n",
            "Wed Oct 31 14:31:55 2018 Train Epoch: 9 [588800/1281167 (46%)]\tLoss: 3.479896\n",
            "Wed Oct 31 14:32:22 2018 Train Epoch: 9 [595200/1281167 (46%)]\tLoss: 4.208977\n",
            "Wed Oct 31 14:32:49 2018 Train Epoch: 9 [601600/1281167 (47%)]\tLoss: 3.429254\n",
            "Wed Oct 31 14:33:16 2018 Train Epoch: 9 [608000/1281167 (47%)]\tLoss: 3.916213\n",
            "Wed Oct 31 14:33:44 2018 Train Epoch: 9 [614400/1281167 (48%)]\tLoss: 3.838688\n",
            "Wed Oct 31 14:34:11 2018 Train Epoch: 9 [620800/1281167 (48%)]\tLoss: 3.219805\n",
            "Wed Oct 31 14:34:38 2018 Train Epoch: 9 [627200/1281167 (49%)]\tLoss: 3.758875\n",
            "Wed Oct 31 14:35:05 2018 Train Epoch: 9 [633600/1281167 (49%)]\tLoss: 3.360448\n",
            "Wed Oct 31 14:35:33 2018 Train Epoch: 9 [640000/1281167 (50%)]\tLoss: 3.462022\n",
            "Wed Oct 31 14:36:00 2018 Train Epoch: 9 [646400/1281167 (50%)]\tLoss: 3.236670\n",
            "Wed Oct 31 14:36:27 2018 Train Epoch: 9 [652800/1281167 (51%)]\tLoss: 3.687531\n",
            "Wed Oct 31 14:36:54 2018 Train Epoch: 9 [659200/1281167 (51%)]\tLoss: 3.993635\n",
            "Wed Oct 31 14:37:22 2018 Train Epoch: 9 [665600/1281167 (52%)]\tLoss: 3.471571\n",
            "Wed Oct 31 14:37:49 2018 Train Epoch: 9 [672000/1281167 (52%)]\tLoss: 3.505489\n",
            "Wed Oct 31 14:38:16 2018 Train Epoch: 9 [678400/1281167 (53%)]\tLoss: 3.513510\n",
            "Wed Oct 31 14:38:43 2018 Train Epoch: 9 [684800/1281167 (53%)]\tLoss: 3.818481\n",
            "Wed Oct 31 14:39:11 2018 Train Epoch: 9 [691200/1281167 (54%)]\tLoss: 3.459366\n",
            "Wed Oct 31 14:39:38 2018 Train Epoch: 9 [697600/1281167 (54%)]\tLoss: 3.663924\n",
            "Wed Oct 31 14:40:05 2018 Train Epoch: 9 [704000/1281167 (55%)]\tLoss: 3.516722\n",
            "Wed Oct 31 14:40:32 2018 Train Epoch: 9 [710400/1281167 (55%)]\tLoss: 3.658062\n",
            "Wed Oct 31 14:41:00 2018 Train Epoch: 9 [716800/1281167 (56%)]\tLoss: 3.401036\n",
            "Wed Oct 31 14:41:27 2018 Train Epoch: 9 [723200/1281167 (56%)]\tLoss: 3.666835\n",
            "Wed Oct 31 14:41:54 2018 Train Epoch: 9 [729600/1281167 (57%)]\tLoss: 3.788554\n",
            "Wed Oct 31 14:42:21 2018 Train Epoch: 9 [736000/1281167 (57%)]\tLoss: 2.954920\n",
            "Wed Oct 31 14:42:48 2018 Train Epoch: 9 [742400/1281167 (58%)]\tLoss: 3.692711\n",
            "Wed Oct 31 14:43:16 2018 Train Epoch: 9 [748800/1281167 (58%)]\tLoss: 3.606111\n",
            "Wed Oct 31 14:43:43 2018 Train Epoch: 9 [755200/1281167 (59%)]\tLoss: 3.046543\n",
            "Wed Oct 31 14:44:10 2018 Train Epoch: 9 [761600/1281167 (59%)]\tLoss: 3.610025\n",
            "Wed Oct 31 14:44:37 2018 Train Epoch: 9 [768000/1281167 (60%)]\tLoss: 3.458302\n",
            "Wed Oct 31 14:45:05 2018 Train Epoch: 9 [774400/1281167 (60%)]\tLoss: 3.441909\n",
            "Wed Oct 31 14:45:32 2018 Train Epoch: 9 [780800/1281167 (61%)]\tLoss: 3.576224\n",
            "Wed Oct 31 14:45:59 2018 Train Epoch: 9 [787200/1281167 (61%)]\tLoss: 3.731999\n",
            "Wed Oct 31 14:46:26 2018 Train Epoch: 9 [793600/1281167 (62%)]\tLoss: 3.503185\n",
            "Wed Oct 31 14:46:53 2018 Train Epoch: 9 [800000/1281167 (62%)]\tLoss: 4.138539\n",
            "Wed Oct 31 14:47:21 2018 Train Epoch: 9 [806400/1281167 (63%)]\tLoss: 3.358456\n",
            "Wed Oct 31 14:47:48 2018 Train Epoch: 9 [812800/1281167 (63%)]\tLoss: 3.780928\n",
            "Wed Oct 31 14:48:15 2018 Train Epoch: 9 [819200/1281167 (64%)]\tLoss: 3.820000\n",
            "Wed Oct 31 14:48:42 2018 Train Epoch: 9 [825600/1281167 (64%)]\tLoss: 3.758554\n",
            "Wed Oct 31 14:49:10 2018 Train Epoch: 9 [832000/1281167 (65%)]\tLoss: 3.853684\n",
            "Wed Oct 31 14:49:37 2018 Train Epoch: 9 [838400/1281167 (65%)]\tLoss: 3.690093\n",
            "Wed Oct 31 14:50:04 2018 Train Epoch: 9 [844800/1281167 (66%)]\tLoss: 3.562746\n",
            "Wed Oct 31 14:50:31 2018 Train Epoch: 9 [851200/1281167 (66%)]\tLoss: 3.619924\n",
            "Wed Oct 31 14:50:58 2018 Train Epoch: 9 [857600/1281167 (67%)]\tLoss: 3.533713\n",
            "Wed Oct 31 14:51:26 2018 Train Epoch: 9 [864000/1281167 (67%)]\tLoss: 3.438293\n",
            "Wed Oct 31 14:51:53 2018 Train Epoch: 9 [870400/1281167 (68%)]\tLoss: 3.690019\n",
            "Wed Oct 31 14:52:20 2018 Train Epoch: 9 [876800/1281167 (68%)]\tLoss: 3.555076\n",
            "Wed Oct 31 14:52:47 2018 Train Epoch: 9 [883200/1281167 (69%)]\tLoss: 3.758642\n",
            "Wed Oct 31 14:53:14 2018 Train Epoch: 9 [889600/1281167 (69%)]\tLoss: 3.366571\n",
            "Wed Oct 31 14:53:42 2018 Train Epoch: 9 [896000/1281167 (70%)]\tLoss: 3.407244\n",
            "Wed Oct 31 14:54:09 2018 Train Epoch: 9 [902400/1281167 (70%)]\tLoss: 3.507233\n",
            "Wed Oct 31 14:54:36 2018 Train Epoch: 9 [908800/1281167 (71%)]\tLoss: 3.430935\n",
            "Wed Oct 31 14:55:03 2018 Train Epoch: 9 [915200/1281167 (71%)]\tLoss: 3.414165\n",
            "Wed Oct 31 14:55:31 2018 Train Epoch: 9 [921600/1281167 (72%)]\tLoss: 3.267147\n",
            "Wed Oct 31 14:55:58 2018 Train Epoch: 9 [928000/1281167 (72%)]\tLoss: 3.851373\n",
            "Wed Oct 31 14:56:25 2018 Train Epoch: 9 [934400/1281167 (73%)]\tLoss: 3.796978\n",
            "Wed Oct 31 14:56:52 2018 Train Epoch: 9 [940800/1281167 (73%)]\tLoss: 3.793247\n",
            "Wed Oct 31 14:57:19 2018 Train Epoch: 9 [947200/1281167 (74%)]\tLoss: 3.018009\n",
            "Wed Oct 31 14:57:47 2018 Train Epoch: 9 [953600/1281167 (74%)]\tLoss: 3.718399\n",
            "Wed Oct 31 14:58:14 2018 Train Epoch: 9 [960000/1281167 (75%)]\tLoss: 3.609093\n",
            "Wed Oct 31 14:58:41 2018 Train Epoch: 9 [966400/1281167 (75%)]\tLoss: 3.610245\n",
            "Wed Oct 31 14:59:08 2018 Train Epoch: 9 [972800/1281167 (76%)]\tLoss: 3.881303\n",
            "Wed Oct 31 14:59:35 2018 Train Epoch: 9 [979200/1281167 (76%)]\tLoss: 3.963536\n",
            "Wed Oct 31 15:00:03 2018 Train Epoch: 9 [985600/1281167 (77%)]\tLoss: 3.173493\n",
            "Wed Oct 31 15:00:30 2018 Train Epoch: 9 [992000/1281167 (77%)]\tLoss: 3.094753\n",
            "Wed Oct 31 15:00:57 2018 Train Epoch: 9 [998400/1281167 (78%)]\tLoss: 3.347945\n",
            "Wed Oct 31 15:01:24 2018 Train Epoch: 9 [1004800/1281167 (78%)]\tLoss: 3.931993\n",
            "Wed Oct 31 15:01:52 2018 Train Epoch: 9 [1011200/1281167 (79%)]\tLoss: 3.431568\n",
            "Wed Oct 31 15:02:19 2018 Train Epoch: 9 [1017600/1281167 (79%)]\tLoss: 3.268799\n",
            "Wed Oct 31 15:02:46 2018 Train Epoch: 9 [1024000/1281167 (80%)]\tLoss: 3.226733\n",
            "Wed Oct 31 15:03:13 2018 Train Epoch: 9 [1030400/1281167 (80%)]\tLoss: 3.175450\n",
            "Wed Oct 31 15:03:40 2018 Train Epoch: 9 [1036800/1281167 (81%)]\tLoss: 2.953091\n",
            "Wed Oct 31 15:04:08 2018 Train Epoch: 9 [1043200/1281167 (81%)]\tLoss: 4.042202\n",
            "Wed Oct 31 15:04:35 2018 Train Epoch: 9 [1049600/1281167 (82%)]\tLoss: 3.438215\n",
            "Wed Oct 31 15:05:02 2018 Train Epoch: 9 [1056000/1281167 (82%)]\tLoss: 3.929454\n",
            "Wed Oct 31 15:05:29 2018 Train Epoch: 9 [1062400/1281167 (83%)]\tLoss: 3.314711\n",
            "Wed Oct 31 15:05:56 2018 Train Epoch: 9 [1068800/1281167 (83%)]\tLoss: 3.245821\n",
            "Wed Oct 31 15:06:24 2018 Train Epoch: 9 [1075200/1281167 (84%)]\tLoss: 3.399088\n",
            "Wed Oct 31 15:06:51 2018 Train Epoch: 9 [1081600/1281167 (84%)]\tLoss: 3.761565\n",
            "Wed Oct 31 15:07:18 2018 Train Epoch: 9 [1088000/1281167 (85%)]\tLoss: 3.451364\n",
            "Wed Oct 31 15:07:45 2018 Train Epoch: 9 [1094400/1281167 (85%)]\tLoss: 4.262168\n",
            "Wed Oct 31 15:08:12 2018 Train Epoch: 9 [1100800/1281167 (86%)]\tLoss: 3.991486\n",
            "Wed Oct 31 15:08:40 2018 Train Epoch: 9 [1107200/1281167 (86%)]\tLoss: 4.124165\n",
            "Wed Oct 31 15:09:07 2018 Train Epoch: 9 [1113600/1281167 (87%)]\tLoss: 3.681871\n",
            "Wed Oct 31 15:09:34 2018 Train Epoch: 9 [1120000/1281167 (87%)]\tLoss: 4.156198\n",
            "Wed Oct 31 15:10:01 2018 Train Epoch: 9 [1126400/1281167 (88%)]\tLoss: 3.257603\n",
            "Wed Oct 31 15:10:29 2018 Train Epoch: 9 [1132800/1281167 (88%)]\tLoss: 3.489727\n",
            "Wed Oct 31 15:10:56 2018 Train Epoch: 9 [1139200/1281167 (89%)]\tLoss: 3.638258\n",
            "Wed Oct 31 15:11:23 2018 Train Epoch: 9 [1145600/1281167 (89%)]\tLoss: 3.636825\n",
            "Wed Oct 31 15:11:50 2018 Train Epoch: 9 [1152000/1281167 (90%)]\tLoss: 3.548363\n",
            "Wed Oct 31 15:12:17 2018 Train Epoch: 9 [1158400/1281167 (90%)]\tLoss: 3.405456\n",
            "Wed Oct 31 15:12:45 2018 Train Epoch: 9 [1164800/1281167 (91%)]\tLoss: 3.315604\n",
            "Wed Oct 31 15:13:12 2018 Train Epoch: 9 [1171200/1281167 (91%)]\tLoss: 3.187880\n",
            "Wed Oct 31 15:13:39 2018 Train Epoch: 9 [1177600/1281167 (92%)]\tLoss: 3.216190\n",
            "Wed Oct 31 15:14:06 2018 Train Epoch: 9 [1184000/1281167 (92%)]\tLoss: 3.986138\n",
            "Wed Oct 31 15:14:34 2018 Train Epoch: 9 [1190400/1281167 (93%)]\tLoss: 4.096663\n",
            "Wed Oct 31 15:15:01 2018 Train Epoch: 9 [1196800/1281167 (93%)]\tLoss: 3.794565\n",
            "Wed Oct 31 15:15:28 2018 Train Epoch: 9 [1203200/1281167 (94%)]\tLoss: 3.444978\n",
            "Wed Oct 31 15:15:55 2018 Train Epoch: 9 [1209600/1281167 (94%)]\tLoss: 3.326149\n",
            "Wed Oct 31 15:16:22 2018 Train Epoch: 9 [1216000/1281167 (95%)]\tLoss: 3.731444\n",
            "Wed Oct 31 15:16:50 2018 Train Epoch: 9 [1222400/1281167 (95%)]\tLoss: 3.692039\n",
            "Wed Oct 31 15:17:17 2018 Train Epoch: 9 [1228800/1281167 (96%)]\tLoss: 3.803393\n",
            "Wed Oct 31 15:17:44 2018 Train Epoch: 9 [1235200/1281167 (96%)]\tLoss: 3.602023\n",
            "Wed Oct 31 15:18:11 2018 Train Epoch: 9 [1241600/1281167 (97%)]\tLoss: 3.912537\n",
            "Wed Oct 31 15:18:38 2018 Train Epoch: 9 [1248000/1281167 (97%)]\tLoss: 3.624196\n",
            "Wed Oct 31 15:19:06 2018 Train Epoch: 9 [1254400/1281167 (98%)]\tLoss: 3.645281\n",
            "Wed Oct 31 15:19:33 2018 Train Epoch: 9 [1260800/1281167 (98%)]\tLoss: 3.690335\n",
            "Wed Oct 31 15:20:00 2018 Train Epoch: 9 [1267200/1281167 (99%)]\tLoss: 3.631671\n",
            "Wed Oct 31 15:20:27 2018 Train Epoch: 9 [1273600/1281167 (99%)]\tLoss: 3.537354\n",
            "Wed Oct 31 15:20:55 2018 Train Epoch: 9 [1280000/1281167 (100%)]\tLoss: 3.621196\n",
            "\n",
            "Test set: Average loss: 3.2729, Accuracy: 15099/50000 (30%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/009.pt\n",
            "\n",
            "Wed Oct 31 15:24:53 2018 Train Epoch: 10 [0/1281167 (0%)]\tLoss: 3.689685\n",
            "Wed Oct 31 15:25:20 2018 Train Epoch: 10 [6400/1281167 (0%)]\tLoss: 3.438797\n",
            "Wed Oct 31 15:25:47 2018 Train Epoch: 10 [12800/1281167 (1%)]\tLoss: 3.359337\n",
            "Wed Oct 31 15:26:14 2018 Train Epoch: 10 [19200/1281167 (1%)]\tLoss: 3.677629\n",
            "Wed Oct 31 15:26:41 2018 Train Epoch: 10 [25600/1281167 (2%)]\tLoss: 3.494082\n",
            "Wed Oct 31 15:27:08 2018 Train Epoch: 10 [32000/1281167 (2%)]\tLoss: 3.177366\n",
            "Wed Oct 31 15:27:36 2018 Train Epoch: 10 [38400/1281167 (3%)]\tLoss: 3.477161\n",
            "Wed Oct 31 15:28:03 2018 Train Epoch: 10 [44800/1281167 (3%)]\tLoss: 3.393601\n",
            "Wed Oct 31 15:28:30 2018 Train Epoch: 10 [51200/1281167 (4%)]\tLoss: 3.532263\n",
            "Wed Oct 31 15:28:57 2018 Train Epoch: 10 [57600/1281167 (4%)]\tLoss: 3.659426\n",
            "Wed Oct 31 15:29:24 2018 Train Epoch: 10 [64000/1281167 (5%)]\tLoss: 3.888395\n",
            "Wed Oct 31 15:29:52 2018 Train Epoch: 10 [70400/1281167 (5%)]\tLoss: 3.567142\n",
            "Wed Oct 31 15:30:19 2018 Train Epoch: 10 [76800/1281167 (6%)]\tLoss: 3.682259\n",
            "Wed Oct 31 15:30:46 2018 Train Epoch: 10 [83200/1281167 (6%)]\tLoss: 3.413109\n",
            "Wed Oct 31 15:31:13 2018 Train Epoch: 10 [89600/1281167 (7%)]\tLoss: 3.210293\n",
            "Wed Oct 31 15:31:40 2018 Train Epoch: 10 [96000/1281167 (7%)]\tLoss: 3.774641\n",
            "Wed Oct 31 15:32:08 2018 Train Epoch: 10 [102400/1281167 (8%)]\tLoss: 3.546182\n",
            "Wed Oct 31 15:32:35 2018 Train Epoch: 10 [108800/1281167 (8%)]\tLoss: 3.525364\n",
            "Wed Oct 31 15:33:02 2018 Train Epoch: 10 [115200/1281167 (9%)]\tLoss: 3.453676\n",
            "Wed Oct 31 15:33:29 2018 Train Epoch: 10 [121600/1281167 (9%)]\tLoss: 3.415546\n",
            "Wed Oct 31 15:33:57 2018 Train Epoch: 10 [128000/1281167 (10%)]\tLoss: 3.502991\n",
            "Wed Oct 31 15:34:24 2018 Train Epoch: 10 [134400/1281167 (10%)]\tLoss: 3.763817\n",
            "Wed Oct 31 15:34:51 2018 Train Epoch: 10 [140800/1281167 (11%)]\tLoss: 3.289442\n",
            "Wed Oct 31 15:35:18 2018 Train Epoch: 10 [147200/1281167 (11%)]\tLoss: 3.595480\n",
            "Wed Oct 31 15:35:45 2018 Train Epoch: 10 [153600/1281167 (12%)]\tLoss: 3.236436\n",
            "Wed Oct 31 15:36:13 2018 Train Epoch: 10 [160000/1281167 (12%)]\tLoss: 3.635916\n",
            "Wed Oct 31 15:36:40 2018 Train Epoch: 10 [166400/1281167 (13%)]\tLoss: 3.282751\n",
            "Wed Oct 31 15:37:07 2018 Train Epoch: 10 [172800/1281167 (13%)]\tLoss: 3.681052\n",
            "Wed Oct 31 15:37:34 2018 Train Epoch: 10 [179200/1281167 (14%)]\tLoss: 3.889823\n",
            "Wed Oct 31 15:38:02 2018 Train Epoch: 10 [185600/1281167 (14%)]\tLoss: 3.430362\n",
            "Wed Oct 31 15:38:29 2018 Train Epoch: 10 [192000/1281167 (15%)]\tLoss: 3.127676\n",
            "Wed Oct 31 15:38:56 2018 Train Epoch: 10 [198400/1281167 (15%)]\tLoss: 3.530187\n",
            "Wed Oct 31 15:39:23 2018 Train Epoch: 10 [204800/1281167 (16%)]\tLoss: 3.461471\n",
            "Wed Oct 31 15:39:50 2018 Train Epoch: 10 [211200/1281167 (16%)]\tLoss: 3.593690\n",
            "Wed Oct 31 15:40:18 2018 Train Epoch: 10 [217600/1281167 (17%)]\tLoss: 3.204980\n",
            "Wed Oct 31 15:40:45 2018 Train Epoch: 10 [224000/1281167 (17%)]\tLoss: 3.457992\n",
            "Wed Oct 31 15:41:12 2018 Train Epoch: 10 [230400/1281167 (18%)]\tLoss: 3.507337\n",
            "Wed Oct 31 15:41:39 2018 Train Epoch: 10 [236800/1281167 (18%)]\tLoss: 3.447309\n",
            "Wed Oct 31 15:42:07 2018 Train Epoch: 10 [243200/1281167 (19%)]\tLoss: 3.680778\n",
            "Wed Oct 31 15:42:34 2018 Train Epoch: 10 [249600/1281167 (19%)]\tLoss: 3.390162\n",
            "Wed Oct 31 15:43:01 2018 Train Epoch: 10 [256000/1281167 (20%)]\tLoss: 3.591576\n",
            "Wed Oct 31 15:43:28 2018 Train Epoch: 10 [262400/1281167 (20%)]\tLoss: 3.805469\n",
            "Wed Oct 31 15:43:55 2018 Train Epoch: 10 [268800/1281167 (21%)]\tLoss: 3.495740\n",
            "Wed Oct 31 15:44:23 2018 Train Epoch: 10 [275200/1281167 (21%)]\tLoss: 3.383991\n",
            "Wed Oct 31 15:44:50 2018 Train Epoch: 10 [281600/1281167 (22%)]\tLoss: 3.327945\n",
            "Wed Oct 31 15:45:17 2018 Train Epoch: 10 [288000/1281167 (22%)]\tLoss: 3.613815\n",
            "Wed Oct 31 15:45:44 2018 Train Epoch: 10 [294400/1281167 (23%)]\tLoss: 3.543397\n",
            "Wed Oct 31 15:46:11 2018 Train Epoch: 10 [300800/1281167 (23%)]\tLoss: 3.433483\n",
            "Wed Oct 31 15:46:39 2018 Train Epoch: 10 [307200/1281167 (24%)]\tLoss: 3.543035\n",
            "Wed Oct 31 15:47:06 2018 Train Epoch: 10 [313600/1281167 (24%)]\tLoss: 3.408698\n",
            "Wed Oct 31 15:47:33 2018 Train Epoch: 10 [320000/1281167 (25%)]\tLoss: 3.535601\n",
            "Wed Oct 31 15:48:00 2018 Train Epoch: 10 [326400/1281167 (25%)]\tLoss: 3.831905\n",
            "Wed Oct 31 15:48:27 2018 Train Epoch: 10 [332800/1281167 (26%)]\tLoss: 3.779114\n",
            "Wed Oct 31 15:48:55 2018 Train Epoch: 10 [339200/1281167 (26%)]\tLoss: 3.771797\n",
            "Wed Oct 31 15:49:22 2018 Train Epoch: 10 [345600/1281167 (27%)]\tLoss: 3.982713\n",
            "Wed Oct 31 15:49:49 2018 Train Epoch: 10 [352000/1281167 (27%)]\tLoss: 3.065128\n",
            "Wed Oct 31 15:50:16 2018 Train Epoch: 10 [358400/1281167 (28%)]\tLoss: 3.718579\n",
            "Wed Oct 31 15:50:43 2018 Train Epoch: 10 [364800/1281167 (28%)]\tLoss: 3.246590\n",
            "Wed Oct 31 15:51:11 2018 Train Epoch: 10 [371200/1281167 (29%)]\tLoss: 3.562675\n",
            "Wed Oct 31 15:51:38 2018 Train Epoch: 10 [377600/1281167 (29%)]\tLoss: 3.041223\n",
            "Wed Oct 31 15:52:05 2018 Train Epoch: 10 [384000/1281167 (30%)]\tLoss: 3.674642\n",
            "Wed Oct 31 15:52:32 2018 Train Epoch: 10 [390400/1281167 (30%)]\tLoss: 3.600621\n",
            "Wed Oct 31 15:52:59 2018 Train Epoch: 10 [396800/1281167 (31%)]\tLoss: 4.227763\n",
            "Wed Oct 31 15:53:27 2018 Train Epoch: 10 [403200/1281167 (31%)]\tLoss: 3.663588\n",
            "Wed Oct 31 15:53:54 2018 Train Epoch: 10 [409600/1281167 (32%)]\tLoss: 3.434484\n",
            "Wed Oct 31 15:54:21 2018 Train Epoch: 10 [416000/1281167 (32%)]\tLoss: 3.335124\n",
            "Wed Oct 31 15:54:48 2018 Train Epoch: 10 [422400/1281167 (33%)]\tLoss: 3.449267\n",
            "Wed Oct 31 15:55:15 2018 Train Epoch: 10 [428800/1281167 (33%)]\tLoss: 3.037525\n",
            "Wed Oct 31 15:55:43 2018 Train Epoch: 10 [435200/1281167 (34%)]\tLoss: 3.422425\n",
            "Wed Oct 31 15:56:10 2018 Train Epoch: 10 [441600/1281167 (34%)]\tLoss: 4.081757\n",
            "Wed Oct 31 15:56:37 2018 Train Epoch: 10 [448000/1281167 (35%)]\tLoss: 3.554406\n",
            "Wed Oct 31 15:57:04 2018 Train Epoch: 10 [454400/1281167 (35%)]\tLoss: 3.541089\n",
            "Wed Oct 31 15:57:32 2018 Train Epoch: 10 [460800/1281167 (36%)]\tLoss: 3.705507\n",
            "Wed Oct 31 15:57:59 2018 Train Epoch: 10 [467200/1281167 (36%)]\tLoss: 3.232575\n",
            "Wed Oct 31 15:58:26 2018 Train Epoch: 10 [473600/1281167 (37%)]\tLoss: 3.356843\n",
            "Wed Oct 31 15:58:53 2018 Train Epoch: 10 [480000/1281167 (37%)]\tLoss: 3.471488\n",
            "Wed Oct 31 15:59:20 2018 Train Epoch: 10 [486400/1281167 (38%)]\tLoss: 3.511922\n",
            "Wed Oct 31 15:59:48 2018 Train Epoch: 10 [492800/1281167 (38%)]\tLoss: 3.563451\n",
            "Wed Oct 31 16:00:15 2018 Train Epoch: 10 [499200/1281167 (39%)]\tLoss: 4.154625\n",
            "Wed Oct 31 16:00:42 2018 Train Epoch: 10 [505600/1281167 (39%)]\tLoss: 3.545320\n",
            "Wed Oct 31 16:01:09 2018 Train Epoch: 10 [512000/1281167 (40%)]\tLoss: 3.277382\n",
            "Wed Oct 31 16:01:37 2018 Train Epoch: 10 [518400/1281167 (40%)]\tLoss: 4.033857\n",
            "Wed Oct 31 16:02:04 2018 Train Epoch: 10 [524800/1281167 (41%)]\tLoss: 3.474872\n",
            "Wed Oct 31 16:02:31 2018 Train Epoch: 10 [531200/1281167 (41%)]\tLoss: 3.269651\n",
            "Wed Oct 31 16:02:58 2018 Train Epoch: 10 [537600/1281167 (42%)]\tLoss: 3.015549\n",
            "Wed Oct 31 16:03:26 2018 Train Epoch: 10 [544000/1281167 (42%)]\tLoss: 4.065895\n",
            "Wed Oct 31 16:03:53 2018 Train Epoch: 10 [550400/1281167 (43%)]\tLoss: 3.534679\n",
            "Wed Oct 31 16:04:20 2018 Train Epoch: 10 [556800/1281167 (43%)]\tLoss: 3.590103\n",
            "Wed Oct 31 16:04:47 2018 Train Epoch: 10 [563200/1281167 (44%)]\tLoss: 3.795424\n",
            "Wed Oct 31 16:05:14 2018 Train Epoch: 10 [569600/1281167 (44%)]\tLoss: 3.262955\n",
            "Wed Oct 31 16:05:42 2018 Train Epoch: 10 [576000/1281167 (45%)]\tLoss: 3.237407\n",
            "Wed Oct 31 16:06:09 2018 Train Epoch: 10 [582400/1281167 (45%)]\tLoss: 3.386747\n",
            "Wed Oct 31 16:06:36 2018 Train Epoch: 10 [588800/1281167 (46%)]\tLoss: 4.167238\n",
            "Wed Oct 31 16:07:03 2018 Train Epoch: 10 [595200/1281167 (46%)]\tLoss: 3.705099\n",
            "Wed Oct 31 16:07:30 2018 Train Epoch: 10 [601600/1281167 (47%)]\tLoss: 3.557656\n",
            "Wed Oct 31 16:07:58 2018 Train Epoch: 10 [608000/1281167 (47%)]\tLoss: 3.440425\n",
            "Wed Oct 31 16:08:25 2018 Train Epoch: 10 [614400/1281167 (48%)]\tLoss: 3.434822\n",
            "Wed Oct 31 16:08:52 2018 Train Epoch: 10 [620800/1281167 (48%)]\tLoss: 3.557356\n",
            "Wed Oct 31 16:09:19 2018 Train Epoch: 10 [627200/1281167 (49%)]\tLoss: 3.611878\n",
            "Wed Oct 31 16:09:47 2018 Train Epoch: 10 [633600/1281167 (49%)]\tLoss: 3.201121\n",
            "Wed Oct 31 16:10:14 2018 Train Epoch: 10 [640000/1281167 (50%)]\tLoss: 3.802186\n",
            "Wed Oct 31 16:10:41 2018 Train Epoch: 10 [646400/1281167 (50%)]\tLoss: 3.058360\n",
            "Wed Oct 31 16:11:08 2018 Train Epoch: 10 [652800/1281167 (51%)]\tLoss: 3.769323\n",
            "Wed Oct 31 16:11:35 2018 Train Epoch: 10 [659200/1281167 (51%)]\tLoss: 3.582975\n",
            "Wed Oct 31 16:12:03 2018 Train Epoch: 10 [665600/1281167 (52%)]\tLoss: 3.483951\n",
            "Wed Oct 31 16:12:30 2018 Train Epoch: 10 [672000/1281167 (52%)]\tLoss: 3.371480\n",
            "Wed Oct 31 16:12:57 2018 Train Epoch: 10 [678400/1281167 (53%)]\tLoss: 3.807089\n",
            "Wed Oct 31 16:13:24 2018 Train Epoch: 10 [684800/1281167 (53%)]\tLoss: 3.661998\n",
            "Wed Oct 31 16:13:51 2018 Train Epoch: 10 [691200/1281167 (54%)]\tLoss: 3.633919\n",
            "Wed Oct 31 16:14:19 2018 Train Epoch: 10 [697600/1281167 (54%)]\tLoss: 3.717942\n",
            "Wed Oct 31 16:14:46 2018 Train Epoch: 10 [704000/1281167 (55%)]\tLoss: 3.287761\n",
            "Wed Oct 31 16:15:13 2018 Train Epoch: 10 [710400/1281167 (55%)]\tLoss: 3.193040\n",
            "Wed Oct 31 16:15:40 2018 Train Epoch: 10 [716800/1281167 (56%)]\tLoss: 3.802191\n",
            "Wed Oct 31 16:16:08 2018 Train Epoch: 10 [723200/1281167 (56%)]\tLoss: 3.093071\n",
            "Wed Oct 31 16:16:35 2018 Train Epoch: 10 [729600/1281167 (57%)]\tLoss: 3.617354\n",
            "Wed Oct 31 16:17:02 2018 Train Epoch: 10 [736000/1281167 (57%)]\tLoss: 3.756838\n",
            "Wed Oct 31 16:17:29 2018 Train Epoch: 10 [742400/1281167 (58%)]\tLoss: 3.449293\n",
            "Wed Oct 31 16:17:57 2018 Train Epoch: 10 [748800/1281167 (58%)]\tLoss: 3.724868\n",
            "Wed Oct 31 16:18:24 2018 Train Epoch: 10 [755200/1281167 (59%)]\tLoss: 4.114362\n",
            "Wed Oct 31 16:18:51 2018 Train Epoch: 10 [761600/1281167 (59%)]\tLoss: 2.982445\n",
            "Wed Oct 31 16:19:18 2018 Train Epoch: 10 [768000/1281167 (60%)]\tLoss: 4.001621\n",
            "Wed Oct 31 16:19:45 2018 Train Epoch: 10 [774400/1281167 (60%)]\tLoss: 3.649089\n",
            "Wed Oct 31 16:20:13 2018 Train Epoch: 10 [780800/1281167 (61%)]\tLoss: 3.634614\n",
            "Wed Oct 31 16:20:40 2018 Train Epoch: 10 [787200/1281167 (61%)]\tLoss: 3.727821\n",
            "Wed Oct 31 16:21:07 2018 Train Epoch: 10 [793600/1281167 (62%)]\tLoss: 3.572721\n",
            "Wed Oct 31 16:21:34 2018 Train Epoch: 10 [800000/1281167 (62%)]\tLoss: 3.574508\n",
            "Wed Oct 31 16:22:01 2018 Train Epoch: 10 [806400/1281167 (63%)]\tLoss: 3.437497\n",
            "Wed Oct 31 16:22:29 2018 Train Epoch: 10 [812800/1281167 (63%)]\tLoss: 3.395019\n",
            "Wed Oct 31 16:22:56 2018 Train Epoch: 10 [819200/1281167 (64%)]\tLoss: 3.980067\n",
            "Wed Oct 31 16:23:23 2018 Train Epoch: 10 [825600/1281167 (64%)]\tLoss: 4.130529\n",
            "Wed Oct 31 16:23:50 2018 Train Epoch: 10 [832000/1281167 (65%)]\tLoss: 3.338623\n",
            "Wed Oct 31 16:24:17 2018 Train Epoch: 10 [838400/1281167 (65%)]\tLoss: 3.351863\n",
            "Wed Oct 31 16:24:45 2018 Train Epoch: 10 [844800/1281167 (66%)]\tLoss: 3.616318\n",
            "Wed Oct 31 16:25:12 2018 Train Epoch: 10 [851200/1281167 (66%)]\tLoss: 3.587648\n",
            "Wed Oct 31 16:25:39 2018 Train Epoch: 10 [857600/1281167 (67%)]\tLoss: 3.675504\n",
            "Wed Oct 31 16:26:06 2018 Train Epoch: 10 [864000/1281167 (67%)]\tLoss: 3.328657\n",
            "Wed Oct 31 16:26:34 2018 Train Epoch: 10 [870400/1281167 (68%)]\tLoss: 3.536142\n",
            "Wed Oct 31 16:27:01 2018 Train Epoch: 10 [876800/1281167 (68%)]\tLoss: 3.588526\n",
            "Wed Oct 31 16:27:28 2018 Train Epoch: 10 [883200/1281167 (69%)]\tLoss: 3.330069\n",
            "Wed Oct 31 16:27:55 2018 Train Epoch: 10 [889600/1281167 (69%)]\tLoss: 3.737013\n",
            "Wed Oct 31 16:28:22 2018 Train Epoch: 10 [896000/1281167 (70%)]\tLoss: 3.553157\n",
            "Wed Oct 31 16:28:50 2018 Train Epoch: 10 [902400/1281167 (70%)]\tLoss: 3.448984\n",
            "Wed Oct 31 16:29:17 2018 Train Epoch: 10 [908800/1281167 (71%)]\tLoss: 3.538543\n",
            "Wed Oct 31 16:29:44 2018 Train Epoch: 10 [915200/1281167 (71%)]\tLoss: 3.556573\n",
            "Wed Oct 31 16:30:11 2018 Train Epoch: 10 [921600/1281167 (72%)]\tLoss: 3.973280\n",
            "Wed Oct 31 16:30:39 2018 Train Epoch: 10 [928000/1281167 (72%)]\tLoss: 2.709157\n",
            "Wed Oct 31 16:31:06 2018 Train Epoch: 10 [934400/1281167 (73%)]\tLoss: 3.864651\n",
            "Wed Oct 31 16:31:33 2018 Train Epoch: 10 [940800/1281167 (73%)]\tLoss: 3.410080\n",
            "Wed Oct 31 16:32:00 2018 Train Epoch: 10 [947200/1281167 (74%)]\tLoss: 3.654765\n",
            "Wed Oct 31 16:32:28 2018 Train Epoch: 10 [953600/1281167 (74%)]\tLoss: 3.351915\n",
            "Wed Oct 31 16:32:55 2018 Train Epoch: 10 [960000/1281167 (75%)]\tLoss: 3.067381\n",
            "Wed Oct 31 16:33:22 2018 Train Epoch: 10 [966400/1281167 (75%)]\tLoss: 3.758313\n",
            "Wed Oct 31 16:33:49 2018 Train Epoch: 10 [972800/1281167 (76%)]\tLoss: 3.732215\n",
            "Wed Oct 31 16:34:17 2018 Train Epoch: 10 [979200/1281167 (76%)]\tLoss: 3.477532\n",
            "Wed Oct 31 16:34:44 2018 Train Epoch: 10 [985600/1281167 (77%)]\tLoss: 4.202027\n",
            "Wed Oct 31 16:35:11 2018 Train Epoch: 10 [992000/1281167 (77%)]\tLoss: 3.443022\n",
            "Wed Oct 31 16:35:38 2018 Train Epoch: 10 [998400/1281167 (78%)]\tLoss: 2.767716\n",
            "Wed Oct 31 16:36:06 2018 Train Epoch: 10 [1004800/1281167 (78%)]\tLoss: 3.288035\n",
            "Wed Oct 31 16:36:33 2018 Train Epoch: 10 [1011200/1281167 (79%)]\tLoss: 3.543726\n",
            "Wed Oct 31 16:37:00 2018 Train Epoch: 10 [1017600/1281167 (79%)]\tLoss: 3.575966\n",
            "Wed Oct 31 16:37:27 2018 Train Epoch: 10 [1024000/1281167 (80%)]\tLoss: 3.640542\n",
            "Wed Oct 31 16:37:55 2018 Train Epoch: 10 [1030400/1281167 (80%)]\tLoss: 3.751686\n",
            "Wed Oct 31 16:38:22 2018 Train Epoch: 10 [1036800/1281167 (81%)]\tLoss: 3.914400\n",
            "Wed Oct 31 16:38:49 2018 Train Epoch: 10 [1043200/1281167 (81%)]\tLoss: 3.411484\n",
            "Wed Oct 31 16:39:16 2018 Train Epoch: 10 [1049600/1281167 (82%)]\tLoss: 4.063828\n",
            "Wed Oct 31 16:39:43 2018 Train Epoch: 10 [1056000/1281167 (82%)]\tLoss: 3.681479\n",
            "Wed Oct 31 16:40:11 2018 Train Epoch: 10 [1062400/1281167 (83%)]\tLoss: 4.088079\n",
            "Wed Oct 31 16:40:38 2018 Train Epoch: 10 [1068800/1281167 (83%)]\tLoss: 3.240062\n",
            "Wed Oct 31 16:41:05 2018 Train Epoch: 10 [1075200/1281167 (84%)]\tLoss: 3.060276\n",
            "Wed Oct 31 16:41:32 2018 Train Epoch: 10 [1081600/1281167 (84%)]\tLoss: 3.443214\n",
            "Wed Oct 31 16:42:00 2018 Train Epoch: 10 [1088000/1281167 (85%)]\tLoss: 4.060156\n",
            "Wed Oct 31 16:42:27 2018 Train Epoch: 10 [1094400/1281167 (85%)]\tLoss: 3.502830\n",
            "Wed Oct 31 16:42:54 2018 Train Epoch: 10 [1100800/1281167 (86%)]\tLoss: 3.421954\n",
            "Wed Oct 31 16:43:22 2018 Train Epoch: 10 [1107200/1281167 (86%)]\tLoss: 3.291905\n",
            "Wed Oct 31 16:43:49 2018 Train Epoch: 10 [1113600/1281167 (87%)]\tLoss: 3.599510\n",
            "Wed Oct 31 16:44:16 2018 Train Epoch: 10 [1120000/1281167 (87%)]\tLoss: 3.445481\n",
            "Wed Oct 31 16:44:43 2018 Train Epoch: 10 [1126400/1281167 (88%)]\tLoss: 3.534400\n",
            "Wed Oct 31 16:45:11 2018 Train Epoch: 10 [1132800/1281167 (88%)]\tLoss: 3.768376\n",
            "Wed Oct 31 16:45:38 2018 Train Epoch: 10 [1139200/1281167 (89%)]\tLoss: 3.375925\n",
            "Wed Oct 31 16:46:05 2018 Train Epoch: 10 [1145600/1281167 (89%)]\tLoss: 3.469473\n",
            "Wed Oct 31 16:46:32 2018 Train Epoch: 10 [1152000/1281167 (90%)]\tLoss: 3.873840\n",
            "Wed Oct 31 16:46:59 2018 Train Epoch: 10 [1158400/1281167 (90%)]\tLoss: 3.302370\n",
            "Wed Oct 31 16:47:27 2018 Train Epoch: 10 [1164800/1281167 (91%)]\tLoss: 3.698060\n",
            "Wed Oct 31 16:47:54 2018 Train Epoch: 10 [1171200/1281167 (91%)]\tLoss: 3.307515\n",
            "Wed Oct 31 16:48:21 2018 Train Epoch: 10 [1177600/1281167 (92%)]\tLoss: 3.216099\n",
            "Wed Oct 31 16:48:49 2018 Train Epoch: 10 [1184000/1281167 (92%)]\tLoss: 3.793152\n",
            "Wed Oct 31 16:49:16 2018 Train Epoch: 10 [1190400/1281167 (93%)]\tLoss: 3.458637\n",
            "Wed Oct 31 16:49:43 2018 Train Epoch: 10 [1196800/1281167 (93%)]\tLoss: 3.364878\n",
            "Wed Oct 31 16:50:10 2018 Train Epoch: 10 [1203200/1281167 (94%)]\tLoss: 3.460462\n",
            "Wed Oct 31 16:50:38 2018 Train Epoch: 10 [1209600/1281167 (94%)]\tLoss: 3.355421\n",
            "Wed Oct 31 16:51:05 2018 Train Epoch: 10 [1216000/1281167 (95%)]\tLoss: 3.924112\n",
            "Wed Oct 31 16:51:32 2018 Train Epoch: 10 [1222400/1281167 (95%)]\tLoss: 3.559656\n",
            "Wed Oct 31 16:51:59 2018 Train Epoch: 10 [1228800/1281167 (96%)]\tLoss: 3.821123\n",
            "Wed Oct 31 16:52:27 2018 Train Epoch: 10 [1235200/1281167 (96%)]\tLoss: 3.156069\n",
            "Wed Oct 31 16:52:54 2018 Train Epoch: 10 [1241600/1281167 (97%)]\tLoss: 3.785736\n",
            "Wed Oct 31 16:53:21 2018 Train Epoch: 10 [1248000/1281167 (97%)]\tLoss: 3.974550\n",
            "Wed Oct 31 16:53:48 2018 Train Epoch: 10 [1254400/1281167 (98%)]\tLoss: 3.763071\n",
            "Wed Oct 31 16:54:16 2018 Train Epoch: 10 [1260800/1281167 (98%)]\tLoss: 3.898304\n",
            "Wed Oct 31 16:54:43 2018 Train Epoch: 10 [1267200/1281167 (99%)]\tLoss: 3.050289\n",
            "Wed Oct 31 16:55:10 2018 Train Epoch: 10 [1273600/1281167 (99%)]\tLoss: 3.525686\n",
            "Wed Oct 31 16:55:37 2018 Train Epoch: 10 [1280000/1281167 (100%)]\tLoss: 3.553408\n",
            "\n",
            "Test set: Average loss: 3.4039, Accuracy: 14456/50000 (29%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/010.pt\n",
            "\n",
            "Wed Oct 31 16:59:42 2018 Train Epoch: 11 [0/1281167 (0%)]\tLoss: 4.282572\n",
            "Wed Oct 31 17:00:10 2018 Train Epoch: 11 [6400/1281167 (0%)]\tLoss: 3.521922\n",
            "Wed Oct 31 17:00:37 2018 Train Epoch: 11 [12800/1281167 (1%)]\tLoss: 3.491275\n",
            "Wed Oct 31 17:01:04 2018 Train Epoch: 11 [19200/1281167 (1%)]\tLoss: 3.185103\n",
            "Wed Oct 31 17:01:31 2018 Train Epoch: 11 [25600/1281167 (2%)]\tLoss: 3.011152\n",
            "Wed Oct 31 17:01:58 2018 Train Epoch: 11 [32000/1281167 (2%)]\tLoss: 3.859104\n",
            "Wed Oct 31 17:02:26 2018 Train Epoch: 11 [38400/1281167 (3%)]\tLoss: 3.760078\n",
            "Wed Oct 31 17:02:53 2018 Train Epoch: 11 [44800/1281167 (3%)]\tLoss: 3.396173\n",
            "Wed Oct 31 17:03:20 2018 Train Epoch: 11 [51200/1281167 (4%)]\tLoss: 3.726719\n",
            "Wed Oct 31 17:03:47 2018 Train Epoch: 11 [57600/1281167 (4%)]\tLoss: 2.923421\n",
            "Wed Oct 31 17:04:15 2018 Train Epoch: 11 [64000/1281167 (5%)]\tLoss: 3.408212\n",
            "Wed Oct 31 17:04:42 2018 Train Epoch: 11 [70400/1281167 (5%)]\tLoss: 3.412443\n",
            "Wed Oct 31 17:05:09 2018 Train Epoch: 11 [76800/1281167 (6%)]\tLoss: 3.648983\n",
            "Wed Oct 31 17:05:36 2018 Train Epoch: 11 [83200/1281167 (6%)]\tLoss: 3.663152\n",
            "Wed Oct 31 17:06:04 2018 Train Epoch: 11 [89600/1281167 (7%)]\tLoss: 3.480839\n",
            "Wed Oct 31 17:06:31 2018 Train Epoch: 11 [96000/1281167 (7%)]\tLoss: 3.727593\n",
            "Wed Oct 31 17:06:58 2018 Train Epoch: 11 [102400/1281167 (8%)]\tLoss: 3.311239\n",
            "Wed Oct 31 17:07:25 2018 Train Epoch: 11 [108800/1281167 (8%)]\tLoss: 3.299306\n",
            "Wed Oct 31 17:07:52 2018 Train Epoch: 11 [115200/1281167 (9%)]\tLoss: 3.468434\n",
            "Wed Oct 31 17:08:20 2018 Train Epoch: 11 [121600/1281167 (9%)]\tLoss: 3.574105\n",
            "Wed Oct 31 17:08:47 2018 Train Epoch: 11 [128000/1281167 (10%)]\tLoss: 3.778764\n",
            "Wed Oct 31 17:09:14 2018 Train Epoch: 11 [134400/1281167 (10%)]\tLoss: 3.011448\n",
            "Wed Oct 31 17:09:41 2018 Train Epoch: 11 [140800/1281167 (11%)]\tLoss: 3.520703\n",
            "Wed Oct 31 17:10:09 2018 Train Epoch: 11 [147200/1281167 (11%)]\tLoss: 3.735519\n",
            "Wed Oct 31 17:10:36 2018 Train Epoch: 11 [153600/1281167 (12%)]\tLoss: 3.408822\n",
            "Wed Oct 31 17:11:03 2018 Train Epoch: 11 [160000/1281167 (12%)]\tLoss: 3.375544\n",
            "Wed Oct 31 17:11:30 2018 Train Epoch: 11 [166400/1281167 (13%)]\tLoss: 3.858628\n",
            "Wed Oct 31 17:11:57 2018 Train Epoch: 11 [172800/1281167 (13%)]\tLoss: 3.592692\n",
            "Wed Oct 31 17:12:25 2018 Train Epoch: 11 [179200/1281167 (14%)]\tLoss: 4.160414\n",
            "Wed Oct 31 17:12:52 2018 Train Epoch: 11 [185600/1281167 (14%)]\tLoss: 3.091557\n",
            "Wed Oct 31 17:13:19 2018 Train Epoch: 11 [192000/1281167 (15%)]\tLoss: 3.709059\n",
            "Wed Oct 31 17:13:46 2018 Train Epoch: 11 [198400/1281167 (15%)]\tLoss: 3.863647\n",
            "Wed Oct 31 17:14:13 2018 Train Epoch: 11 [204800/1281167 (16%)]\tLoss: 2.969796\n",
            "Wed Oct 31 17:14:41 2018 Train Epoch: 11 [211200/1281167 (16%)]\tLoss: 3.235607\n",
            "Wed Oct 31 17:15:08 2018 Train Epoch: 11 [217600/1281167 (17%)]\tLoss: 3.261861\n",
            "Wed Oct 31 17:15:35 2018 Train Epoch: 11 [224000/1281167 (17%)]\tLoss: 3.321249\n",
            "Wed Oct 31 17:16:02 2018 Train Epoch: 11 [230400/1281167 (18%)]\tLoss: 3.636182\n",
            "Wed Oct 31 17:16:30 2018 Train Epoch: 11 [236800/1281167 (18%)]\tLoss: 3.298407\n",
            "Wed Oct 31 17:16:57 2018 Train Epoch: 11 [243200/1281167 (19%)]\tLoss: 3.099961\n",
            "Wed Oct 31 17:17:24 2018 Train Epoch: 11 [249600/1281167 (19%)]\tLoss: 3.471647\n",
            "Wed Oct 31 17:17:51 2018 Train Epoch: 11 [256000/1281167 (20%)]\tLoss: 3.373467\n",
            "Wed Oct 31 17:18:18 2018 Train Epoch: 11 [262400/1281167 (20%)]\tLoss: 4.021900\n",
            "Wed Oct 31 17:18:46 2018 Train Epoch: 11 [268800/1281167 (21%)]\tLoss: 3.399215\n",
            "Wed Oct 31 17:19:13 2018 Train Epoch: 11 [275200/1281167 (21%)]\tLoss: 3.430637\n",
            "Wed Oct 31 17:19:40 2018 Train Epoch: 11 [281600/1281167 (22%)]\tLoss: 4.058047\n",
            "Wed Oct 31 17:20:07 2018 Train Epoch: 11 [288000/1281167 (22%)]\tLoss: 3.578795\n",
            "Wed Oct 31 17:20:35 2018 Train Epoch: 11 [294400/1281167 (23%)]\tLoss: 3.061045\n",
            "Wed Oct 31 17:21:02 2018 Train Epoch: 11 [300800/1281167 (23%)]\tLoss: 3.813787\n",
            "Wed Oct 31 17:21:29 2018 Train Epoch: 11 [307200/1281167 (24%)]\tLoss: 3.802504\n",
            "Wed Oct 31 17:21:56 2018 Train Epoch: 11 [313600/1281167 (24%)]\tLoss: 3.424132\n",
            "Wed Oct 31 17:22:23 2018 Train Epoch: 11 [320000/1281167 (25%)]\tLoss: 3.776962\n",
            "Wed Oct 31 17:22:51 2018 Train Epoch: 11 [326400/1281167 (25%)]\tLoss: 3.869742\n",
            "Wed Oct 31 17:23:18 2018 Train Epoch: 11 [332800/1281167 (26%)]\tLoss: 3.451670\n",
            "Wed Oct 31 17:23:45 2018 Train Epoch: 11 [339200/1281167 (26%)]\tLoss: 3.944736\n",
            "Wed Oct 31 17:24:12 2018 Train Epoch: 11 [345600/1281167 (27%)]\tLoss: 3.508703\n",
            "Wed Oct 31 17:24:40 2018 Train Epoch: 11 [352000/1281167 (27%)]\tLoss: 3.541577\n",
            "Wed Oct 31 17:25:07 2018 Train Epoch: 11 [358400/1281167 (28%)]\tLoss: 3.274345\n",
            "Wed Oct 31 17:25:34 2018 Train Epoch: 11 [364800/1281167 (28%)]\tLoss: 4.134264\n",
            "Wed Oct 31 17:26:01 2018 Train Epoch: 11 [371200/1281167 (29%)]\tLoss: 3.875900\n",
            "Wed Oct 31 17:26:29 2018 Train Epoch: 11 [377600/1281167 (29%)]\tLoss: 3.475007\n",
            "Wed Oct 31 17:26:56 2018 Train Epoch: 11 [384000/1281167 (30%)]\tLoss: 3.577088\n",
            "Wed Oct 31 17:27:23 2018 Train Epoch: 11 [390400/1281167 (30%)]\tLoss: 2.845100\n",
            "Wed Oct 31 17:27:50 2018 Train Epoch: 11 [396800/1281167 (31%)]\tLoss: 3.794591\n",
            "Wed Oct 31 17:28:17 2018 Train Epoch: 11 [403200/1281167 (31%)]\tLoss: 3.721148\n",
            "Wed Oct 31 17:28:45 2018 Train Epoch: 11 [409600/1281167 (32%)]\tLoss: 3.283949\n",
            "Wed Oct 31 17:29:12 2018 Train Epoch: 11 [416000/1281167 (32%)]\tLoss: 3.603334\n",
            "Wed Oct 31 17:29:39 2018 Train Epoch: 11 [422400/1281167 (33%)]\tLoss: 3.543336\n",
            "Wed Oct 31 17:30:06 2018 Train Epoch: 11 [428800/1281167 (33%)]\tLoss: 3.423499\n",
            "Wed Oct 31 17:30:34 2018 Train Epoch: 11 [435200/1281167 (34%)]\tLoss: 3.691260\n",
            "Wed Oct 31 17:31:01 2018 Train Epoch: 11 [441600/1281167 (34%)]\tLoss: 2.995986\n",
            "Wed Oct 31 17:31:28 2018 Train Epoch: 11 [448000/1281167 (35%)]\tLoss: 3.326880\n",
            "Wed Oct 31 17:31:55 2018 Train Epoch: 11 [454400/1281167 (35%)]\tLoss: 3.712535\n",
            "Wed Oct 31 17:32:22 2018 Train Epoch: 11 [460800/1281167 (36%)]\tLoss: 3.386360\n",
            "Wed Oct 31 17:32:50 2018 Train Epoch: 11 [467200/1281167 (36%)]\tLoss: 3.929150\n",
            "Wed Oct 31 17:33:17 2018 Train Epoch: 11 [473600/1281167 (37%)]\tLoss: 3.486910\n",
            "Wed Oct 31 17:33:44 2018 Train Epoch: 11 [480000/1281167 (37%)]\tLoss: 3.605674\n",
            "Wed Oct 31 17:34:11 2018 Train Epoch: 11 [486400/1281167 (38%)]\tLoss: 3.397769\n",
            "Wed Oct 31 17:34:38 2018 Train Epoch: 11 [492800/1281167 (38%)]\tLoss: 3.497022\n",
            "Wed Oct 31 17:35:06 2018 Train Epoch: 11 [499200/1281167 (39%)]\tLoss: 3.952912\n",
            "Wed Oct 31 17:35:33 2018 Train Epoch: 11 [505600/1281167 (39%)]\tLoss: 3.531481\n",
            "Wed Oct 31 17:36:00 2018 Train Epoch: 11 [512000/1281167 (40%)]\tLoss: 3.809212\n",
            "Wed Oct 31 17:36:27 2018 Train Epoch: 11 [518400/1281167 (40%)]\tLoss: 3.756641\n",
            "Wed Oct 31 17:36:54 2018 Train Epoch: 11 [524800/1281167 (41%)]\tLoss: 3.479549\n",
            "Wed Oct 31 17:37:22 2018 Train Epoch: 11 [531200/1281167 (41%)]\tLoss: 3.547228\n",
            "Wed Oct 31 17:37:49 2018 Train Epoch: 11 [537600/1281167 (42%)]\tLoss: 3.460702\n",
            "Wed Oct 31 17:38:16 2018 Train Epoch: 11 [544000/1281167 (42%)]\tLoss: 3.624800\n",
            "Wed Oct 31 17:38:43 2018 Train Epoch: 11 [550400/1281167 (43%)]\tLoss: 3.843789\n",
            "Wed Oct 31 17:39:10 2018 Train Epoch: 11 [556800/1281167 (43%)]\tLoss: 3.356937\n",
            "Wed Oct 31 17:39:38 2018 Train Epoch: 11 [563200/1281167 (44%)]\tLoss: 3.422065\n",
            "Wed Oct 31 17:40:05 2018 Train Epoch: 11 [569600/1281167 (44%)]\tLoss: 3.594241\n",
            "Wed Oct 31 17:40:32 2018 Train Epoch: 11 [576000/1281167 (45%)]\tLoss: 3.702776\n",
            "Wed Oct 31 17:40:59 2018 Train Epoch: 11 [582400/1281167 (45%)]\tLoss: 3.456187\n",
            "Wed Oct 31 17:41:26 2018 Train Epoch: 11 [588800/1281167 (46%)]\tLoss: 3.454775\n",
            "Wed Oct 31 17:41:54 2018 Train Epoch: 11 [595200/1281167 (46%)]\tLoss: 3.480097\n",
            "Wed Oct 31 17:42:21 2018 Train Epoch: 11 [601600/1281167 (47%)]\tLoss: 3.583997\n",
            "Wed Oct 31 17:42:48 2018 Train Epoch: 11 [608000/1281167 (47%)]\tLoss: 3.818624\n",
            "Wed Oct 31 17:43:15 2018 Train Epoch: 11 [614400/1281167 (48%)]\tLoss: 3.858172\n",
            "Wed Oct 31 17:43:42 2018 Train Epoch: 11 [620800/1281167 (48%)]\tLoss: 3.771169\n",
            "Wed Oct 31 17:44:10 2018 Train Epoch: 11 [627200/1281167 (49%)]\tLoss: 3.742476\n",
            "Wed Oct 31 17:44:37 2018 Train Epoch: 11 [633600/1281167 (49%)]\tLoss: 3.371519\n",
            "Wed Oct 31 17:45:04 2018 Train Epoch: 11 [640000/1281167 (50%)]\tLoss: 3.564159\n",
            "Wed Oct 31 17:45:31 2018 Train Epoch: 11 [646400/1281167 (50%)]\tLoss: 3.758944\n",
            "Wed Oct 31 17:45:59 2018 Train Epoch: 11 [652800/1281167 (51%)]\tLoss: 3.629752\n",
            "Wed Oct 31 17:46:26 2018 Train Epoch: 11 [659200/1281167 (51%)]\tLoss: 3.224289\n",
            "Wed Oct 31 17:46:53 2018 Train Epoch: 11 [665600/1281167 (52%)]\tLoss: 3.670687\n",
            "Wed Oct 31 17:47:20 2018 Train Epoch: 11 [672000/1281167 (52%)]\tLoss: 3.472269\n",
            "Wed Oct 31 17:47:47 2018 Train Epoch: 11 [678400/1281167 (53%)]\tLoss: 3.192438\n",
            "Wed Oct 31 17:48:15 2018 Train Epoch: 11 [684800/1281167 (53%)]\tLoss: 3.652178\n",
            "Wed Oct 31 17:48:42 2018 Train Epoch: 11 [691200/1281167 (54%)]\tLoss: 3.365521\n",
            "Wed Oct 31 17:49:09 2018 Train Epoch: 11 [697600/1281167 (54%)]\tLoss: 3.868881\n",
            "Wed Oct 31 17:49:36 2018 Train Epoch: 11 [704000/1281167 (55%)]\tLoss: 2.932616\n",
            "Wed Oct 31 17:50:03 2018 Train Epoch: 11 [710400/1281167 (55%)]\tLoss: 3.678881\n",
            "Wed Oct 31 17:50:31 2018 Train Epoch: 11 [716800/1281167 (56%)]\tLoss: 3.468520\n",
            "Wed Oct 31 17:50:58 2018 Train Epoch: 11 [723200/1281167 (56%)]\tLoss: 3.344133\n",
            "Wed Oct 31 17:51:25 2018 Train Epoch: 11 [729600/1281167 (57%)]\tLoss: 3.565461\n",
            "Wed Oct 31 17:51:52 2018 Train Epoch: 11 [736000/1281167 (57%)]\tLoss: 3.324144\n",
            "Wed Oct 31 17:52:20 2018 Train Epoch: 11 [742400/1281167 (58%)]\tLoss: 3.799976\n",
            "Wed Oct 31 17:52:47 2018 Train Epoch: 11 [748800/1281167 (58%)]\tLoss: 3.315783\n",
            "Wed Oct 31 17:53:14 2018 Train Epoch: 11 [755200/1281167 (59%)]\tLoss: 3.628438\n",
            "Wed Oct 31 17:53:41 2018 Train Epoch: 11 [761600/1281167 (59%)]\tLoss: 3.747131\n",
            "Wed Oct 31 17:54:09 2018 Train Epoch: 11 [768000/1281167 (60%)]\tLoss: 3.716907\n",
            "Wed Oct 31 17:54:36 2018 Train Epoch: 11 [774400/1281167 (60%)]\tLoss: 3.248858\n",
            "Wed Oct 31 17:55:03 2018 Train Epoch: 11 [780800/1281167 (61%)]\tLoss: 3.645624\n",
            "Wed Oct 31 17:55:30 2018 Train Epoch: 11 [787200/1281167 (61%)]\tLoss: 3.902914\n",
            "Wed Oct 31 17:55:57 2018 Train Epoch: 11 [793600/1281167 (62%)]\tLoss: 3.407535\n",
            "Wed Oct 31 17:56:25 2018 Train Epoch: 11 [800000/1281167 (62%)]\tLoss: 3.690935\n",
            "Wed Oct 31 17:56:52 2018 Train Epoch: 11 [806400/1281167 (63%)]\tLoss: 3.021004\n",
            "Wed Oct 31 17:57:19 2018 Train Epoch: 11 [812800/1281167 (63%)]\tLoss: 3.618211\n",
            "Wed Oct 31 17:57:46 2018 Train Epoch: 11 [819200/1281167 (64%)]\tLoss: 3.446223\n",
            "Wed Oct 31 17:58:14 2018 Train Epoch: 11 [825600/1281167 (64%)]\tLoss: 3.417643\n",
            "Wed Oct 31 17:58:41 2018 Train Epoch: 11 [832000/1281167 (65%)]\tLoss: 3.485616\n",
            "Wed Oct 31 17:59:08 2018 Train Epoch: 11 [838400/1281167 (65%)]\tLoss: 3.886720\n",
            "Wed Oct 31 17:59:35 2018 Train Epoch: 11 [844800/1281167 (66%)]\tLoss: 3.467914\n",
            "Wed Oct 31 18:00:03 2018 Train Epoch: 11 [851200/1281167 (66%)]\tLoss: 3.485646\n",
            "Wed Oct 31 18:00:30 2018 Train Epoch: 11 [857600/1281167 (67%)]\tLoss: 4.042252\n",
            "Wed Oct 31 18:00:57 2018 Train Epoch: 11 [864000/1281167 (67%)]\tLoss: 3.281103\n",
            "Wed Oct 31 18:01:24 2018 Train Epoch: 11 [870400/1281167 (68%)]\tLoss: 3.593859\n",
            "Wed Oct 31 18:01:52 2018 Train Epoch: 11 [876800/1281167 (68%)]\tLoss: 3.146703\n",
            "Wed Oct 31 18:02:19 2018 Train Epoch: 11 [883200/1281167 (69%)]\tLoss: 3.925161\n",
            "Wed Oct 31 18:02:46 2018 Train Epoch: 11 [889600/1281167 (69%)]\tLoss: 3.991846\n",
            "Wed Oct 31 18:03:13 2018 Train Epoch: 11 [896000/1281167 (70%)]\tLoss: 3.939514\n",
            "Wed Oct 31 18:03:40 2018 Train Epoch: 11 [902400/1281167 (70%)]\tLoss: 3.242399\n",
            "Wed Oct 31 18:04:08 2018 Train Epoch: 11 [908800/1281167 (71%)]\tLoss: 3.280979\n",
            "Wed Oct 31 18:04:35 2018 Train Epoch: 11 [915200/1281167 (71%)]\tLoss: 3.340878\n",
            "Wed Oct 31 18:05:02 2018 Train Epoch: 11 [921600/1281167 (72%)]\tLoss: 2.843956\n",
            "Wed Oct 31 18:05:29 2018 Train Epoch: 11 [928000/1281167 (72%)]\tLoss: 3.348699\n",
            "Wed Oct 31 18:05:57 2018 Train Epoch: 11 [934400/1281167 (73%)]\tLoss: 3.631223\n",
            "Wed Oct 31 18:06:24 2018 Train Epoch: 11 [940800/1281167 (73%)]\tLoss: 3.539561\n",
            "Wed Oct 31 18:06:51 2018 Train Epoch: 11 [947200/1281167 (74%)]\tLoss: 3.693549\n",
            "Wed Oct 31 18:07:18 2018 Train Epoch: 11 [953600/1281167 (74%)]\tLoss: 3.451800\n",
            "Wed Oct 31 18:07:46 2018 Train Epoch: 11 [960000/1281167 (75%)]\tLoss: 2.948283\n",
            "Wed Oct 31 18:08:13 2018 Train Epoch: 11 [966400/1281167 (75%)]\tLoss: 3.828198\n",
            "Wed Oct 31 18:08:40 2018 Train Epoch: 11 [972800/1281167 (76%)]\tLoss: 4.004023\n",
            "Wed Oct 31 18:09:07 2018 Train Epoch: 11 [979200/1281167 (76%)]\tLoss: 3.253125\n",
            "Wed Oct 31 18:09:34 2018 Train Epoch: 11 [985600/1281167 (77%)]\tLoss: 3.737822\n",
            "Wed Oct 31 18:10:02 2018 Train Epoch: 11 [992000/1281167 (77%)]\tLoss: 3.327301\n",
            "Wed Oct 31 18:10:29 2018 Train Epoch: 11 [998400/1281167 (78%)]\tLoss: 3.681230\n",
            "Wed Oct 31 18:10:56 2018 Train Epoch: 11 [1004800/1281167 (78%)]\tLoss: 4.053688\n",
            "Wed Oct 31 18:11:23 2018 Train Epoch: 11 [1011200/1281167 (79%)]\tLoss: 3.787356\n",
            "Wed Oct 31 18:11:51 2018 Train Epoch: 11 [1017600/1281167 (79%)]\tLoss: 3.393957\n",
            "Wed Oct 31 18:12:18 2018 Train Epoch: 11 [1024000/1281167 (80%)]\tLoss: 3.228400\n",
            "Wed Oct 31 18:12:45 2018 Train Epoch: 11 [1030400/1281167 (80%)]\tLoss: 3.411250\n",
            "Wed Oct 31 18:13:12 2018 Train Epoch: 11 [1036800/1281167 (81%)]\tLoss: 3.633178\n",
            "Wed Oct 31 18:13:40 2018 Train Epoch: 11 [1043200/1281167 (81%)]\tLoss: 3.441147\n",
            "Wed Oct 31 18:14:07 2018 Train Epoch: 11 [1049600/1281167 (82%)]\tLoss: 3.453714\n",
            "Wed Oct 31 18:14:34 2018 Train Epoch: 11 [1056000/1281167 (82%)]\tLoss: 3.745012\n",
            "Wed Oct 31 18:15:01 2018 Train Epoch: 11 [1062400/1281167 (83%)]\tLoss: 3.428047\n",
            "Wed Oct 31 18:15:28 2018 Train Epoch: 11 [1068800/1281167 (83%)]\tLoss: 3.442027\n",
            "Wed Oct 31 18:15:56 2018 Train Epoch: 11 [1075200/1281167 (84%)]\tLoss: 3.283211\n",
            "Wed Oct 31 18:16:23 2018 Train Epoch: 11 [1081600/1281167 (84%)]\tLoss: 3.566779\n",
            "Wed Oct 31 18:16:50 2018 Train Epoch: 11 [1088000/1281167 (85%)]\tLoss: 3.235578\n",
            "Wed Oct 31 18:17:17 2018 Train Epoch: 11 [1094400/1281167 (85%)]\tLoss: 4.255979\n",
            "Wed Oct 31 18:17:44 2018 Train Epoch: 11 [1100800/1281167 (86%)]\tLoss: 3.332133\n",
            "Wed Oct 31 18:18:12 2018 Train Epoch: 11 [1107200/1281167 (86%)]\tLoss: 3.549577\n",
            "Wed Oct 31 18:18:39 2018 Train Epoch: 11 [1113600/1281167 (87%)]\tLoss: 3.473535\n",
            "Wed Oct 31 18:19:06 2018 Train Epoch: 11 [1120000/1281167 (87%)]\tLoss: 3.192321\n",
            "Wed Oct 31 18:19:33 2018 Train Epoch: 11 [1126400/1281167 (88%)]\tLoss: 3.750339\n",
            "Wed Oct 31 18:20:00 2018 Train Epoch: 11 [1132800/1281167 (88%)]\tLoss: 3.370862\n",
            "Wed Oct 31 18:20:28 2018 Train Epoch: 11 [1139200/1281167 (89%)]\tLoss: 3.489172\n",
            "Wed Oct 31 18:20:55 2018 Train Epoch: 11 [1145600/1281167 (89%)]\tLoss: 3.737939\n",
            "Wed Oct 31 18:21:22 2018 Train Epoch: 11 [1152000/1281167 (90%)]\tLoss: 3.318105\n",
            "Wed Oct 31 18:21:49 2018 Train Epoch: 11 [1158400/1281167 (90%)]\tLoss: 3.501429\n",
            "Wed Oct 31 18:22:16 2018 Train Epoch: 11 [1164800/1281167 (91%)]\tLoss: 3.261177\n",
            "Wed Oct 31 18:22:43 2018 Train Epoch: 11 [1171200/1281167 (91%)]\tLoss: 3.625109\n",
            "Wed Oct 31 18:23:11 2018 Train Epoch: 11 [1177600/1281167 (92%)]\tLoss: 3.416716\n",
            "Wed Oct 31 18:23:38 2018 Train Epoch: 11 [1184000/1281167 (92%)]\tLoss: 3.449516\n",
            "Wed Oct 31 18:24:05 2018 Train Epoch: 11 [1190400/1281167 (93%)]\tLoss: 4.025728\n",
            "Wed Oct 31 18:24:32 2018 Train Epoch: 11 [1196800/1281167 (93%)]\tLoss: 3.838979\n",
            "Wed Oct 31 18:24:59 2018 Train Epoch: 11 [1203200/1281167 (94%)]\tLoss: 3.497089\n",
            "Wed Oct 31 18:25:27 2018 Train Epoch: 11 [1209600/1281167 (94%)]\tLoss: 4.469676\n",
            "Wed Oct 31 18:25:54 2018 Train Epoch: 11 [1216000/1281167 (95%)]\tLoss: 3.588595\n",
            "Wed Oct 31 18:26:21 2018 Train Epoch: 11 [1222400/1281167 (95%)]\tLoss: 3.674221\n",
            "Wed Oct 31 18:26:48 2018 Train Epoch: 11 [1228800/1281167 (96%)]\tLoss: 3.560031\n",
            "Wed Oct 31 18:27:15 2018 Train Epoch: 11 [1235200/1281167 (96%)]\tLoss: 3.541570\n",
            "Wed Oct 31 18:27:42 2018 Train Epoch: 11 [1241600/1281167 (97%)]\tLoss: 3.792086\n",
            "Wed Oct 31 18:28:09 2018 Train Epoch: 11 [1248000/1281167 (97%)]\tLoss: 4.245797\n",
            "Wed Oct 31 18:28:37 2018 Train Epoch: 11 [1254400/1281167 (98%)]\tLoss: 3.522996\n",
            "Wed Oct 31 18:29:04 2018 Train Epoch: 11 [1260800/1281167 (98%)]\tLoss: 3.102624\n",
            "Wed Oct 31 18:29:31 2018 Train Epoch: 11 [1267200/1281167 (99%)]\tLoss: 3.292840\n",
            "Wed Oct 31 18:29:58 2018 Train Epoch: 11 [1273600/1281167 (99%)]\tLoss: 3.126753\n",
            "Wed Oct 31 18:30:26 2018 Train Epoch: 11 [1280000/1281167 (100%)]\tLoss: 3.307224\n",
            "\n",
            "Test set: Average loss: 3.2387, Accuracy: 15494/50000 (31%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints1/011.pt\n",
            "\n",
            "Wed Oct 31 18:34:11 2018 Train Epoch: 12 [0/1281167 (0%)]\tLoss: 3.462920\n",
            "Wed Oct 31 18:34:38 2018 Train Epoch: 12 [6400/1281167 (0%)]\tLoss: 3.813189\n",
            "Wed Oct 31 18:35:05 2018 Train Epoch: 12 [12800/1281167 (1%)]\tLoss: 3.902233\n",
            "Wed Oct 31 18:35:32 2018 Train Epoch: 12 [19200/1281167 (1%)]\tLoss: 3.315149\n",
            "Wed Oct 31 18:36:00 2018 Train Epoch: 12 [25600/1281167 (2%)]\tLoss: 3.673629\n",
            "Wed Oct 31 18:36:27 2018 Train Epoch: 12 [32000/1281167 (2%)]\tLoss: 3.210263\n",
            "Wed Oct 31 18:36:54 2018 Train Epoch: 12 [38400/1281167 (3%)]\tLoss: 3.612678\n",
            "Wed Oct 31 18:37:21 2018 Train Epoch: 12 [44800/1281167 (3%)]\tLoss: 3.644962\n",
            "Wed Oct 31 18:37:48 2018 Train Epoch: 12 [51200/1281167 (4%)]\tLoss: 3.177135\n",
            "Wed Oct 31 18:38:15 2018 Train Epoch: 12 [57600/1281167 (4%)]\tLoss: 3.371304\n",
            "Wed Oct 31 18:38:43 2018 Train Epoch: 12 [64000/1281167 (5%)]\tLoss: 4.018703\n",
            "Wed Oct 31 18:39:10 2018 Train Epoch: 12 [70400/1281167 (5%)]\tLoss: 3.394868\n",
            "Wed Oct 31 18:39:37 2018 Train Epoch: 12 [76800/1281167 (6%)]\tLoss: 3.442087\n",
            "Wed Oct 31 18:40:04 2018 Train Epoch: 12 [83200/1281167 (6%)]\tLoss: 3.494659\n",
            "Wed Oct 31 18:40:31 2018 Train Epoch: 12 [89600/1281167 (7%)]\tLoss: 3.879074\n",
            "Wed Oct 31 18:40:59 2018 Train Epoch: 12 [96000/1281167 (7%)]\tLoss: 3.788872\n",
            "Wed Oct 31 18:41:26 2018 Train Epoch: 12 [102400/1281167 (8%)]\tLoss: 3.531469\n",
            "Wed Oct 31 18:41:53 2018 Train Epoch: 12 [108800/1281167 (8%)]\tLoss: 3.759906\n",
            "Wed Oct 31 18:42:20 2018 Train Epoch: 12 [115200/1281167 (9%)]\tLoss: 3.959998\n",
            "Wed Oct 31 18:42:47 2018 Train Epoch: 12 [121600/1281167 (9%)]\tLoss: 3.317002\n",
            "Wed Oct 31 18:43:14 2018 Train Epoch: 12 [128000/1281167 (10%)]\tLoss: 3.616863\n",
            "Wed Oct 31 18:43:41 2018 Train Epoch: 12 [134400/1281167 (10%)]\tLoss: 3.175446\n",
            "Wed Oct 31 18:44:09 2018 Train Epoch: 12 [140800/1281167 (11%)]\tLoss: 3.593682\n",
            "Wed Oct 31 18:44:36 2018 Train Epoch: 12 [147200/1281167 (11%)]\tLoss: 3.356567\n",
            "Wed Oct 31 18:45:03 2018 Train Epoch: 12 [153600/1281167 (12%)]\tLoss: 3.222744\n",
            "Wed Oct 31 18:45:30 2018 Train Epoch: 12 [160000/1281167 (12%)]\tLoss: 3.408358\n",
            "Wed Oct 31 18:45:57 2018 Train Epoch: 12 [166400/1281167 (13%)]\tLoss: 3.144187\n",
            "Wed Oct 31 18:46:25 2018 Train Epoch: 12 [172800/1281167 (13%)]\tLoss: 3.403739\n",
            "Wed Oct 31 18:46:52 2018 Train Epoch: 12 [179200/1281167 (14%)]\tLoss: 3.280898\n",
            "Wed Oct 31 18:47:19 2018 Train Epoch: 12 [185600/1281167 (14%)]\tLoss: 3.169771\n",
            "Wed Oct 31 18:47:46 2018 Train Epoch: 12 [192000/1281167 (15%)]\tLoss: 3.243799\n",
            "Wed Oct 31 18:48:14 2018 Train Epoch: 12 [198400/1281167 (15%)]\tLoss: 3.856277\n",
            "Wed Oct 31 18:48:41 2018 Train Epoch: 12 [204800/1281167 (16%)]\tLoss: 3.668669\n",
            "Wed Oct 31 18:49:08 2018 Train Epoch: 12 [211200/1281167 (16%)]\tLoss: 3.370154\n",
            "Wed Oct 31 18:49:35 2018 Train Epoch: 12 [217600/1281167 (17%)]\tLoss: 3.726921\n",
            "Wed Oct 31 18:50:02 2018 Train Epoch: 12 [224000/1281167 (17%)]\tLoss: 3.309532\n",
            "Wed Oct 31 18:50:29 2018 Train Epoch: 12 [230400/1281167 (18%)]\tLoss: 3.235455\n",
            "Wed Oct 31 18:50:57 2018 Train Epoch: 12 [236800/1281167 (18%)]\tLoss: 3.676523\n",
            "Wed Oct 31 18:51:24 2018 Train Epoch: 12 [243200/1281167 (19%)]\tLoss: 2.966176\n",
            "Wed Oct 31 18:51:51 2018 Train Epoch: 12 [249600/1281167 (19%)]\tLoss: 3.447644\n",
            "Wed Oct 31 18:52:18 2018 Train Epoch: 12 [256000/1281167 (20%)]\tLoss: 3.841096\n",
            "Wed Oct 31 18:52:45 2018 Train Epoch: 12 [262400/1281167 (20%)]\tLoss: 3.159891\n",
            "Wed Oct 31 18:53:13 2018 Train Epoch: 12 [268800/1281167 (21%)]\tLoss: 3.645668\n",
            "Wed Oct 31 18:53:40 2018 Train Epoch: 12 [275200/1281167 (21%)]\tLoss: 3.990088\n",
            "Wed Oct 31 18:54:07 2018 Train Epoch: 12 [281600/1281167 (22%)]\tLoss: 3.453681\n",
            "Wed Oct 31 18:54:34 2018 Train Epoch: 12 [288000/1281167 (22%)]\tLoss: 3.663179\n",
            "Wed Oct 31 18:55:01 2018 Train Epoch: 12 [294400/1281167 (23%)]\tLoss: 3.396187\n",
            "Wed Oct 31 18:55:29 2018 Train Epoch: 12 [300800/1281167 (23%)]\tLoss: 3.558122\n",
            "Wed Oct 31 18:55:56 2018 Train Epoch: 12 [307200/1281167 (24%)]\tLoss: 3.223089\n",
            "Wed Oct 31 18:56:23 2018 Train Epoch: 12 [313600/1281167 (24%)]\tLoss: 3.340927\n",
            "Wed Oct 31 18:56:50 2018 Train Epoch: 12 [320000/1281167 (25%)]\tLoss: 3.268535\n",
            "Wed Oct 31 18:57:17 2018 Train Epoch: 12 [326400/1281167 (25%)]\tLoss: 3.380296\n",
            "Wed Oct 31 18:57:45 2018 Train Epoch: 12 [332800/1281167 (26%)]\tLoss: 3.297818\n",
            "Wed Oct 31 18:58:12 2018 Train Epoch: 12 [339200/1281167 (26%)]\tLoss: 3.999087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tL0yjhP_Gnd0",
        "colab_type": "code",
        "outputId": "8d4634ed-fecb-4350-87b3-3d39949ea9b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15147
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "best_accuracy = torch.FloatTensor([0])\n",
        "TEST_BATCH_SIZE = 50\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0.0005\n",
        "CHECKPOINT_PATH = BASE_PATH + 'imagenet_full/checkpoints'\n",
        "LOG_PATH = BASE_PATH + 'imagenet_full/' + 'log.pkl'\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "\n",
        "class_names = sorted([(int(key), val['label'].split(',')[0]) for key, val in imagenet_synset_data.items()])\n",
        "name_to_class = {line[1]: line[0] for line in class_names}\n",
        "class_names = [line[1] for line in class_names]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "# model = resnet18().to(device)\n",
        "model = FullImagenetNet().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(CHECKPOINT_PATH)\n",
        "\n",
        "log_data = pt_util.read_log(LOG_PATH, [])\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\n",
        "        _,_,_,_,_,test_loss, acc = test(model, device, test_loader, True)\n",
        "        acc = torch.FloatTensor([acc])\n",
        "        is_best = bool(acc.numpy() > best_accuracy.numpy())\n",
        "        best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n",
        "        model.save_best_model(acc, CHECKPOINT_PATH + '/%03d.pt' % epoch, is_best=is_best)\n",
        "        data2dump = [epoch, loss.item(), test_loss, acc.item()]\n",
        "        print (data2dump)\n",
        "        log_data.append(data2dump)\n",
        "        \n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model(CHECKPOINT_PATH + '/%03d.pt' % epoch, 0)\n",
        "    pt_util.write_log(LOG_PATH, log_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n",
            "Restoring:\n",
            "conv_1.weight -> \ttorch.Size([128, 3, 5, 5]) = 0MB\n",
            "conv_1.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_1.weight -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_1.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_1.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_1.running_var -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_1.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "conv_2.weight -> \ttorch.Size([128, 128, 5, 5]) = 1MB\n",
            "conv_2.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_2.weight -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_2.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_2.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_2.running_var -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_2.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "conv_3.weight -> \ttorch.Size([128, 128, 5, 5]) = 1MB\n",
            "conv_3.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_3.weight -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_3.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_3.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_3.running_var -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_3.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "conv_4.weight -> \ttorch.Size([128, 128, 5, 5]) = 1MB\n",
            "conv_4.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_4.weight -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_4.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_4.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_4.running_var -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_4.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "conv_5.weight -> \ttorch.Size([128, 128, 5, 5]) = 1MB\n",
            "conv_5.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_5.weight -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_5.bias -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_5.running_mean -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_5.running_var -> \ttorch.Size([128]) = 0MB\n",
            "batch_norm_5.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "fc_1.weight -> \ttorch.Size([2048, 8192]) = 67MB\n",
            "fc_1.bias -> \ttorch.Size([2048]) = 0MB\n",
            "batch_norm_6.weight -> \ttorch.Size([2048]) = 0MB\n",
            "batch_norm_6.bias -> \ttorch.Size([2048]) = 0MB\n",
            "batch_norm_6.running_mean -> \ttorch.Size([2048]) = 0MB\n",
            "batch_norm_6.running_var -> \ttorch.Size([2048]) = 0MB\n",
            "batch_norm_6.num_batches_tracked -> \ttorch.Size([]) = 0MB\n",
            "fc_2.weight -> \ttorch.Size([1000, 2048]) = 8MB\n",
            "fc_2.bias -> \ttorch.Size([1000]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints/003.pt\n",
            "Tue Oct 30 08:56:44 2018 Train Epoch: 3 [0/1281167 (0%)]\tLoss: 4.791663\n",
            "Tue Oct 30 08:57:35 2018 Train Epoch: 3 [6400/1281167 (0%)]\tLoss: 4.874481\n",
            "Tue Oct 30 08:58:25 2018 Train Epoch: 3 [12800/1281167 (1%)]\tLoss: 4.498984\n",
            "Tue Oct 30 08:59:16 2018 Train Epoch: 3 [19200/1281167 (1%)]\tLoss: 4.457158\n",
            "Tue Oct 30 09:00:07 2018 Train Epoch: 3 [25600/1281167 (2%)]\tLoss: 4.413355\n",
            "Tue Oct 30 09:00:58 2018 Train Epoch: 3 [32000/1281167 (2%)]\tLoss: 4.623548\n",
            "Tue Oct 30 09:01:48 2018 Train Epoch: 3 [38400/1281167 (3%)]\tLoss: 4.239979\n",
            "Tue Oct 30 09:02:39 2018 Train Epoch: 3 [44800/1281167 (3%)]\tLoss: 4.551451\n",
            "Tue Oct 30 09:03:30 2018 Train Epoch: 3 [51200/1281167 (4%)]\tLoss: 4.324369\n",
            "Tue Oct 30 09:04:21 2018 Train Epoch: 3 [57600/1281167 (4%)]\tLoss: 4.006964\n",
            "Tue Oct 30 09:05:11 2018 Train Epoch: 3 [64000/1281167 (5%)]\tLoss: 4.678429\n",
            "Tue Oct 30 09:06:02 2018 Train Epoch: 3 [70400/1281167 (5%)]\tLoss: 4.495939\n",
            "Tue Oct 30 09:06:53 2018 Train Epoch: 3 [76800/1281167 (6%)]\tLoss: 4.505141\n",
            "Tue Oct 30 09:07:44 2018 Train Epoch: 3 [83200/1281167 (6%)]\tLoss: 4.137762\n",
            "Tue Oct 30 09:08:35 2018 Train Epoch: 3 [89600/1281167 (7%)]\tLoss: 4.750127\n",
            "Tue Oct 30 09:09:25 2018 Train Epoch: 3 [96000/1281167 (7%)]\tLoss: 4.616655\n",
            "Tue Oct 30 09:10:16 2018 Train Epoch: 3 [102400/1281167 (8%)]\tLoss: 4.637879\n",
            "Tue Oct 30 09:11:07 2018 Train Epoch: 3 [108800/1281167 (8%)]\tLoss: 4.434384\n",
            "Tue Oct 30 09:11:58 2018 Train Epoch: 3 [115200/1281167 (9%)]\tLoss: 4.539649\n",
            "Tue Oct 30 09:12:48 2018 Train Epoch: 3 [121600/1281167 (9%)]\tLoss: 4.157934\n",
            "Tue Oct 30 09:13:39 2018 Train Epoch: 3 [128000/1281167 (10%)]\tLoss: 4.195934\n",
            "Tue Oct 30 09:14:30 2018 Train Epoch: 3 [134400/1281167 (10%)]\tLoss: 4.452714\n",
            "Tue Oct 30 09:15:21 2018 Train Epoch: 3 [140800/1281167 (11%)]\tLoss: 4.835939\n",
            "Tue Oct 30 09:16:11 2018 Train Epoch: 3 [147200/1281167 (11%)]\tLoss: 4.077430\n",
            "Tue Oct 30 09:17:02 2018 Train Epoch: 3 [153600/1281167 (12%)]\tLoss: 4.516977\n",
            "Tue Oct 30 09:17:53 2018 Train Epoch: 3 [160000/1281167 (12%)]\tLoss: 4.209569\n",
            "Tue Oct 30 09:18:44 2018 Train Epoch: 3 [166400/1281167 (13%)]\tLoss: 4.269244\n",
            "Tue Oct 30 09:19:34 2018 Train Epoch: 3 [172800/1281167 (13%)]\tLoss: 4.785536\n",
            "Tue Oct 30 09:20:25 2018 Train Epoch: 3 [179200/1281167 (14%)]\tLoss: 4.311306\n",
            "Tue Oct 30 09:21:16 2018 Train Epoch: 3 [185600/1281167 (14%)]\tLoss: 4.669415\n",
            "Tue Oct 30 09:22:07 2018 Train Epoch: 3 [192000/1281167 (15%)]\tLoss: 4.574512\n",
            "Tue Oct 30 09:22:57 2018 Train Epoch: 3 [198400/1281167 (15%)]\tLoss: 4.634690\n",
            "Tue Oct 30 09:23:48 2018 Train Epoch: 3 [204800/1281167 (16%)]\tLoss: 4.479391\n",
            "Tue Oct 30 09:24:39 2018 Train Epoch: 3 [211200/1281167 (16%)]\tLoss: 4.770629\n",
            "Tue Oct 30 09:25:30 2018 Train Epoch: 3 [217600/1281167 (17%)]\tLoss: 4.720484\n",
            "Tue Oct 30 09:26:20 2018 Train Epoch: 3 [224000/1281167 (17%)]\tLoss: 4.110404\n",
            "Tue Oct 30 09:27:11 2018 Train Epoch: 3 [230400/1281167 (18%)]\tLoss: 4.180837\n",
            "Tue Oct 30 09:28:02 2018 Train Epoch: 3 [236800/1281167 (18%)]\tLoss: 4.209981\n",
            "Tue Oct 30 09:28:53 2018 Train Epoch: 3 [243200/1281167 (19%)]\tLoss: 4.756128\n",
            "Tue Oct 30 09:29:43 2018 Train Epoch: 3 [249600/1281167 (19%)]\tLoss: 4.327131\n",
            "Tue Oct 30 09:30:34 2018 Train Epoch: 3 [256000/1281167 (20%)]\tLoss: 4.971623\n",
            "Tue Oct 30 09:31:25 2018 Train Epoch: 3 [262400/1281167 (20%)]\tLoss: 4.652493\n",
            "Tue Oct 30 09:32:16 2018 Train Epoch: 3 [268800/1281167 (21%)]\tLoss: 4.399108\n",
            "Tue Oct 30 09:33:06 2018 Train Epoch: 3 [275200/1281167 (21%)]\tLoss: 4.502572\n",
            "Tue Oct 30 09:33:57 2018 Train Epoch: 3 [281600/1281167 (22%)]\tLoss: 4.738145\n",
            "Tue Oct 30 09:34:48 2018 Train Epoch: 3 [288000/1281167 (22%)]\tLoss: 4.223463\n",
            "Tue Oct 30 09:35:39 2018 Train Epoch: 3 [294400/1281167 (23%)]\tLoss: 4.244356\n",
            "Tue Oct 30 09:36:29 2018 Train Epoch: 3 [300800/1281167 (23%)]\tLoss: 4.657895\n",
            "Tue Oct 30 09:37:20 2018 Train Epoch: 3 [307200/1281167 (24%)]\tLoss: 4.561508\n",
            "Tue Oct 30 09:38:11 2018 Train Epoch: 3 [313600/1281167 (24%)]\tLoss: 4.401908\n",
            "Tue Oct 30 09:39:02 2018 Train Epoch: 3 [320000/1281167 (25%)]\tLoss: 4.236837\n",
            "Tue Oct 30 09:39:52 2018 Train Epoch: 3 [326400/1281167 (25%)]\tLoss: 4.540340\n",
            "Tue Oct 30 09:40:43 2018 Train Epoch: 3 [332800/1281167 (26%)]\tLoss: 4.540094\n",
            "Tue Oct 30 09:41:34 2018 Train Epoch: 3 [339200/1281167 (26%)]\tLoss: 4.343924\n",
            "Tue Oct 30 09:42:25 2018 Train Epoch: 3 [345600/1281167 (27%)]\tLoss: 4.625747\n",
            "Tue Oct 30 09:43:15 2018 Train Epoch: 3 [352000/1281167 (27%)]\tLoss: 4.292982\n",
            "Tue Oct 30 09:44:06 2018 Train Epoch: 3 [358400/1281167 (28%)]\tLoss: 4.320127\n",
            "Tue Oct 30 09:44:57 2018 Train Epoch: 3 [364800/1281167 (28%)]\tLoss: 4.549765\n",
            "Tue Oct 30 09:45:48 2018 Train Epoch: 3 [371200/1281167 (29%)]\tLoss: 4.522766\n",
            "Tue Oct 30 09:46:38 2018 Train Epoch: 3 [377600/1281167 (29%)]\tLoss: 4.437763\n",
            "Tue Oct 30 09:47:29 2018 Train Epoch: 3 [384000/1281167 (30%)]\tLoss: 4.178205\n",
            "Tue Oct 30 09:48:20 2018 Train Epoch: 3 [390400/1281167 (30%)]\tLoss: 4.420371\n",
            "Tue Oct 30 09:49:11 2018 Train Epoch: 3 [396800/1281167 (31%)]\tLoss: 4.790552\n",
            "Tue Oct 30 09:50:01 2018 Train Epoch: 3 [403200/1281167 (31%)]\tLoss: 4.537591\n",
            "Tue Oct 30 09:50:52 2018 Train Epoch: 3 [409600/1281167 (32%)]\tLoss: 4.831908\n",
            "Tue Oct 30 09:51:43 2018 Train Epoch: 3 [416000/1281167 (32%)]\tLoss: 4.231879\n",
            "Tue Oct 30 09:52:34 2018 Train Epoch: 3 [422400/1281167 (33%)]\tLoss: 4.549398\n",
            "Tue Oct 30 09:53:24 2018 Train Epoch: 3 [428800/1281167 (33%)]\tLoss: 4.558711\n",
            "Tue Oct 30 09:54:15 2018 Train Epoch: 3 [435200/1281167 (34%)]\tLoss: 4.343285\n",
            "Tue Oct 30 09:55:06 2018 Train Epoch: 3 [441600/1281167 (34%)]\tLoss: 4.646134\n",
            "Tue Oct 30 09:55:57 2018 Train Epoch: 3 [448000/1281167 (35%)]\tLoss: 4.296608\n",
            "Tue Oct 30 09:56:48 2018 Train Epoch: 3 [454400/1281167 (35%)]\tLoss: 4.277489\n",
            "Tue Oct 30 09:57:38 2018 Train Epoch: 3 [460800/1281167 (36%)]\tLoss: 4.777721\n",
            "Tue Oct 30 09:58:29 2018 Train Epoch: 3 [467200/1281167 (36%)]\tLoss: 4.404830\n",
            "Tue Oct 30 09:59:20 2018 Train Epoch: 3 [473600/1281167 (37%)]\tLoss: 4.135993\n",
            "Tue Oct 30 10:00:11 2018 Train Epoch: 3 [480000/1281167 (37%)]\tLoss: 4.399512\n",
            "Tue Oct 30 10:01:01 2018 Train Epoch: 3 [486400/1281167 (38%)]\tLoss: 4.327628\n",
            "Tue Oct 30 10:01:52 2018 Train Epoch: 3 [492800/1281167 (38%)]\tLoss: 4.342002\n",
            "Tue Oct 30 10:02:43 2018 Train Epoch: 3 [499200/1281167 (39%)]\tLoss: 4.477585\n",
            "Tue Oct 30 10:03:34 2018 Train Epoch: 3 [505600/1281167 (39%)]\tLoss: 4.844293\n",
            "Tue Oct 30 10:04:24 2018 Train Epoch: 3 [512000/1281167 (40%)]\tLoss: 4.561127\n",
            "Tue Oct 30 10:05:15 2018 Train Epoch: 3 [518400/1281167 (40%)]\tLoss: 4.333297\n",
            "Tue Oct 30 10:06:06 2018 Train Epoch: 3 [524800/1281167 (41%)]\tLoss: 4.725461\n",
            "Tue Oct 30 10:06:57 2018 Train Epoch: 3 [531200/1281167 (41%)]\tLoss: 3.994256\n",
            "Tue Oct 30 10:07:48 2018 Train Epoch: 3 [537600/1281167 (42%)]\tLoss: 4.811141\n",
            "Tue Oct 30 10:08:38 2018 Train Epoch: 3 [544000/1281167 (42%)]\tLoss: 3.977852\n",
            "Tue Oct 30 10:09:29 2018 Train Epoch: 3 [550400/1281167 (43%)]\tLoss: 4.464774\n",
            "Tue Oct 30 10:10:20 2018 Train Epoch: 3 [556800/1281167 (43%)]\tLoss: 4.387481\n",
            "Tue Oct 30 10:11:11 2018 Train Epoch: 3 [563200/1281167 (44%)]\tLoss: 4.116551\n",
            "Tue Oct 30 10:12:01 2018 Train Epoch: 3 [569600/1281167 (44%)]\tLoss: 4.238830\n",
            "Tue Oct 30 10:12:52 2018 Train Epoch: 3 [576000/1281167 (45%)]\tLoss: 4.767317\n",
            "Tue Oct 30 10:13:43 2018 Train Epoch: 3 [582400/1281167 (45%)]\tLoss: 4.726641\n",
            "Tue Oct 30 10:14:34 2018 Train Epoch: 3 [588800/1281167 (46%)]\tLoss: 4.600311\n",
            "Tue Oct 30 10:15:24 2018 Train Epoch: 3 [595200/1281167 (46%)]\tLoss: 4.395458\n",
            "Tue Oct 30 10:16:15 2018 Train Epoch: 3 [601600/1281167 (47%)]\tLoss: 4.677580\n",
            "Tue Oct 30 10:17:06 2018 Train Epoch: 3 [608000/1281167 (47%)]\tLoss: 4.099112\n",
            "Tue Oct 30 10:17:57 2018 Train Epoch: 3 [614400/1281167 (48%)]\tLoss: 4.420856\n",
            "Tue Oct 30 10:18:48 2018 Train Epoch: 3 [620800/1281167 (48%)]\tLoss: 4.636132\n",
            "Tue Oct 30 10:19:38 2018 Train Epoch: 3 [627200/1281167 (49%)]\tLoss: 4.319814\n",
            "Tue Oct 30 10:20:29 2018 Train Epoch: 3 [633600/1281167 (49%)]\tLoss: 4.492191\n",
            "Tue Oct 30 10:21:20 2018 Train Epoch: 3 [640000/1281167 (50%)]\tLoss: 4.767551\n",
            "Tue Oct 30 10:22:11 2018 Train Epoch: 3 [646400/1281167 (50%)]\tLoss: 4.479576\n",
            "Tue Oct 30 10:23:01 2018 Train Epoch: 3 [652800/1281167 (51%)]\tLoss: 4.645159\n",
            "Tue Oct 30 10:23:52 2018 Train Epoch: 3 [659200/1281167 (51%)]\tLoss: 4.433668\n",
            "Tue Oct 30 10:24:43 2018 Train Epoch: 3 [665600/1281167 (52%)]\tLoss: 4.309827\n",
            "Tue Oct 30 10:25:34 2018 Train Epoch: 3 [672000/1281167 (52%)]\tLoss: 4.886159\n",
            "Tue Oct 30 10:26:24 2018 Train Epoch: 3 [678400/1281167 (53%)]\tLoss: 4.956456\n",
            "Tue Oct 30 10:27:15 2018 Train Epoch: 3 [684800/1281167 (53%)]\tLoss: 3.938976\n",
            "Tue Oct 30 10:28:06 2018 Train Epoch: 3 [691200/1281167 (54%)]\tLoss: 4.560974\n",
            "Tue Oct 30 10:28:57 2018 Train Epoch: 3 [697600/1281167 (54%)]\tLoss: 4.633450\n",
            "Tue Oct 30 10:29:47 2018 Train Epoch: 3 [704000/1281167 (55%)]\tLoss: 4.301568\n",
            "Tue Oct 30 10:30:38 2018 Train Epoch: 3 [710400/1281167 (55%)]\tLoss: 4.149811\n",
            "Tue Oct 30 10:31:29 2018 Train Epoch: 3 [716800/1281167 (56%)]\tLoss: 4.004864\n",
            "Tue Oct 30 10:32:20 2018 Train Epoch: 3 [723200/1281167 (56%)]\tLoss: 4.516452\n",
            "Tue Oct 30 10:33:10 2018 Train Epoch: 3 [729600/1281167 (57%)]\tLoss: 4.141863\n",
            "Tue Oct 30 10:34:01 2018 Train Epoch: 3 [736000/1281167 (57%)]\tLoss: 4.547230\n",
            "Tue Oct 30 10:34:52 2018 Train Epoch: 3 [742400/1281167 (58%)]\tLoss: 4.294990\n",
            "Tue Oct 30 10:35:43 2018 Train Epoch: 3 [748800/1281167 (58%)]\tLoss: 4.347606\n",
            "Tue Oct 30 10:36:33 2018 Train Epoch: 3 [755200/1281167 (59%)]\tLoss: 4.405739\n",
            "Tue Oct 30 10:37:24 2018 Train Epoch: 3 [761600/1281167 (59%)]\tLoss: 4.397342\n",
            "Tue Oct 30 10:38:15 2018 Train Epoch: 3 [768000/1281167 (60%)]\tLoss: 4.478834\n",
            "Tue Oct 30 10:39:06 2018 Train Epoch: 3 [774400/1281167 (60%)]\tLoss: 4.737330\n",
            "Tue Oct 30 10:39:56 2018 Train Epoch: 3 [780800/1281167 (61%)]\tLoss: 4.837797\n",
            "Tue Oct 30 10:40:47 2018 Train Epoch: 3 [787200/1281167 (61%)]\tLoss: 4.511124\n",
            "Tue Oct 30 10:41:38 2018 Train Epoch: 3 [793600/1281167 (62%)]\tLoss: 4.207623\n",
            "Tue Oct 30 10:42:29 2018 Train Epoch: 3 [800000/1281167 (62%)]\tLoss: 4.434748\n",
            "Tue Oct 30 10:43:19 2018 Train Epoch: 3 [806400/1281167 (63%)]\tLoss: 4.574771\n",
            "Tue Oct 30 10:44:10 2018 Train Epoch: 3 [812800/1281167 (63%)]\tLoss: 4.113205\n",
            "Tue Oct 30 10:45:01 2018 Train Epoch: 3 [819200/1281167 (64%)]\tLoss: 4.467360\n",
            "Tue Oct 30 10:45:52 2018 Train Epoch: 3 [825600/1281167 (64%)]\tLoss: 4.210201\n",
            "Tue Oct 30 10:46:42 2018 Train Epoch: 3 [832000/1281167 (65%)]\tLoss: 4.400614\n",
            "Tue Oct 30 10:47:33 2018 Train Epoch: 3 [838400/1281167 (65%)]\tLoss: 4.284188\n",
            "Tue Oct 30 10:48:24 2018 Train Epoch: 3 [844800/1281167 (66%)]\tLoss: 4.102483\n",
            "Tue Oct 30 10:49:15 2018 Train Epoch: 3 [851200/1281167 (66%)]\tLoss: 4.088034\n",
            "Tue Oct 30 10:50:06 2018 Train Epoch: 3 [857600/1281167 (67%)]\tLoss: 4.483883\n",
            "Tue Oct 30 10:50:56 2018 Train Epoch: 3 [864000/1281167 (67%)]\tLoss: 4.529027\n",
            "Tue Oct 30 10:51:47 2018 Train Epoch: 3 [870400/1281167 (68%)]\tLoss: 4.161256\n",
            "Tue Oct 30 10:52:38 2018 Train Epoch: 3 [876800/1281167 (68%)]\tLoss: 4.607841\n",
            "Tue Oct 30 10:53:29 2018 Train Epoch: 3 [883200/1281167 (69%)]\tLoss: 4.165840\n",
            "Tue Oct 30 10:54:20 2018 Train Epoch: 3 [889600/1281167 (69%)]\tLoss: 5.085025\n",
            "Tue Oct 30 10:55:10 2018 Train Epoch: 3 [896000/1281167 (70%)]\tLoss: 4.516360\n",
            "Tue Oct 30 10:56:01 2018 Train Epoch: 3 [902400/1281167 (70%)]\tLoss: 4.499909\n",
            "Tue Oct 30 10:56:52 2018 Train Epoch: 3 [908800/1281167 (71%)]\tLoss: 4.505845\n",
            "Tue Oct 30 10:57:43 2018 Train Epoch: 3 [915200/1281167 (71%)]\tLoss: 4.385690\n",
            "Tue Oct 30 10:58:34 2018 Train Epoch: 3 [921600/1281167 (72%)]\tLoss: 4.302953\n",
            "Tue Oct 30 10:59:25 2018 Train Epoch: 3 [928000/1281167 (72%)]\tLoss: 4.324466\n",
            "Tue Oct 30 11:00:15 2018 Train Epoch: 3 [934400/1281167 (73%)]\tLoss: 4.234963\n",
            "Tue Oct 30 11:01:06 2018 Train Epoch: 3 [940800/1281167 (73%)]\tLoss: 4.119831\n",
            "Tue Oct 30 11:01:57 2018 Train Epoch: 3 [947200/1281167 (74%)]\tLoss: 4.467309\n",
            "Tue Oct 30 11:02:48 2018 Train Epoch: 3 [953600/1281167 (74%)]\tLoss: 3.985875\n",
            "Tue Oct 30 11:03:39 2018 Train Epoch: 3 [960000/1281167 (75%)]\tLoss: 4.156108\n",
            "Tue Oct 30 11:04:29 2018 Train Epoch: 3 [966400/1281167 (75%)]\tLoss: 4.012202\n",
            "Tue Oct 30 11:05:20 2018 Train Epoch: 3 [972800/1281167 (76%)]\tLoss: 3.979895\n",
            "Tue Oct 30 11:06:11 2018 Train Epoch: 3 [979200/1281167 (76%)]\tLoss: 4.187482\n",
            "Tue Oct 30 11:07:02 2018 Train Epoch: 3 [985600/1281167 (77%)]\tLoss: 4.533925\n",
            "Tue Oct 30 11:07:53 2018 Train Epoch: 3 [992000/1281167 (77%)]\tLoss: 4.438002\n",
            "Tue Oct 30 11:08:43 2018 Train Epoch: 3 [998400/1281167 (78%)]\tLoss: 4.165618\n",
            "Tue Oct 30 11:09:34 2018 Train Epoch: 3 [1004800/1281167 (78%)]\tLoss: 4.095183\n",
            "Tue Oct 30 11:10:25 2018 Train Epoch: 3 [1011200/1281167 (79%)]\tLoss: 4.966158\n",
            "Tue Oct 30 11:11:16 2018 Train Epoch: 3 [1017600/1281167 (79%)]\tLoss: 4.387110\n",
            "Tue Oct 30 11:12:07 2018 Train Epoch: 3 [1024000/1281167 (80%)]\tLoss: 4.006280\n",
            "Tue Oct 30 11:12:57 2018 Train Epoch: 3 [1030400/1281167 (80%)]\tLoss: 4.207583\n",
            "Tue Oct 30 11:13:48 2018 Train Epoch: 3 [1036800/1281167 (81%)]\tLoss: 4.810829\n",
            "Tue Oct 30 11:14:39 2018 Train Epoch: 3 [1043200/1281167 (81%)]\tLoss: 4.706242\n",
            "Tue Oct 30 11:15:30 2018 Train Epoch: 3 [1049600/1281167 (82%)]\tLoss: 4.362979\n",
            "Tue Oct 30 11:16:21 2018 Train Epoch: 3 [1056000/1281167 (82%)]\tLoss: 4.173868\n",
            "Tue Oct 30 11:17:11 2018 Train Epoch: 3 [1062400/1281167 (83%)]\tLoss: 4.442876\n",
            "Tue Oct 30 11:18:02 2018 Train Epoch: 3 [1068800/1281167 (83%)]\tLoss: 4.680627\n",
            "Tue Oct 30 11:18:53 2018 Train Epoch: 3 [1075200/1281167 (84%)]\tLoss: 4.156992\n",
            "Tue Oct 30 11:19:44 2018 Train Epoch: 3 [1081600/1281167 (84%)]\tLoss: 4.492135\n",
            "Tue Oct 30 11:20:35 2018 Train Epoch: 3 [1088000/1281167 (85%)]\tLoss: 3.972952\n",
            "Tue Oct 30 11:21:26 2018 Train Epoch: 3 [1094400/1281167 (85%)]\tLoss: 4.658095\n",
            "Tue Oct 30 11:22:16 2018 Train Epoch: 3 [1100800/1281167 (86%)]\tLoss: 4.468998\n",
            "Tue Oct 30 11:23:07 2018 Train Epoch: 3 [1107200/1281167 (86%)]\tLoss: 4.039925\n",
            "Tue Oct 30 11:23:58 2018 Train Epoch: 3 [1113600/1281167 (87%)]\tLoss: 4.961937\n",
            "Tue Oct 30 11:24:49 2018 Train Epoch: 3 [1120000/1281167 (87%)]\tLoss: 5.040980\n",
            "Tue Oct 30 11:25:40 2018 Train Epoch: 3 [1126400/1281167 (88%)]\tLoss: 4.406839\n",
            "Tue Oct 30 11:26:30 2018 Train Epoch: 3 [1132800/1281167 (88%)]\tLoss: 4.474010\n",
            "Tue Oct 30 11:27:21 2018 Train Epoch: 3 [1139200/1281167 (89%)]\tLoss: 4.278490\n",
            "Tue Oct 30 11:28:12 2018 Train Epoch: 3 [1145600/1281167 (89%)]\tLoss: 4.720146\n",
            "Tue Oct 30 11:29:03 2018 Train Epoch: 3 [1152000/1281167 (90%)]\tLoss: 4.498650\n",
            "Tue Oct 30 11:29:54 2018 Train Epoch: 3 [1158400/1281167 (90%)]\tLoss: 4.324547\n",
            "Tue Oct 30 11:30:44 2018 Train Epoch: 3 [1164800/1281167 (91%)]\tLoss: 4.389920\n",
            "Tue Oct 30 11:31:35 2018 Train Epoch: 3 [1171200/1281167 (91%)]\tLoss: 4.451218\n",
            "Tue Oct 30 11:32:26 2018 Train Epoch: 3 [1177600/1281167 (92%)]\tLoss: 4.555700\n",
            "Tue Oct 30 11:33:17 2018 Train Epoch: 3 [1184000/1281167 (92%)]\tLoss: 4.419662\n",
            "Tue Oct 30 11:34:08 2018 Train Epoch: 3 [1190400/1281167 (93%)]\tLoss: 4.190711\n",
            "Tue Oct 30 11:34:58 2018 Train Epoch: 3 [1196800/1281167 (93%)]\tLoss: 4.441488\n",
            "Tue Oct 30 11:35:49 2018 Train Epoch: 3 [1203200/1281167 (94%)]\tLoss: 4.202088\n",
            "Tue Oct 30 11:36:40 2018 Train Epoch: 3 [1209600/1281167 (94%)]\tLoss: 4.308753\n",
            "Tue Oct 30 11:37:31 2018 Train Epoch: 3 [1216000/1281167 (95%)]\tLoss: 4.469442\n",
            "Tue Oct 30 11:38:22 2018 Train Epoch: 3 [1222400/1281167 (95%)]\tLoss: 4.537624\n",
            "Tue Oct 30 11:39:13 2018 Train Epoch: 3 [1228800/1281167 (96%)]\tLoss: 4.561214\n",
            "Tue Oct 30 11:40:03 2018 Train Epoch: 3 [1235200/1281167 (96%)]\tLoss: 4.742081\n",
            "Tue Oct 30 11:40:54 2018 Train Epoch: 3 [1241600/1281167 (97%)]\tLoss: 4.257378\n",
            "Tue Oct 30 11:41:45 2018 Train Epoch: 3 [1248000/1281167 (97%)]\tLoss: 5.023127\n",
            "Tue Oct 30 11:42:36 2018 Train Epoch: 3 [1254400/1281167 (98%)]\tLoss: 4.544676\n",
            "Tue Oct 30 11:43:27 2018 Train Epoch: 3 [1260800/1281167 (98%)]\tLoss: 4.571074\n",
            "Tue Oct 30 11:44:17 2018 Train Epoch: 3 [1267200/1281167 (99%)]\tLoss: 4.677085\n",
            "Tue Oct 30 11:45:08 2018 Train Epoch: 3 [1273600/1281167 (99%)]\tLoss: 4.387393\n",
            "Tue Oct 30 11:45:59 2018 Train Epoch: 3 [1280000/1281167 (100%)]\tLoss: 4.252637\n",
            "\n",
            "Test set: Average loss: 3.9568, Accuracy: 10306/50000 (21%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints/003.pt\n",
            "\n",
            "Tue Oct 30 11:50:16 2018 Train Epoch: 4 [0/1281167 (0%)]\tLoss: 4.484968\n",
            "Tue Oct 30 11:51:07 2018 Train Epoch: 4 [6400/1281167 (0%)]\tLoss: 4.611473\n",
            "Tue Oct 30 11:51:57 2018 Train Epoch: 4 [12800/1281167 (1%)]\tLoss: 4.740357\n",
            "Tue Oct 30 11:52:48 2018 Train Epoch: 4 [19200/1281167 (1%)]\tLoss: 4.361804\n",
            "Tue Oct 30 11:53:39 2018 Train Epoch: 4 [25600/1281167 (2%)]\tLoss: 4.388730\n",
            "Tue Oct 30 11:54:30 2018 Train Epoch: 4 [32000/1281167 (2%)]\tLoss: 3.953430\n",
            "Tue Oct 30 11:55:20 2018 Train Epoch: 4 [38400/1281167 (3%)]\tLoss: 4.477792\n",
            "Tue Oct 30 11:56:11 2018 Train Epoch: 4 [44800/1281167 (3%)]\tLoss: 4.266710\n",
            "Tue Oct 30 11:57:02 2018 Train Epoch: 4 [51200/1281167 (4%)]\tLoss: 4.332563\n",
            "Tue Oct 30 11:57:53 2018 Train Epoch: 4 [57600/1281167 (4%)]\tLoss: 4.826693\n",
            "Tue Oct 30 11:58:44 2018 Train Epoch: 4 [64000/1281167 (5%)]\tLoss: 4.157123\n",
            "Tue Oct 30 11:59:34 2018 Train Epoch: 4 [70400/1281167 (5%)]\tLoss: 4.254300\n",
            "Tue Oct 30 12:00:25 2018 Train Epoch: 4 [76800/1281167 (6%)]\tLoss: 4.234657\n",
            "Tue Oct 30 12:01:16 2018 Train Epoch: 4 [83200/1281167 (6%)]\tLoss: 4.575933\n",
            "Tue Oct 30 12:02:07 2018 Train Epoch: 4 [89600/1281167 (7%)]\tLoss: 4.122499\n",
            "Tue Oct 30 12:02:58 2018 Train Epoch: 4 [96000/1281167 (7%)]\tLoss: 4.493247\n",
            "Tue Oct 30 12:03:48 2018 Train Epoch: 4 [102400/1281167 (8%)]\tLoss: 4.710827\n",
            "Tue Oct 30 12:04:39 2018 Train Epoch: 4 [108800/1281167 (8%)]\tLoss: 5.044518\n",
            "Tue Oct 30 12:05:30 2018 Train Epoch: 4 [115200/1281167 (9%)]\tLoss: 4.762078\n",
            "Tue Oct 30 12:06:21 2018 Train Epoch: 4 [121600/1281167 (9%)]\tLoss: 3.930094\n",
            "Tue Oct 30 12:07:11 2018 Train Epoch: 4 [128000/1281167 (10%)]\tLoss: 4.717323\n",
            "Tue Oct 30 12:08:02 2018 Train Epoch: 4 [134400/1281167 (10%)]\tLoss: 4.248150\n",
            "Tue Oct 30 12:08:53 2018 Train Epoch: 4 [140800/1281167 (11%)]\tLoss: 4.656613\n",
            "Tue Oct 30 12:09:44 2018 Train Epoch: 4 [147200/1281167 (11%)]\tLoss: 4.495981\n",
            "Tue Oct 30 12:10:34 2018 Train Epoch: 4 [153600/1281167 (12%)]\tLoss: 4.230221\n",
            "Tue Oct 30 12:11:25 2018 Train Epoch: 4 [160000/1281167 (12%)]\tLoss: 4.842377\n",
            "Tue Oct 30 12:12:16 2018 Train Epoch: 4 [166400/1281167 (13%)]\tLoss: 4.201478\n",
            "Tue Oct 30 12:13:06 2018 Train Epoch: 4 [172800/1281167 (13%)]\tLoss: 4.387622\n",
            "Tue Oct 30 12:13:57 2018 Train Epoch: 4 [179200/1281167 (14%)]\tLoss: 4.456609\n",
            "Tue Oct 30 12:14:48 2018 Train Epoch: 4 [185600/1281167 (14%)]\tLoss: 4.303946\n",
            "Tue Oct 30 12:15:39 2018 Train Epoch: 4 [192000/1281167 (15%)]\tLoss: 4.239178\n",
            "Tue Oct 30 12:16:30 2018 Train Epoch: 4 [198400/1281167 (15%)]\tLoss: 4.365232\n",
            "Tue Oct 30 12:17:20 2018 Train Epoch: 4 [204800/1281167 (16%)]\tLoss: 4.233488\n",
            "Tue Oct 30 12:18:11 2018 Train Epoch: 4 [211200/1281167 (16%)]\tLoss: 4.525743\n",
            "Tue Oct 30 12:19:02 2018 Train Epoch: 4 [217600/1281167 (17%)]\tLoss: 4.293754\n",
            "Tue Oct 30 12:19:52 2018 Train Epoch: 4 [224000/1281167 (17%)]\tLoss: 4.230021\n",
            "Tue Oct 30 12:20:43 2018 Train Epoch: 4 [230400/1281167 (18%)]\tLoss: 3.878428\n",
            "Tue Oct 30 12:21:34 2018 Train Epoch: 4 [236800/1281167 (18%)]\tLoss: 4.658979\n",
            "Tue Oct 30 12:22:25 2018 Train Epoch: 4 [243200/1281167 (19%)]\tLoss: 4.707808\n",
            "Tue Oct 30 12:23:15 2018 Train Epoch: 4 [249600/1281167 (19%)]\tLoss: 4.473241\n",
            "Tue Oct 30 12:24:06 2018 Train Epoch: 4 [256000/1281167 (20%)]\tLoss: 4.648575\n",
            "Tue Oct 30 12:24:57 2018 Train Epoch: 4 [262400/1281167 (20%)]\tLoss: 5.062503\n",
            "Tue Oct 30 12:25:47 2018 Train Epoch: 4 [268800/1281167 (21%)]\tLoss: 4.704962\n",
            "Tue Oct 30 12:26:38 2018 Train Epoch: 4 [275200/1281167 (21%)]\tLoss: 4.611044\n",
            "Tue Oct 30 12:27:29 2018 Train Epoch: 4 [281600/1281167 (22%)]\tLoss: 4.540557\n",
            "Tue Oct 30 12:28:20 2018 Train Epoch: 4 [288000/1281167 (22%)]\tLoss: 4.386198\n",
            "Tue Oct 30 12:29:10 2018 Train Epoch: 4 [294400/1281167 (23%)]\tLoss: 4.567625\n",
            "Tue Oct 30 12:30:01 2018 Train Epoch: 4 [300800/1281167 (23%)]\tLoss: 4.107860\n",
            "Tue Oct 30 12:30:52 2018 Train Epoch: 4 [307200/1281167 (24%)]\tLoss: 4.318325\n",
            "Tue Oct 30 12:31:43 2018 Train Epoch: 4 [313600/1281167 (24%)]\tLoss: 4.195356\n",
            "Tue Oct 30 12:32:33 2018 Train Epoch: 4 [320000/1281167 (25%)]\tLoss: 4.764721\n",
            "Tue Oct 30 12:33:24 2018 Train Epoch: 4 [326400/1281167 (25%)]\tLoss: 4.752174\n",
            "Tue Oct 30 12:34:15 2018 Train Epoch: 4 [332800/1281167 (26%)]\tLoss: 4.354541\n",
            "Tue Oct 30 12:35:05 2018 Train Epoch: 4 [339200/1281167 (26%)]\tLoss: 4.289201\n",
            "Tue Oct 30 12:35:56 2018 Train Epoch: 4 [345600/1281167 (27%)]\tLoss: 4.024268\n",
            "Tue Oct 30 12:36:47 2018 Train Epoch: 4 [352000/1281167 (27%)]\tLoss: 4.519376\n",
            "Tue Oct 30 12:37:38 2018 Train Epoch: 4 [358400/1281167 (28%)]\tLoss: 4.412104\n",
            "Tue Oct 30 12:38:28 2018 Train Epoch: 4 [364800/1281167 (28%)]\tLoss: 4.843306\n",
            "Tue Oct 30 12:39:19 2018 Train Epoch: 4 [371200/1281167 (29%)]\tLoss: 4.162871\n",
            "Tue Oct 30 12:40:10 2018 Train Epoch: 4 [377600/1281167 (29%)]\tLoss: 4.302839\n",
            "Tue Oct 30 12:41:01 2018 Train Epoch: 4 [384000/1281167 (30%)]\tLoss: 4.199947\n",
            "Tue Oct 30 12:41:51 2018 Train Epoch: 4 [390400/1281167 (30%)]\tLoss: 4.099625\n",
            "Tue Oct 30 12:42:42 2018 Train Epoch: 4 [396800/1281167 (31%)]\tLoss: 4.307702\n",
            "Tue Oct 30 12:43:33 2018 Train Epoch: 4 [403200/1281167 (31%)]\tLoss: 4.297246\n",
            "Tue Oct 30 12:44:23 2018 Train Epoch: 4 [409600/1281167 (32%)]\tLoss: 4.298001\n",
            "Tue Oct 30 12:45:14 2018 Train Epoch: 4 [416000/1281167 (32%)]\tLoss: 4.433777\n",
            "Tue Oct 30 12:46:05 2018 Train Epoch: 4 [422400/1281167 (33%)]\tLoss: 4.559068\n",
            "Tue Oct 30 12:46:56 2018 Train Epoch: 4 [428800/1281167 (33%)]\tLoss: 4.342007\n",
            "Tue Oct 30 12:47:46 2018 Train Epoch: 4 [435200/1281167 (34%)]\tLoss: 4.332063\n",
            "Tue Oct 30 12:48:37 2018 Train Epoch: 4 [441600/1281167 (34%)]\tLoss: 4.783902\n",
            "Tue Oct 30 12:49:28 2018 Train Epoch: 4 [448000/1281167 (35%)]\tLoss: 4.144944\n",
            "Tue Oct 30 12:50:19 2018 Train Epoch: 4 [454400/1281167 (35%)]\tLoss: 4.150016\n",
            "Tue Oct 30 12:51:09 2018 Train Epoch: 4 [460800/1281167 (36%)]\tLoss: 4.782455\n",
            "Tue Oct 30 12:52:00 2018 Train Epoch: 4 [467200/1281167 (36%)]\tLoss: 4.390683\n",
            "Tue Oct 30 12:52:51 2018 Train Epoch: 4 [473600/1281167 (37%)]\tLoss: 4.467516\n",
            "Tue Oct 30 12:53:42 2018 Train Epoch: 4 [480000/1281167 (37%)]\tLoss: 4.340535\n",
            "Tue Oct 30 12:54:32 2018 Train Epoch: 4 [486400/1281167 (38%)]\tLoss: 4.321676\n",
            "Tue Oct 30 12:55:23 2018 Train Epoch: 4 [492800/1281167 (38%)]\tLoss: 4.790138\n",
            "Tue Oct 30 12:56:14 2018 Train Epoch: 4 [499200/1281167 (39%)]\tLoss: 4.681558\n",
            "Tue Oct 30 12:57:04 2018 Train Epoch: 4 [505600/1281167 (39%)]\tLoss: 4.039748\n",
            "Tue Oct 30 12:57:55 2018 Train Epoch: 4 [512000/1281167 (40%)]\tLoss: 4.797383\n",
            "Tue Oct 30 12:58:46 2018 Train Epoch: 4 [518400/1281167 (40%)]\tLoss: 4.387637\n",
            "Tue Oct 30 12:59:37 2018 Train Epoch: 4 [524800/1281167 (41%)]\tLoss: 4.556544\n",
            "Tue Oct 30 13:00:27 2018 Train Epoch: 4 [531200/1281167 (41%)]\tLoss: 4.428275\n",
            "Tue Oct 30 13:01:18 2018 Train Epoch: 4 [537600/1281167 (42%)]\tLoss: 4.759162\n",
            "Tue Oct 30 13:02:09 2018 Train Epoch: 4 [544000/1281167 (42%)]\tLoss: 4.562788\n",
            "Tue Oct 30 13:03:00 2018 Train Epoch: 4 [550400/1281167 (43%)]\tLoss: 4.280142\n",
            "Tue Oct 30 13:03:50 2018 Train Epoch: 4 [556800/1281167 (43%)]\tLoss: 4.443187\n",
            "Tue Oct 30 13:04:41 2018 Train Epoch: 4 [563200/1281167 (44%)]\tLoss: 4.628863\n",
            "Tue Oct 30 13:05:32 2018 Train Epoch: 4 [569600/1281167 (44%)]\tLoss: 4.534281\n",
            "Tue Oct 30 13:06:23 2018 Train Epoch: 4 [576000/1281167 (45%)]\tLoss: 4.373561\n",
            "Tue Oct 30 13:07:13 2018 Train Epoch: 4 [582400/1281167 (45%)]\tLoss: 4.274845\n",
            "Tue Oct 30 13:08:04 2018 Train Epoch: 4 [588800/1281167 (46%)]\tLoss: 4.815477\n",
            "Tue Oct 30 13:08:55 2018 Train Epoch: 4 [595200/1281167 (46%)]\tLoss: 4.154080\n",
            "Tue Oct 30 13:09:45 2018 Train Epoch: 4 [601600/1281167 (47%)]\tLoss: 4.201673\n",
            "Tue Oct 30 13:10:36 2018 Train Epoch: 4 [608000/1281167 (47%)]\tLoss: 4.929697\n",
            "Tue Oct 30 13:11:27 2018 Train Epoch: 4 [614400/1281167 (48%)]\tLoss: 4.633496\n",
            "Tue Oct 30 13:12:17 2018 Train Epoch: 4 [620800/1281167 (48%)]\tLoss: 4.430157\n",
            "Tue Oct 30 13:13:08 2018 Train Epoch: 4 [627200/1281167 (49%)]\tLoss: 4.415212\n",
            "Tue Oct 30 13:13:59 2018 Train Epoch: 4 [633600/1281167 (49%)]\tLoss: 4.472823\n",
            "Tue Oct 30 13:14:50 2018 Train Epoch: 4 [640000/1281167 (50%)]\tLoss: 4.594940\n",
            "Tue Oct 30 13:15:41 2018 Train Epoch: 4 [646400/1281167 (50%)]\tLoss: 4.651232\n",
            "Tue Oct 30 13:16:31 2018 Train Epoch: 4 [652800/1281167 (51%)]\tLoss: 4.508890\n",
            "Tue Oct 30 13:17:22 2018 Train Epoch: 4 [659200/1281167 (51%)]\tLoss: 4.102576\n",
            "Tue Oct 30 13:18:13 2018 Train Epoch: 4 [665600/1281167 (52%)]\tLoss: 4.432431\n",
            "Tue Oct 30 13:19:04 2018 Train Epoch: 4 [672000/1281167 (52%)]\tLoss: 4.112559\n",
            "Tue Oct 30 13:19:54 2018 Train Epoch: 4 [678400/1281167 (53%)]\tLoss: 4.150522\n",
            "Tue Oct 30 13:20:45 2018 Train Epoch: 4 [684800/1281167 (53%)]\tLoss: 4.460538\n",
            "Tue Oct 30 13:21:36 2018 Train Epoch: 4 [691200/1281167 (54%)]\tLoss: 4.241921\n",
            "Tue Oct 30 13:22:27 2018 Train Epoch: 4 [697600/1281167 (54%)]\tLoss: 4.837453\n",
            "Tue Oct 30 13:23:17 2018 Train Epoch: 4 [704000/1281167 (55%)]\tLoss: 4.542426\n",
            "Tue Oct 30 13:24:08 2018 Train Epoch: 4 [710400/1281167 (55%)]\tLoss: 4.643503\n",
            "Tue Oct 30 13:24:59 2018 Train Epoch: 4 [716800/1281167 (56%)]\tLoss: 4.601066\n",
            "Tue Oct 30 13:25:49 2018 Train Epoch: 4 [723200/1281167 (56%)]\tLoss: 4.199448\n",
            "Tue Oct 30 13:26:40 2018 Train Epoch: 4 [729600/1281167 (57%)]\tLoss: 4.310502\n",
            "Tue Oct 30 13:27:31 2018 Train Epoch: 4 [736000/1281167 (57%)]\tLoss: 4.972345\n",
            "Tue Oct 30 13:28:22 2018 Train Epoch: 4 [742400/1281167 (58%)]\tLoss: 4.628204\n",
            "Tue Oct 30 13:29:13 2018 Train Epoch: 4 [748800/1281167 (58%)]\tLoss: 4.507900\n",
            "Tue Oct 30 13:30:03 2018 Train Epoch: 4 [755200/1281167 (59%)]\tLoss: 4.410424\n",
            "Tue Oct 30 13:30:54 2018 Train Epoch: 4 [761600/1281167 (59%)]\tLoss: 4.210584\n",
            "Tue Oct 30 13:31:45 2018 Train Epoch: 4 [768000/1281167 (60%)]\tLoss: 4.645737\n",
            "Tue Oct 30 13:32:36 2018 Train Epoch: 4 [774400/1281167 (60%)]\tLoss: 4.677285\n",
            "Tue Oct 30 13:33:26 2018 Train Epoch: 4 [780800/1281167 (61%)]\tLoss: 4.806065\n",
            "Tue Oct 30 13:34:17 2018 Train Epoch: 4 [787200/1281167 (61%)]\tLoss: 4.543155\n",
            "Tue Oct 30 13:35:08 2018 Train Epoch: 4 [793600/1281167 (62%)]\tLoss: 4.721940\n",
            "Tue Oct 30 13:35:58 2018 Train Epoch: 4 [800000/1281167 (62%)]\tLoss: 4.660515\n",
            "Tue Oct 30 13:36:49 2018 Train Epoch: 4 [806400/1281167 (63%)]\tLoss: 4.202046\n",
            "Tue Oct 30 13:37:40 2018 Train Epoch: 4 [812800/1281167 (63%)]\tLoss: 4.498447\n",
            "Tue Oct 30 13:38:31 2018 Train Epoch: 4 [819200/1281167 (64%)]\tLoss: 4.550214\n",
            "Tue Oct 30 13:39:21 2018 Train Epoch: 4 [825600/1281167 (64%)]\tLoss: 4.559912\n",
            "Tue Oct 30 13:40:12 2018 Train Epoch: 4 [832000/1281167 (65%)]\tLoss: 4.529997\n",
            "Tue Oct 30 13:41:03 2018 Train Epoch: 4 [838400/1281167 (65%)]\tLoss: 4.625236\n",
            "Tue Oct 30 13:41:54 2018 Train Epoch: 4 [844800/1281167 (66%)]\tLoss: 4.545787\n",
            "Tue Oct 30 13:42:44 2018 Train Epoch: 4 [851200/1281167 (66%)]\tLoss: 4.800355\n",
            "Tue Oct 30 13:43:35 2018 Train Epoch: 4 [857600/1281167 (67%)]\tLoss: 4.266092\n",
            "Tue Oct 30 13:44:26 2018 Train Epoch: 4 [864000/1281167 (67%)]\tLoss: 4.297553\n",
            "Tue Oct 30 13:45:16 2018 Train Epoch: 4 [870400/1281167 (68%)]\tLoss: 4.421417\n",
            "Tue Oct 30 13:46:07 2018 Train Epoch: 4 [876800/1281167 (68%)]\tLoss: 4.761045\n",
            "Tue Oct 30 13:46:58 2018 Train Epoch: 4 [883200/1281167 (69%)]\tLoss: 4.208665\n",
            "Tue Oct 30 13:47:49 2018 Train Epoch: 4 [889600/1281167 (69%)]\tLoss: 4.522786\n",
            "Tue Oct 30 13:48:40 2018 Train Epoch: 4 [896000/1281167 (70%)]\tLoss: 4.629989\n",
            "Tue Oct 30 13:49:30 2018 Train Epoch: 4 [902400/1281167 (70%)]\tLoss: 4.789456\n",
            "Tue Oct 30 13:50:21 2018 Train Epoch: 4 [908800/1281167 (71%)]\tLoss: 4.030528\n",
            "Tue Oct 30 13:51:12 2018 Train Epoch: 4 [915200/1281167 (71%)]\tLoss: 4.620831\n",
            "Tue Oct 30 13:52:03 2018 Train Epoch: 4 [921600/1281167 (72%)]\tLoss: 4.154321\n",
            "Tue Oct 30 13:52:54 2018 Train Epoch: 4 [928000/1281167 (72%)]\tLoss: 4.299662\n",
            "Tue Oct 30 13:53:44 2018 Train Epoch: 4 [934400/1281167 (73%)]\tLoss: 4.647158\n",
            "Tue Oct 30 13:54:35 2018 Train Epoch: 4 [940800/1281167 (73%)]\tLoss: 4.281418\n",
            "Tue Oct 30 13:55:26 2018 Train Epoch: 4 [947200/1281167 (74%)]\tLoss: 4.070550\n",
            "Tue Oct 30 13:56:16 2018 Train Epoch: 4 [953600/1281167 (74%)]\tLoss: 4.263114\n",
            "Tue Oct 30 13:57:07 2018 Train Epoch: 4 [960000/1281167 (75%)]\tLoss: 4.092345\n",
            "Tue Oct 30 13:57:58 2018 Train Epoch: 4 [966400/1281167 (75%)]\tLoss: 4.357211\n",
            "Tue Oct 30 13:58:49 2018 Train Epoch: 4 [972800/1281167 (76%)]\tLoss: 4.270913\n",
            "Tue Oct 30 13:59:40 2018 Train Epoch: 4 [979200/1281167 (76%)]\tLoss: 4.354040\n",
            "Tue Oct 30 14:00:30 2018 Train Epoch: 4 [985600/1281167 (77%)]\tLoss: 4.706467\n",
            "Tue Oct 30 14:01:21 2018 Train Epoch: 4 [992000/1281167 (77%)]\tLoss: 4.293530\n",
            "Tue Oct 30 14:02:12 2018 Train Epoch: 4 [998400/1281167 (78%)]\tLoss: 4.598738\n",
            "Tue Oct 30 14:03:02 2018 Train Epoch: 4 [1004800/1281167 (78%)]\tLoss: 4.425619\n",
            "Tue Oct 30 14:03:53 2018 Train Epoch: 4 [1011200/1281167 (79%)]\tLoss: 4.851290\n",
            "Tue Oct 30 14:04:44 2018 Train Epoch: 4 [1017600/1281167 (79%)]\tLoss: 4.962358\n",
            "Tue Oct 30 14:05:35 2018 Train Epoch: 4 [1024000/1281167 (80%)]\tLoss: 4.872799\n",
            "Tue Oct 30 14:06:25 2018 Train Epoch: 4 [1030400/1281167 (80%)]\tLoss: 4.549200\n",
            "Tue Oct 30 14:07:16 2018 Train Epoch: 4 [1036800/1281167 (81%)]\tLoss: 4.277658\n",
            "Tue Oct 30 14:08:07 2018 Train Epoch: 4 [1043200/1281167 (81%)]\tLoss: 3.995620\n",
            "Tue Oct 30 14:08:58 2018 Train Epoch: 4 [1049600/1281167 (82%)]\tLoss: 4.376963\n",
            "Tue Oct 30 14:09:48 2018 Train Epoch: 4 [1056000/1281167 (82%)]\tLoss: 4.088165\n",
            "Tue Oct 30 14:10:39 2018 Train Epoch: 4 [1062400/1281167 (83%)]\tLoss: 4.764798\n",
            "Tue Oct 30 14:11:30 2018 Train Epoch: 4 [1068800/1281167 (83%)]\tLoss: 4.478088\n",
            "Tue Oct 30 14:12:21 2018 Train Epoch: 4 [1075200/1281167 (84%)]\tLoss: 4.568624\n",
            "Tue Oct 30 14:13:12 2018 Train Epoch: 4 [1081600/1281167 (84%)]\tLoss: 3.855352\n",
            "Tue Oct 30 14:14:02 2018 Train Epoch: 4 [1088000/1281167 (85%)]\tLoss: 4.531487\n",
            "Tue Oct 30 14:14:53 2018 Train Epoch: 4 [1094400/1281167 (85%)]\tLoss: 3.929321\n",
            "Tue Oct 30 14:15:44 2018 Train Epoch: 4 [1100800/1281167 (86%)]\tLoss: 4.112450\n",
            "Tue Oct 30 14:16:35 2018 Train Epoch: 4 [1107200/1281167 (86%)]\tLoss: 4.017338\n",
            "Tue Oct 30 14:17:25 2018 Train Epoch: 4 [1113600/1281167 (87%)]\tLoss: 4.676836\n",
            "Tue Oct 30 14:18:16 2018 Train Epoch: 4 [1120000/1281167 (87%)]\tLoss: 4.586837\n",
            "Tue Oct 30 14:19:07 2018 Train Epoch: 4 [1126400/1281167 (88%)]\tLoss: 4.292011\n",
            "Tue Oct 30 14:19:58 2018 Train Epoch: 4 [1132800/1281167 (88%)]\tLoss: 4.696681\n",
            "Tue Oct 30 14:20:48 2018 Train Epoch: 4 [1139200/1281167 (89%)]\tLoss: 4.628151\n",
            "Tue Oct 30 14:21:39 2018 Train Epoch: 4 [1145600/1281167 (89%)]\tLoss: 4.148594\n",
            "Tue Oct 30 14:22:30 2018 Train Epoch: 4 [1152000/1281167 (90%)]\tLoss: 4.547943\n",
            "Tue Oct 30 14:23:21 2018 Train Epoch: 4 [1158400/1281167 (90%)]\tLoss: 4.513001\n",
            "Tue Oct 30 14:24:12 2018 Train Epoch: 4 [1164800/1281167 (91%)]\tLoss: 3.808950\n",
            "Tue Oct 30 14:25:02 2018 Train Epoch: 4 [1171200/1281167 (91%)]\tLoss: 4.335780\n",
            "Tue Oct 30 14:25:53 2018 Train Epoch: 4 [1177600/1281167 (92%)]\tLoss: 4.772411\n",
            "Tue Oct 30 14:26:44 2018 Train Epoch: 4 [1184000/1281167 (92%)]\tLoss: 4.474961\n",
            "Tue Oct 30 14:27:35 2018 Train Epoch: 4 [1190400/1281167 (93%)]\tLoss: 3.924497\n",
            "Tue Oct 30 14:28:26 2018 Train Epoch: 4 [1196800/1281167 (93%)]\tLoss: 4.362881\n",
            "Tue Oct 30 14:29:16 2018 Train Epoch: 4 [1203200/1281167 (94%)]\tLoss: 4.486747\n",
            "Tue Oct 30 14:30:07 2018 Train Epoch: 4 [1209600/1281167 (94%)]\tLoss: 4.449974\n",
            "Tue Oct 30 14:30:58 2018 Train Epoch: 4 [1216000/1281167 (95%)]\tLoss: 4.580257\n",
            "Tue Oct 30 14:31:49 2018 Train Epoch: 4 [1222400/1281167 (95%)]\tLoss: 4.841825\n",
            "Tue Oct 30 14:32:40 2018 Train Epoch: 4 [1228800/1281167 (96%)]\tLoss: 4.110154\n",
            "Tue Oct 30 14:33:30 2018 Train Epoch: 4 [1235200/1281167 (96%)]\tLoss: 4.799195\n",
            "Tue Oct 30 14:34:21 2018 Train Epoch: 4 [1241600/1281167 (97%)]\tLoss: 4.479894\n",
            "Tue Oct 30 14:35:12 2018 Train Epoch: 4 [1248000/1281167 (97%)]\tLoss: 4.004769\n",
            "Tue Oct 30 14:36:02 2018 Train Epoch: 4 [1254400/1281167 (98%)]\tLoss: 4.058695\n",
            "Tue Oct 30 14:36:53 2018 Train Epoch: 4 [1260800/1281167 (98%)]\tLoss: 4.115025\n",
            "Tue Oct 30 14:37:44 2018 Train Epoch: 4 [1267200/1281167 (99%)]\tLoss: 4.446707\n",
            "Tue Oct 30 14:38:35 2018 Train Epoch: 4 [1273600/1281167 (99%)]\tLoss: 4.502441\n",
            "Tue Oct 30 14:39:26 2018 Train Epoch: 4 [1280000/1281167 (100%)]\tLoss: 4.046620\n",
            "\n",
            "Test set: Average loss: 3.9967, Accuracy: 10239/50000 (20%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints/004.pt\n",
            "\n",
            "Tue Oct 30 14:44:14 2018 Train Epoch: 5 [0/1281167 (0%)]\tLoss: 4.088728\n",
            "Tue Oct 30 14:45:04 2018 Train Epoch: 5 [6400/1281167 (0%)]\tLoss: 4.248027\n",
            "Tue Oct 30 14:45:55 2018 Train Epoch: 5 [12800/1281167 (1%)]\tLoss: 4.616394\n",
            "Tue Oct 30 14:46:46 2018 Train Epoch: 5 [19200/1281167 (1%)]\tLoss: 4.381684\n",
            "Tue Oct 30 14:47:37 2018 Train Epoch: 5 [25600/1281167 (2%)]\tLoss: 4.300542\n",
            "Tue Oct 30 14:48:28 2018 Train Epoch: 5 [32000/1281167 (2%)]\tLoss: 4.414816\n",
            "Tue Oct 30 14:49:18 2018 Train Epoch: 5 [38400/1281167 (3%)]\tLoss: 4.874703\n",
            "Tue Oct 30 14:50:09 2018 Train Epoch: 5 [44800/1281167 (3%)]\tLoss: 4.398482\n",
            "Tue Oct 30 14:51:00 2018 Train Epoch: 5 [51200/1281167 (4%)]\tLoss: 4.525040\n",
            "Tue Oct 30 14:51:51 2018 Train Epoch: 5 [57600/1281167 (4%)]\tLoss: 4.164746\n",
            "Tue Oct 30 14:52:42 2018 Train Epoch: 5 [64000/1281167 (5%)]\tLoss: 4.228833\n",
            "Tue Oct 30 14:53:33 2018 Train Epoch: 5 [70400/1281167 (5%)]\tLoss: 4.586053\n",
            "Tue Oct 30 14:54:23 2018 Train Epoch: 5 [76800/1281167 (6%)]\tLoss: 4.475944\n",
            "Tue Oct 30 14:55:14 2018 Train Epoch: 5 [83200/1281167 (6%)]\tLoss: 4.711662\n",
            "Tue Oct 30 14:56:05 2018 Train Epoch: 5 [89600/1281167 (7%)]\tLoss: 4.543445\n",
            "Tue Oct 30 14:56:56 2018 Train Epoch: 5 [96000/1281167 (7%)]\tLoss: 4.018786\n",
            "Tue Oct 30 14:57:47 2018 Train Epoch: 5 [102400/1281167 (8%)]\tLoss: 4.335817\n",
            "Tue Oct 30 14:58:37 2018 Train Epoch: 5 [108800/1281167 (8%)]\tLoss: 4.297790\n",
            "Tue Oct 30 14:59:28 2018 Train Epoch: 5 [115200/1281167 (9%)]\tLoss: 4.050957\n",
            "Tue Oct 30 15:00:19 2018 Train Epoch: 5 [121600/1281167 (9%)]\tLoss: 4.284925\n",
            "Tue Oct 30 15:01:10 2018 Train Epoch: 5 [128000/1281167 (10%)]\tLoss: 4.749276\n",
            "Tue Oct 30 15:02:01 2018 Train Epoch: 5 [134400/1281167 (10%)]\tLoss: 4.713478\n",
            "Tue Oct 30 15:02:51 2018 Train Epoch: 5 [140800/1281167 (11%)]\tLoss: 4.446422\n",
            "Tue Oct 30 15:03:42 2018 Train Epoch: 5 [147200/1281167 (11%)]\tLoss: 4.948208\n",
            "Tue Oct 30 15:04:33 2018 Train Epoch: 5 [153600/1281167 (12%)]\tLoss: 4.084459\n",
            "Tue Oct 30 15:05:24 2018 Train Epoch: 5 [160000/1281167 (12%)]\tLoss: 4.272468\n",
            "Tue Oct 30 15:06:15 2018 Train Epoch: 5 [166400/1281167 (13%)]\tLoss: 4.906515\n",
            "Tue Oct 30 15:07:05 2018 Train Epoch: 5 [172800/1281167 (13%)]\tLoss: 4.704300\n",
            "Tue Oct 30 15:07:56 2018 Train Epoch: 5 [179200/1281167 (14%)]\tLoss: 4.342926\n",
            "Tue Oct 30 15:08:47 2018 Train Epoch: 5 [185600/1281167 (14%)]\tLoss: 4.315537\n",
            "Tue Oct 30 15:09:38 2018 Train Epoch: 5 [192000/1281167 (15%)]\tLoss: 4.971301\n",
            "Tue Oct 30 15:10:28 2018 Train Epoch: 5 [198400/1281167 (15%)]\tLoss: 4.500031\n",
            "Tue Oct 30 15:11:19 2018 Train Epoch: 5 [204800/1281167 (16%)]\tLoss: 4.492450\n",
            "Tue Oct 30 15:12:10 2018 Train Epoch: 5 [211200/1281167 (16%)]\tLoss: 4.551591\n",
            "Tue Oct 30 15:13:00 2018 Train Epoch: 5 [217600/1281167 (17%)]\tLoss: 4.309245\n",
            "Tue Oct 30 15:13:51 2018 Train Epoch: 5 [224000/1281167 (17%)]\tLoss: 4.773957\n",
            "Tue Oct 30 15:14:42 2018 Train Epoch: 5 [230400/1281167 (18%)]\tLoss: 4.394470\n",
            "Tue Oct 30 15:15:33 2018 Train Epoch: 5 [236800/1281167 (18%)]\tLoss: 4.653497\n",
            "Tue Oct 30 15:16:23 2018 Train Epoch: 5 [243200/1281167 (19%)]\tLoss: 4.485395\n",
            "Tue Oct 30 15:17:14 2018 Train Epoch: 5 [249600/1281167 (19%)]\tLoss: 4.650530\n",
            "Tue Oct 30 15:18:05 2018 Train Epoch: 5 [256000/1281167 (20%)]\tLoss: 4.503086\n",
            "Tue Oct 30 15:18:56 2018 Train Epoch: 5 [262400/1281167 (20%)]\tLoss: 4.198109\n",
            "Tue Oct 30 15:19:47 2018 Train Epoch: 5 [268800/1281167 (21%)]\tLoss: 4.445063\n",
            "Tue Oct 30 15:20:37 2018 Train Epoch: 5 [275200/1281167 (21%)]\tLoss: 4.745540\n",
            "Tue Oct 30 15:21:28 2018 Train Epoch: 5 [281600/1281167 (22%)]\tLoss: 4.199045\n",
            "Tue Oct 30 15:22:19 2018 Train Epoch: 5 [288000/1281167 (22%)]\tLoss: 4.487698\n",
            "Tue Oct 30 15:23:09 2018 Train Epoch: 5 [294400/1281167 (23%)]\tLoss: 4.751535\n",
            "Tue Oct 30 15:24:00 2018 Train Epoch: 5 [300800/1281167 (23%)]\tLoss: 4.571313\n",
            "Tue Oct 30 15:24:51 2018 Train Epoch: 5 [307200/1281167 (24%)]\tLoss: 4.036429\n",
            "Tue Oct 30 15:25:42 2018 Train Epoch: 5 [313600/1281167 (24%)]\tLoss: 4.559458\n",
            "Tue Oct 30 15:26:33 2018 Train Epoch: 5 [320000/1281167 (25%)]\tLoss: 4.686769\n",
            "Tue Oct 30 15:27:23 2018 Train Epoch: 5 [326400/1281167 (25%)]\tLoss: 4.233478\n",
            "Tue Oct 30 15:28:14 2018 Train Epoch: 5 [332800/1281167 (26%)]\tLoss: 4.372610\n",
            "Tue Oct 30 15:29:05 2018 Train Epoch: 5 [339200/1281167 (26%)]\tLoss: 4.403720\n",
            "Tue Oct 30 15:29:56 2018 Train Epoch: 5 [345600/1281167 (27%)]\tLoss: 4.286093\n",
            "Tue Oct 30 15:30:47 2018 Train Epoch: 5 [352000/1281167 (27%)]\tLoss: 4.435721\n",
            "Tue Oct 30 15:31:37 2018 Train Epoch: 5 [358400/1281167 (28%)]\tLoss: 4.443100\n",
            "Tue Oct 30 15:32:28 2018 Train Epoch: 5 [364800/1281167 (28%)]\tLoss: 4.243019\n",
            "Tue Oct 30 15:33:19 2018 Train Epoch: 5 [371200/1281167 (29%)]\tLoss: 3.990894\n",
            "Tue Oct 30 15:34:10 2018 Train Epoch: 5 [377600/1281167 (29%)]\tLoss: 4.286910\n",
            "Tue Oct 30 15:35:00 2018 Train Epoch: 5 [384000/1281167 (30%)]\tLoss: 4.561100\n",
            "Tue Oct 30 15:35:51 2018 Train Epoch: 5 [390400/1281167 (30%)]\tLoss: 4.771923\n",
            "Tue Oct 30 15:36:42 2018 Train Epoch: 5 [396800/1281167 (31%)]\tLoss: 4.474010\n",
            "Tue Oct 30 15:37:32 2018 Train Epoch: 5 [403200/1281167 (31%)]\tLoss: 4.805939\n",
            "Tue Oct 30 15:38:23 2018 Train Epoch: 5 [409600/1281167 (32%)]\tLoss: 4.241886\n",
            "Tue Oct 30 15:39:14 2018 Train Epoch: 5 [416000/1281167 (32%)]\tLoss: 4.900828\n",
            "Tue Oct 30 15:40:05 2018 Train Epoch: 5 [422400/1281167 (33%)]\tLoss: 4.348320\n",
            "Tue Oct 30 15:40:55 2018 Train Epoch: 5 [428800/1281167 (33%)]\tLoss: 4.625053\n",
            "Tue Oct 30 15:41:46 2018 Train Epoch: 5 [435200/1281167 (34%)]\tLoss: 4.674705\n",
            "Tue Oct 30 15:42:37 2018 Train Epoch: 5 [441600/1281167 (34%)]\tLoss: 4.912070\n",
            "Tue Oct 30 15:43:28 2018 Train Epoch: 5 [448000/1281167 (35%)]\tLoss: 4.519854\n",
            "Tue Oct 30 15:44:18 2018 Train Epoch: 5 [454400/1281167 (35%)]\tLoss: 4.544498\n",
            "Tue Oct 30 15:45:09 2018 Train Epoch: 5 [460800/1281167 (36%)]\tLoss: 4.672345\n",
            "Tue Oct 30 15:46:00 2018 Train Epoch: 5 [467200/1281167 (36%)]\tLoss: 4.139441\n",
            "Tue Oct 30 15:46:51 2018 Train Epoch: 5 [473600/1281167 (37%)]\tLoss: 4.585102\n",
            "Tue Oct 30 15:47:42 2018 Train Epoch: 5 [480000/1281167 (37%)]\tLoss: 3.939230\n",
            "Tue Oct 30 15:48:32 2018 Train Epoch: 5 [486400/1281167 (38%)]\tLoss: 4.284182\n",
            "Tue Oct 30 15:49:23 2018 Train Epoch: 5 [492800/1281167 (38%)]\tLoss: 3.868310\n",
            "Tue Oct 30 15:50:14 2018 Train Epoch: 5 [499200/1281167 (39%)]\tLoss: 4.099801\n",
            "Tue Oct 30 15:51:05 2018 Train Epoch: 5 [505600/1281167 (39%)]\tLoss: 4.438879\n",
            "Tue Oct 30 15:51:55 2018 Train Epoch: 5 [512000/1281167 (40%)]\tLoss: 3.857712\n",
            "Tue Oct 30 15:52:46 2018 Train Epoch: 5 [518400/1281167 (40%)]\tLoss: 4.752596\n",
            "Tue Oct 30 15:53:37 2018 Train Epoch: 5 [524800/1281167 (41%)]\tLoss: 4.688989\n",
            "Tue Oct 30 15:54:28 2018 Train Epoch: 5 [531200/1281167 (41%)]\tLoss: 4.695862\n",
            "Tue Oct 30 15:55:18 2018 Train Epoch: 5 [537600/1281167 (42%)]\tLoss: 4.774186\n",
            "Tue Oct 30 15:56:09 2018 Train Epoch: 5 [544000/1281167 (42%)]\tLoss: 4.941554\n",
            "Tue Oct 30 15:57:00 2018 Train Epoch: 5 [550400/1281167 (43%)]\tLoss: 4.771322\n",
            "Tue Oct 30 15:57:50 2018 Train Epoch: 5 [556800/1281167 (43%)]\tLoss: 3.818574\n",
            "Tue Oct 30 15:58:41 2018 Train Epoch: 5 [563200/1281167 (44%)]\tLoss: 4.777656\n",
            "Tue Oct 30 15:59:32 2018 Train Epoch: 5 [569600/1281167 (44%)]\tLoss: 5.079805\n",
            "Tue Oct 30 16:00:23 2018 Train Epoch: 5 [576000/1281167 (45%)]\tLoss: 4.527621\n",
            "Tue Oct 30 16:01:14 2018 Train Epoch: 5 [582400/1281167 (45%)]\tLoss: 4.398356\n",
            "Tue Oct 30 16:02:04 2018 Train Epoch: 5 [588800/1281167 (46%)]\tLoss: 4.458526\n",
            "Tue Oct 30 16:02:55 2018 Train Epoch: 5 [595200/1281167 (46%)]\tLoss: 4.827171\n",
            "Tue Oct 30 16:03:46 2018 Train Epoch: 5 [601600/1281167 (47%)]\tLoss: 4.545710\n",
            "Tue Oct 30 16:04:36 2018 Train Epoch: 5 [608000/1281167 (47%)]\tLoss: 4.843643\n",
            "Tue Oct 30 16:05:27 2018 Train Epoch: 5 [614400/1281167 (48%)]\tLoss: 4.087441\n",
            "Tue Oct 30 16:06:18 2018 Train Epoch: 5 [620800/1281167 (48%)]\tLoss: 4.395268\n",
            "Tue Oct 30 16:07:09 2018 Train Epoch: 5 [627200/1281167 (49%)]\tLoss: 4.608300\n",
            "Tue Oct 30 16:08:00 2018 Train Epoch: 5 [633600/1281167 (49%)]\tLoss: 4.488304\n",
            "Tue Oct 30 16:08:50 2018 Train Epoch: 5 [640000/1281167 (50%)]\tLoss: 4.841429\n",
            "Tue Oct 30 16:09:41 2018 Train Epoch: 5 [646400/1281167 (50%)]\tLoss: 4.680462\n",
            "Tue Oct 30 16:10:32 2018 Train Epoch: 5 [652800/1281167 (51%)]\tLoss: 4.438711\n",
            "Tue Oct 30 16:11:22 2018 Train Epoch: 5 [659200/1281167 (51%)]\tLoss: 4.808600\n",
            "Tue Oct 30 16:12:13 2018 Train Epoch: 5 [665600/1281167 (52%)]\tLoss: 4.695272\n",
            "Tue Oct 30 16:13:04 2018 Train Epoch: 5 [672000/1281167 (52%)]\tLoss: 4.059128\n",
            "Tue Oct 30 16:13:55 2018 Train Epoch: 5 [678400/1281167 (53%)]\tLoss: 4.590642\n",
            "Tue Oct 30 16:14:46 2018 Train Epoch: 5 [684800/1281167 (53%)]\tLoss: 4.590654\n",
            "Tue Oct 30 16:15:36 2018 Train Epoch: 5 [691200/1281167 (54%)]\tLoss: 4.292578\n",
            "Tue Oct 30 16:16:27 2018 Train Epoch: 5 [697600/1281167 (54%)]\tLoss: 4.191057\n",
            "Tue Oct 30 16:17:18 2018 Train Epoch: 5 [704000/1281167 (55%)]\tLoss: 4.579885\n",
            "Tue Oct 30 16:18:08 2018 Train Epoch: 5 [710400/1281167 (55%)]\tLoss: 4.777526\n",
            "Tue Oct 30 16:18:59 2018 Train Epoch: 5 [716800/1281167 (56%)]\tLoss: 4.222088\n",
            "Tue Oct 30 16:19:50 2018 Train Epoch: 5 [723200/1281167 (56%)]\tLoss: 3.895839\n",
            "Tue Oct 30 16:20:41 2018 Train Epoch: 5 [729600/1281167 (57%)]\tLoss: 4.612596\n",
            "Tue Oct 30 16:21:31 2018 Train Epoch: 5 [736000/1281167 (57%)]\tLoss: 3.886973\n",
            "Tue Oct 30 16:22:22 2018 Train Epoch: 5 [742400/1281167 (58%)]\tLoss: 4.365654\n",
            "Tue Oct 30 16:23:13 2018 Train Epoch: 5 [748800/1281167 (58%)]\tLoss: 4.612789\n",
            "Tue Oct 30 16:24:04 2018 Train Epoch: 5 [755200/1281167 (59%)]\tLoss: 4.187663\n",
            "Tue Oct 30 16:24:54 2018 Train Epoch: 5 [761600/1281167 (59%)]\tLoss: 4.193141\n",
            "Tue Oct 30 16:25:45 2018 Train Epoch: 5 [768000/1281167 (60%)]\tLoss: 4.597385\n",
            "Tue Oct 30 16:26:36 2018 Train Epoch: 5 [774400/1281167 (60%)]\tLoss: 4.442021\n",
            "Tue Oct 30 16:27:27 2018 Train Epoch: 5 [780800/1281167 (61%)]\tLoss: 4.024860\n",
            "Tue Oct 30 16:28:17 2018 Train Epoch: 5 [787200/1281167 (61%)]\tLoss: 4.608717\n",
            "Tue Oct 30 16:29:08 2018 Train Epoch: 5 [793600/1281167 (62%)]\tLoss: 4.373406\n",
            "Tue Oct 30 16:29:59 2018 Train Epoch: 5 [800000/1281167 (62%)]\tLoss: 4.302611\n",
            "Tue Oct 30 16:30:49 2018 Train Epoch: 5 [806400/1281167 (63%)]\tLoss: 4.563334\n",
            "Tue Oct 30 16:31:40 2018 Train Epoch: 5 [812800/1281167 (63%)]\tLoss: 4.499501\n",
            "Tue Oct 30 16:32:31 2018 Train Epoch: 5 [819200/1281167 (64%)]\tLoss: 4.570326\n",
            "Tue Oct 30 16:33:22 2018 Train Epoch: 5 [825600/1281167 (64%)]\tLoss: 4.430508\n",
            "Tue Oct 30 16:34:13 2018 Train Epoch: 5 [832000/1281167 (65%)]\tLoss: 4.376759\n",
            "Tue Oct 30 16:35:03 2018 Train Epoch: 5 [838400/1281167 (65%)]\tLoss: 4.279802\n",
            "Tue Oct 30 16:35:54 2018 Train Epoch: 5 [844800/1281167 (66%)]\tLoss: 4.464610\n",
            "Tue Oct 30 16:36:45 2018 Train Epoch: 5 [851200/1281167 (66%)]\tLoss: 4.310366\n",
            "Tue Oct 30 16:37:36 2018 Train Epoch: 5 [857600/1281167 (67%)]\tLoss: 4.709105\n",
            "Tue Oct 30 16:38:26 2018 Train Epoch: 5 [864000/1281167 (67%)]\tLoss: 4.162618\n",
            "Tue Oct 30 16:39:17 2018 Train Epoch: 5 [870400/1281167 (68%)]\tLoss: 4.401238\n",
            "Tue Oct 30 16:40:08 2018 Train Epoch: 5 [876800/1281167 (68%)]\tLoss: 4.901444\n",
            "Tue Oct 30 16:40:59 2018 Train Epoch: 5 [883200/1281167 (69%)]\tLoss: 4.367226\n",
            "Tue Oct 30 16:41:49 2018 Train Epoch: 5 [889600/1281167 (69%)]\tLoss: 4.173790\n",
            "Tue Oct 30 16:42:40 2018 Train Epoch: 5 [896000/1281167 (70%)]\tLoss: 4.594368\n",
            "Tue Oct 30 16:43:31 2018 Train Epoch: 5 [902400/1281167 (70%)]\tLoss: 4.272803\n",
            "Tue Oct 30 16:44:22 2018 Train Epoch: 5 [908800/1281167 (71%)]\tLoss: 4.188981\n",
            "Tue Oct 30 16:45:12 2018 Train Epoch: 5 [915200/1281167 (71%)]\tLoss: 4.654523\n",
            "Tue Oct 30 16:46:03 2018 Train Epoch: 5 [921600/1281167 (72%)]\tLoss: 5.009314\n",
            "Tue Oct 30 16:46:54 2018 Train Epoch: 5 [928000/1281167 (72%)]\tLoss: 4.232843\n",
            "Tue Oct 30 16:47:45 2018 Train Epoch: 5 [934400/1281167 (73%)]\tLoss: 4.391093\n",
            "Tue Oct 30 16:48:35 2018 Train Epoch: 5 [940800/1281167 (73%)]\tLoss: 4.153628\n",
            "Tue Oct 30 16:49:26 2018 Train Epoch: 5 [947200/1281167 (74%)]\tLoss: 4.641289\n",
            "Tue Oct 30 16:50:17 2018 Train Epoch: 5 [953600/1281167 (74%)]\tLoss: 4.139238\n",
            "Tue Oct 30 16:51:08 2018 Train Epoch: 5 [960000/1281167 (75%)]\tLoss: 4.337902\n",
            "Tue Oct 30 16:51:59 2018 Train Epoch: 5 [966400/1281167 (75%)]\tLoss: 4.362521\n",
            "Tue Oct 30 16:52:49 2018 Train Epoch: 5 [972800/1281167 (76%)]\tLoss: 4.355000\n",
            "Tue Oct 30 16:53:40 2018 Train Epoch: 5 [979200/1281167 (76%)]\tLoss: 4.391406\n",
            "Tue Oct 30 16:54:31 2018 Train Epoch: 5 [985600/1281167 (77%)]\tLoss: 4.785850\n",
            "Tue Oct 30 16:55:21 2018 Train Epoch: 5 [992000/1281167 (77%)]\tLoss: 4.402971\n",
            "Tue Oct 30 16:56:12 2018 Train Epoch: 5 [998400/1281167 (78%)]\tLoss: 4.212342\n",
            "Tue Oct 30 16:57:03 2018 Train Epoch: 5 [1004800/1281167 (78%)]\tLoss: 4.711353\n",
            "Tue Oct 30 16:57:54 2018 Train Epoch: 5 [1011200/1281167 (79%)]\tLoss: 4.357959\n",
            "Tue Oct 30 16:58:45 2018 Train Epoch: 5 [1017600/1281167 (79%)]\tLoss: 4.746626\n",
            "Tue Oct 30 16:59:35 2018 Train Epoch: 5 [1024000/1281167 (80%)]\tLoss: 4.121311\n",
            "Tue Oct 30 17:00:26 2018 Train Epoch: 5 [1030400/1281167 (80%)]\tLoss: 4.789007\n",
            "Tue Oct 30 17:01:17 2018 Train Epoch: 5 [1036800/1281167 (81%)]\tLoss: 4.026502\n",
            "Tue Oct 30 17:02:08 2018 Train Epoch: 5 [1043200/1281167 (81%)]\tLoss: 4.420250\n",
            "Tue Oct 30 17:02:59 2018 Train Epoch: 5 [1049600/1281167 (82%)]\tLoss: 4.388356\n",
            "Tue Oct 30 17:03:49 2018 Train Epoch: 5 [1056000/1281167 (82%)]\tLoss: 4.539374\n",
            "Tue Oct 30 17:04:40 2018 Train Epoch: 5 [1062400/1281167 (83%)]\tLoss: 4.557135\n",
            "Tue Oct 30 17:05:31 2018 Train Epoch: 5 [1068800/1281167 (83%)]\tLoss: 4.535592\n",
            "Tue Oct 30 17:06:22 2018 Train Epoch: 5 [1075200/1281167 (84%)]\tLoss: 3.964761\n",
            "Tue Oct 30 17:07:12 2018 Train Epoch: 5 [1081600/1281167 (84%)]\tLoss: 4.511739\n",
            "Tue Oct 30 17:08:03 2018 Train Epoch: 5 [1088000/1281167 (85%)]\tLoss: 4.358665\n",
            "Tue Oct 30 17:08:54 2018 Train Epoch: 5 [1094400/1281167 (85%)]\tLoss: 4.494194\n",
            "Tue Oct 30 17:09:45 2018 Train Epoch: 5 [1100800/1281167 (86%)]\tLoss: 4.220880\n",
            "Tue Oct 30 17:10:35 2018 Train Epoch: 5 [1107200/1281167 (86%)]\tLoss: 4.768419\n",
            "Tue Oct 30 17:11:26 2018 Train Epoch: 5 [1113600/1281167 (87%)]\tLoss: 4.474492\n",
            "Tue Oct 30 17:12:17 2018 Train Epoch: 5 [1120000/1281167 (87%)]\tLoss: 4.407974\n",
            "Tue Oct 30 17:13:08 2018 Train Epoch: 5 [1126400/1281167 (88%)]\tLoss: 4.560812\n",
            "Tue Oct 30 17:13:59 2018 Train Epoch: 5 [1132800/1281167 (88%)]\tLoss: 4.163707\n",
            "Tue Oct 30 17:14:49 2018 Train Epoch: 5 [1139200/1281167 (89%)]\tLoss: 4.289778\n",
            "Tue Oct 30 17:15:40 2018 Train Epoch: 5 [1145600/1281167 (89%)]\tLoss: 4.403949\n",
            "Tue Oct 30 17:16:31 2018 Train Epoch: 5 [1152000/1281167 (90%)]\tLoss: 4.775984\n",
            "Tue Oct 30 17:17:21 2018 Train Epoch: 5 [1158400/1281167 (90%)]\tLoss: 4.475427\n",
            "Tue Oct 30 17:18:12 2018 Train Epoch: 5 [1164800/1281167 (91%)]\tLoss: 4.833892\n",
            "Tue Oct 30 17:19:03 2018 Train Epoch: 5 [1171200/1281167 (91%)]\tLoss: 4.081430\n",
            "Tue Oct 30 17:19:54 2018 Train Epoch: 5 [1177600/1281167 (92%)]\tLoss: 4.463521\n",
            "Tue Oct 30 17:20:44 2018 Train Epoch: 5 [1184000/1281167 (92%)]\tLoss: 3.858875\n",
            "Tue Oct 30 17:21:35 2018 Train Epoch: 5 [1190400/1281167 (93%)]\tLoss: 4.643076\n",
            "Tue Oct 30 17:22:26 2018 Train Epoch: 5 [1196800/1281167 (93%)]\tLoss: 4.116141\n",
            "Tue Oct 30 17:23:17 2018 Train Epoch: 5 [1203200/1281167 (94%)]\tLoss: 4.234373\n",
            "Tue Oct 30 17:24:07 2018 Train Epoch: 5 [1209600/1281167 (94%)]\tLoss: 3.804493\n",
            "Tue Oct 30 17:24:58 2018 Train Epoch: 5 [1216000/1281167 (95%)]\tLoss: 4.162137\n",
            "Tue Oct 30 17:25:49 2018 Train Epoch: 5 [1222400/1281167 (95%)]\tLoss: 4.343691\n",
            "Tue Oct 30 17:26:39 2018 Train Epoch: 5 [1228800/1281167 (96%)]\tLoss: 5.054109\n",
            "Tue Oct 30 17:27:30 2018 Train Epoch: 5 [1235200/1281167 (96%)]\tLoss: 4.693175\n",
            "Tue Oct 30 17:28:21 2018 Train Epoch: 5 [1241600/1281167 (97%)]\tLoss: 4.451933\n",
            "Tue Oct 30 17:29:12 2018 Train Epoch: 5 [1248000/1281167 (97%)]\tLoss: 4.114504\n",
            "Tue Oct 30 17:30:02 2018 Train Epoch: 5 [1254400/1281167 (98%)]\tLoss: 4.287147\n",
            "Tue Oct 30 17:30:53 2018 Train Epoch: 5 [1260800/1281167 (98%)]\tLoss: 4.499364\n",
            "Tue Oct 30 17:31:44 2018 Train Epoch: 5 [1267200/1281167 (99%)]\tLoss: 4.294094\n",
            "Tue Oct 30 17:32:35 2018 Train Epoch: 5 [1273600/1281167 (99%)]\tLoss: 4.678521\n",
            "Tue Oct 30 17:33:25 2018 Train Epoch: 5 [1280000/1281167 (100%)]\tLoss: 4.632885\n",
            "\n",
            "Test set: Average loss: 3.9259, Accuracy: 10527/50000 (21%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints/005.pt\n",
            "\n",
            "Tue Oct 30 17:37:49 2018 Train Epoch: 6 [0/1281167 (0%)]\tLoss: 4.494160\n",
            "Tue Oct 30 17:38:40 2018 Train Epoch: 6 [6400/1281167 (0%)]\tLoss: 4.928242\n",
            "Tue Oct 30 17:39:31 2018 Train Epoch: 6 [12800/1281167 (1%)]\tLoss: 4.393927\n",
            "Tue Oct 30 17:40:21 2018 Train Epoch: 6 [19200/1281167 (1%)]\tLoss: 4.490721\n",
            "Tue Oct 30 17:41:12 2018 Train Epoch: 6 [25600/1281167 (2%)]\tLoss: 4.680503\n",
            "Tue Oct 30 17:42:03 2018 Train Epoch: 6 [32000/1281167 (2%)]\tLoss: 4.149325\n",
            "Tue Oct 30 17:42:54 2018 Train Epoch: 6 [38400/1281167 (3%)]\tLoss: 4.759022\n",
            "Tue Oct 30 17:43:44 2018 Train Epoch: 6 [44800/1281167 (3%)]\tLoss: 4.282237\n",
            "Tue Oct 30 17:44:35 2018 Train Epoch: 6 [51200/1281167 (4%)]\tLoss: 4.629981\n",
            "Tue Oct 30 17:45:26 2018 Train Epoch: 6 [57600/1281167 (4%)]\tLoss: 4.337430\n",
            "Tue Oct 30 17:46:16 2018 Train Epoch: 6 [64000/1281167 (5%)]\tLoss: 4.537935\n",
            "Tue Oct 30 17:47:07 2018 Train Epoch: 6 [70400/1281167 (5%)]\tLoss: 4.621165\n",
            "Tue Oct 30 17:47:58 2018 Train Epoch: 6 [76800/1281167 (6%)]\tLoss: 4.156336\n",
            "Tue Oct 30 17:48:49 2018 Train Epoch: 6 [83200/1281167 (6%)]\tLoss: 4.274311\n",
            "Tue Oct 30 17:49:39 2018 Train Epoch: 6 [89600/1281167 (7%)]\tLoss: 4.409196\n",
            "Tue Oct 30 17:50:30 2018 Train Epoch: 6 [96000/1281167 (7%)]\tLoss: 4.564706\n",
            "Tue Oct 30 17:51:21 2018 Train Epoch: 6 [102400/1281167 (8%)]\tLoss: 4.206605\n",
            "Tue Oct 30 17:52:12 2018 Train Epoch: 6 [108800/1281167 (8%)]\tLoss: 4.542739\n",
            "Tue Oct 30 17:53:02 2018 Train Epoch: 6 [115200/1281167 (9%)]\tLoss: 3.984964\n",
            "Tue Oct 30 17:53:53 2018 Train Epoch: 6 [121600/1281167 (9%)]\tLoss: 4.146012\n",
            "Tue Oct 30 17:54:44 2018 Train Epoch: 6 [128000/1281167 (10%)]\tLoss: 4.307233\n",
            "Tue Oct 30 17:55:34 2018 Train Epoch: 6 [134400/1281167 (10%)]\tLoss: 4.100389\n",
            "Tue Oct 30 17:56:25 2018 Train Epoch: 6 [140800/1281167 (11%)]\tLoss: 4.389561\n",
            "Tue Oct 30 17:57:16 2018 Train Epoch: 6 [147200/1281167 (11%)]\tLoss: 4.170390\n",
            "Tue Oct 30 17:58:07 2018 Train Epoch: 6 [153600/1281167 (12%)]\tLoss: 4.468164\n",
            "Tue Oct 30 17:58:57 2018 Train Epoch: 6 [160000/1281167 (12%)]\tLoss: 4.561187\n",
            "Tue Oct 30 17:59:48 2018 Train Epoch: 6 [166400/1281167 (13%)]\tLoss: 3.920195\n",
            "Tue Oct 30 18:00:39 2018 Train Epoch: 6 [172800/1281167 (13%)]\tLoss: 4.462427\n",
            "Tue Oct 30 18:01:30 2018 Train Epoch: 6 [179200/1281167 (14%)]\tLoss: 4.429583\n",
            "Tue Oct 30 18:02:20 2018 Train Epoch: 6 [185600/1281167 (14%)]\tLoss: 4.501823\n",
            "Tue Oct 30 18:03:11 2018 Train Epoch: 6 [192000/1281167 (15%)]\tLoss: 4.082626\n",
            "Tue Oct 30 18:04:02 2018 Train Epoch: 6 [198400/1281167 (15%)]\tLoss: 3.996952\n",
            "Tue Oct 30 18:04:53 2018 Train Epoch: 6 [204800/1281167 (16%)]\tLoss: 4.545144\n",
            "Tue Oct 30 18:05:43 2018 Train Epoch: 6 [211200/1281167 (16%)]\tLoss: 4.894878\n",
            "Tue Oct 30 18:06:34 2018 Train Epoch: 6 [217600/1281167 (17%)]\tLoss: 4.302733\n",
            "Tue Oct 30 18:07:25 2018 Train Epoch: 6 [224000/1281167 (17%)]\tLoss: 4.620513\n",
            "Tue Oct 30 18:08:16 2018 Train Epoch: 6 [230400/1281167 (18%)]\tLoss: 4.302439\n",
            "Tue Oct 30 18:09:06 2018 Train Epoch: 6 [236800/1281167 (18%)]\tLoss: 3.972039\n",
            "Tue Oct 30 18:09:57 2018 Train Epoch: 6 [243200/1281167 (19%)]\tLoss: 4.884535\n",
            "Tue Oct 30 18:10:48 2018 Train Epoch: 6 [249600/1281167 (19%)]\tLoss: 4.563716\n",
            "Tue Oct 30 18:11:39 2018 Train Epoch: 6 [256000/1281167 (20%)]\tLoss: 4.741175\n",
            "Tue Oct 30 18:12:29 2018 Train Epoch: 6 [262400/1281167 (20%)]\tLoss: 4.300991\n",
            "Tue Oct 30 18:13:20 2018 Train Epoch: 6 [268800/1281167 (21%)]\tLoss: 4.348900\n",
            "Tue Oct 30 18:14:11 2018 Train Epoch: 6 [275200/1281167 (21%)]\tLoss: 4.486084\n",
            "Tue Oct 30 18:15:02 2018 Train Epoch: 6 [281600/1281167 (22%)]\tLoss: 4.520999\n",
            "Tue Oct 30 18:15:52 2018 Train Epoch: 6 [288000/1281167 (22%)]\tLoss: 4.538039\n",
            "Tue Oct 30 18:16:43 2018 Train Epoch: 6 [294400/1281167 (23%)]\tLoss: 4.542878\n",
            "Tue Oct 30 18:17:34 2018 Train Epoch: 6 [300800/1281167 (23%)]\tLoss: 4.460214\n",
            "Tue Oct 30 18:18:25 2018 Train Epoch: 6 [307200/1281167 (24%)]\tLoss: 4.150142\n",
            "Tue Oct 30 18:19:15 2018 Train Epoch: 6 [313600/1281167 (24%)]\tLoss: 4.011388\n",
            "Tue Oct 30 18:20:06 2018 Train Epoch: 6 [320000/1281167 (25%)]\tLoss: 4.569537\n",
            "Tue Oct 30 18:20:57 2018 Train Epoch: 6 [326400/1281167 (25%)]\tLoss: 4.595655\n",
            "Tue Oct 30 18:21:47 2018 Train Epoch: 6 [332800/1281167 (26%)]\tLoss: 4.360889\n",
            "Tue Oct 30 18:22:38 2018 Train Epoch: 6 [339200/1281167 (26%)]\tLoss: 3.985449\n",
            "Tue Oct 30 18:23:29 2018 Train Epoch: 6 [345600/1281167 (27%)]\tLoss: 4.244031\n",
            "Tue Oct 30 18:24:20 2018 Train Epoch: 6 [352000/1281167 (27%)]\tLoss: 4.491675\n",
            "Tue Oct 30 18:25:10 2018 Train Epoch: 6 [358400/1281167 (28%)]\tLoss: 4.537387\n",
            "Tue Oct 30 18:26:01 2018 Train Epoch: 6 [364800/1281167 (28%)]\tLoss: 4.155804\n",
            "Tue Oct 30 18:26:52 2018 Train Epoch: 6 [371200/1281167 (29%)]\tLoss: 4.424421\n",
            "Tue Oct 30 18:27:42 2018 Train Epoch: 6 [377600/1281167 (29%)]\tLoss: 4.307522\n",
            "Tue Oct 30 18:28:33 2018 Train Epoch: 6 [384000/1281167 (30%)]\tLoss: 4.320107\n",
            "Tue Oct 30 18:29:24 2018 Train Epoch: 6 [390400/1281167 (30%)]\tLoss: 4.307684\n",
            "Tue Oct 30 18:30:15 2018 Train Epoch: 6 [396800/1281167 (31%)]\tLoss: 4.111716\n",
            "Tue Oct 30 18:31:05 2018 Train Epoch: 6 [403200/1281167 (31%)]\tLoss: 4.991809\n",
            "Tue Oct 30 18:31:56 2018 Train Epoch: 6 [409600/1281167 (32%)]\tLoss: 4.661448\n",
            "Tue Oct 30 18:32:47 2018 Train Epoch: 6 [416000/1281167 (32%)]\tLoss: 4.115453\n",
            "Tue Oct 30 18:33:38 2018 Train Epoch: 6 [422400/1281167 (33%)]\tLoss: 4.519752\n",
            "Tue Oct 30 18:34:28 2018 Train Epoch: 6 [428800/1281167 (33%)]\tLoss: 4.280662\n",
            "Tue Oct 30 18:35:19 2018 Train Epoch: 6 [435200/1281167 (34%)]\tLoss: 4.217865\n",
            "Tue Oct 30 18:36:10 2018 Train Epoch: 6 [441600/1281167 (34%)]\tLoss: 4.196814\n",
            "Tue Oct 30 18:37:00 2018 Train Epoch: 6 [448000/1281167 (35%)]\tLoss: 4.304592\n",
            "Tue Oct 30 18:37:51 2018 Train Epoch: 6 [454400/1281167 (35%)]\tLoss: 4.154550\n",
            "Tue Oct 30 18:38:42 2018 Train Epoch: 6 [460800/1281167 (36%)]\tLoss: 4.342982\n",
            "Tue Oct 30 18:39:33 2018 Train Epoch: 6 [467200/1281167 (36%)]\tLoss: 4.203927\n",
            "Tue Oct 30 18:40:24 2018 Train Epoch: 6 [473600/1281167 (37%)]\tLoss: 4.300972\n",
            "Tue Oct 30 18:41:14 2018 Train Epoch: 6 [480000/1281167 (37%)]\tLoss: 4.519413\n",
            "Tue Oct 30 18:42:05 2018 Train Epoch: 6 [486400/1281167 (38%)]\tLoss: 4.361989\n",
            "Tue Oct 30 18:42:56 2018 Train Epoch: 6 [492800/1281167 (38%)]\tLoss: 4.324025\n",
            "Tue Oct 30 18:43:46 2018 Train Epoch: 6 [499200/1281167 (39%)]\tLoss: 4.560677\n",
            "Tue Oct 30 18:44:37 2018 Train Epoch: 6 [505600/1281167 (39%)]\tLoss: 4.171652\n",
            "Tue Oct 30 18:45:28 2018 Train Epoch: 6 [512000/1281167 (40%)]\tLoss: 4.474167\n",
            "Tue Oct 30 18:46:19 2018 Train Epoch: 6 [518400/1281167 (40%)]\tLoss: 4.928854\n",
            "Tue Oct 30 18:47:09 2018 Train Epoch: 6 [524800/1281167 (41%)]\tLoss: 4.183676\n",
            "Tue Oct 30 18:48:00 2018 Train Epoch: 6 [531200/1281167 (41%)]\tLoss: 4.378428\n",
            "Tue Oct 30 18:48:51 2018 Train Epoch: 6 [537600/1281167 (42%)]\tLoss: 4.702655\n",
            "Tue Oct 30 18:49:42 2018 Train Epoch: 6 [544000/1281167 (42%)]\tLoss: 4.567164\n",
            "Tue Oct 30 18:50:32 2018 Train Epoch: 6 [550400/1281167 (43%)]\tLoss: 4.491701\n",
            "Tue Oct 30 18:51:23 2018 Train Epoch: 6 [556800/1281167 (43%)]\tLoss: 4.782929\n",
            "Tue Oct 30 18:52:14 2018 Train Epoch: 6 [563200/1281167 (44%)]\tLoss: 4.512346\n",
            "Tue Oct 30 18:53:04 2018 Train Epoch: 6 [569600/1281167 (44%)]\tLoss: 4.603122\n",
            "Tue Oct 30 18:53:55 2018 Train Epoch: 6 [576000/1281167 (45%)]\tLoss: 4.415398\n",
            "Tue Oct 30 18:54:46 2018 Train Epoch: 6 [582400/1281167 (45%)]\tLoss: 4.295542\n",
            "Tue Oct 30 18:55:37 2018 Train Epoch: 6 [588800/1281167 (46%)]\tLoss: 4.671707\n",
            "Tue Oct 30 18:56:27 2018 Train Epoch: 6 [595200/1281167 (46%)]\tLoss: 4.266114\n",
            "Tue Oct 30 18:57:18 2018 Train Epoch: 6 [601600/1281167 (47%)]\tLoss: 4.491767\n",
            "Tue Oct 30 18:58:09 2018 Train Epoch: 6 [608000/1281167 (47%)]\tLoss: 4.401926\n",
            "Tue Oct 30 18:58:59 2018 Train Epoch: 6 [614400/1281167 (48%)]\tLoss: 4.370622\n",
            "Tue Oct 30 18:59:50 2018 Train Epoch: 6 [620800/1281167 (48%)]\tLoss: 4.237278\n",
            "Tue Oct 30 19:00:41 2018 Train Epoch: 6 [627200/1281167 (49%)]\tLoss: 4.269248\n",
            "Tue Oct 30 19:01:32 2018 Train Epoch: 6 [633600/1281167 (49%)]\tLoss: 4.689591\n",
            "Tue Oct 30 19:02:23 2018 Train Epoch: 6 [640000/1281167 (50%)]\tLoss: 4.698011\n",
            "Tue Oct 30 19:03:13 2018 Train Epoch: 6 [646400/1281167 (50%)]\tLoss: 4.019310\n",
            "Tue Oct 30 19:04:04 2018 Train Epoch: 6 [652800/1281167 (51%)]\tLoss: 4.168496\n",
            "Tue Oct 30 19:04:55 2018 Train Epoch: 6 [659200/1281167 (51%)]\tLoss: 4.564449\n",
            "Tue Oct 30 19:05:45 2018 Train Epoch: 6 [665600/1281167 (52%)]\tLoss: 5.040380\n",
            "Tue Oct 30 19:06:36 2018 Train Epoch: 6 [672000/1281167 (52%)]\tLoss: 4.043626\n",
            "Tue Oct 30 19:07:27 2018 Train Epoch: 6 [678400/1281167 (53%)]\tLoss: 4.342856\n",
            "Tue Oct 30 19:08:17 2018 Train Epoch: 6 [684800/1281167 (53%)]\tLoss: 4.003327\n",
            "Tue Oct 30 19:09:08 2018 Train Epoch: 6 [691200/1281167 (54%)]\tLoss: 4.217785\n",
            "Tue Oct 30 19:09:59 2018 Train Epoch: 6 [697600/1281167 (54%)]\tLoss: 4.712683\n",
            "Tue Oct 30 19:10:50 2018 Train Epoch: 6 [704000/1281167 (55%)]\tLoss: 4.203351\n",
            "Tue Oct 30 19:11:40 2018 Train Epoch: 6 [710400/1281167 (55%)]\tLoss: 4.390080\n",
            "Tue Oct 30 19:12:31 2018 Train Epoch: 6 [716800/1281167 (56%)]\tLoss: 4.116677\n",
            "Tue Oct 30 19:13:22 2018 Train Epoch: 6 [723200/1281167 (56%)]\tLoss: 4.510801\n",
            "Tue Oct 30 19:14:13 2018 Train Epoch: 6 [729600/1281167 (57%)]\tLoss: 4.379219\n",
            "Tue Oct 30 19:15:03 2018 Train Epoch: 6 [736000/1281167 (57%)]\tLoss: 4.423467\n",
            "Tue Oct 30 19:15:54 2018 Train Epoch: 6 [742400/1281167 (58%)]\tLoss: 4.143243\n",
            "Tue Oct 30 19:16:45 2018 Train Epoch: 6 [748800/1281167 (58%)]\tLoss: 4.409686\n",
            "Tue Oct 30 19:17:36 2018 Train Epoch: 6 [755200/1281167 (59%)]\tLoss: 4.456110\n",
            "Tue Oct 30 19:18:26 2018 Train Epoch: 6 [761600/1281167 (59%)]\tLoss: 4.327739\n",
            "Tue Oct 30 19:19:17 2018 Train Epoch: 6 [768000/1281167 (60%)]\tLoss: 4.626921\n",
            "Tue Oct 30 19:20:08 2018 Train Epoch: 6 [774400/1281167 (60%)]\tLoss: 4.546620\n",
            "Tue Oct 30 19:20:58 2018 Train Epoch: 6 [780800/1281167 (61%)]\tLoss: 4.245030\n",
            "Tue Oct 30 19:21:49 2018 Train Epoch: 6 [787200/1281167 (61%)]\tLoss: 3.912608\n",
            "Tue Oct 30 19:22:40 2018 Train Epoch: 6 [793600/1281167 (62%)]\tLoss: 4.402316\n",
            "Tue Oct 30 19:23:31 2018 Train Epoch: 6 [800000/1281167 (62%)]\tLoss: 4.557131\n",
            "Tue Oct 30 19:24:21 2018 Train Epoch: 6 [806400/1281167 (63%)]\tLoss: 4.207969\n",
            "Tue Oct 30 19:25:12 2018 Train Epoch: 6 [812800/1281167 (63%)]\tLoss: 4.742521\n",
            "Tue Oct 30 19:26:03 2018 Train Epoch: 6 [819200/1281167 (64%)]\tLoss: 4.632886\n",
            "Tue Oct 30 19:26:54 2018 Train Epoch: 6 [825600/1281167 (64%)]\tLoss: 4.213624\n",
            "Tue Oct 30 19:27:44 2018 Train Epoch: 6 [832000/1281167 (65%)]\tLoss: 4.441466\n",
            "Tue Oct 30 19:28:35 2018 Train Epoch: 6 [838400/1281167 (65%)]\tLoss: 4.645082\n",
            "Tue Oct 30 19:29:26 2018 Train Epoch: 6 [844800/1281167 (66%)]\tLoss: 4.687679\n",
            "Tue Oct 30 19:30:16 2018 Train Epoch: 6 [851200/1281167 (66%)]\tLoss: 4.854387\n",
            "Tue Oct 30 19:31:07 2018 Train Epoch: 6 [857600/1281167 (67%)]\tLoss: 4.152404\n",
            "Tue Oct 30 19:31:58 2018 Train Epoch: 6 [864000/1281167 (67%)]\tLoss: 4.328933\n",
            "Tue Oct 30 19:32:49 2018 Train Epoch: 6 [870400/1281167 (68%)]\tLoss: 4.394938\n",
            "Tue Oct 30 19:33:39 2018 Train Epoch: 6 [876800/1281167 (68%)]\tLoss: 4.682942\n",
            "Tue Oct 30 19:34:30 2018 Train Epoch: 6 [883200/1281167 (69%)]\tLoss: 4.522542\n",
            "Tue Oct 30 19:35:21 2018 Train Epoch: 6 [889600/1281167 (69%)]\tLoss: 4.563277\n",
            "Tue Oct 30 19:36:12 2018 Train Epoch: 6 [896000/1281167 (70%)]\tLoss: 4.465862\n",
            "Tue Oct 30 19:37:02 2018 Train Epoch: 6 [902400/1281167 (70%)]\tLoss: 4.370194\n",
            "Tue Oct 30 19:37:53 2018 Train Epoch: 6 [908800/1281167 (71%)]\tLoss: 4.108574\n",
            "Tue Oct 30 19:38:44 2018 Train Epoch: 6 [915200/1281167 (71%)]\tLoss: 4.515652\n",
            "Tue Oct 30 19:39:35 2018 Train Epoch: 6 [921600/1281167 (72%)]\tLoss: 4.418824\n",
            "Tue Oct 30 19:40:25 2018 Train Epoch: 6 [928000/1281167 (72%)]\tLoss: 4.455031\n",
            "Tue Oct 30 19:41:16 2018 Train Epoch: 6 [934400/1281167 (73%)]\tLoss: 4.836535\n",
            "Tue Oct 30 19:42:07 2018 Train Epoch: 6 [940800/1281167 (73%)]\tLoss: 4.588972\n",
            "Tue Oct 30 19:42:57 2018 Train Epoch: 6 [947200/1281167 (74%)]\tLoss: 4.276070\n",
            "Tue Oct 30 19:43:48 2018 Train Epoch: 6 [953600/1281167 (74%)]\tLoss: 4.506755\n",
            "Tue Oct 30 19:44:39 2018 Train Epoch: 6 [960000/1281167 (75%)]\tLoss: 4.850711\n",
            "Tue Oct 30 19:45:30 2018 Train Epoch: 6 [966400/1281167 (75%)]\tLoss: 4.614987\n",
            "Tue Oct 30 19:46:20 2018 Train Epoch: 6 [972800/1281167 (76%)]\tLoss: 4.728222\n",
            "Tue Oct 30 19:47:11 2018 Train Epoch: 6 [979200/1281167 (76%)]\tLoss: 4.294471\n",
            "Tue Oct 30 19:48:02 2018 Train Epoch: 6 [985600/1281167 (77%)]\tLoss: 3.850532\n",
            "Tue Oct 30 19:48:53 2018 Train Epoch: 6 [992000/1281167 (77%)]\tLoss: 4.545927\n",
            "Tue Oct 30 19:49:43 2018 Train Epoch: 6 [998400/1281167 (78%)]\tLoss: 4.237354\n",
            "Tue Oct 30 19:50:34 2018 Train Epoch: 6 [1004800/1281167 (78%)]\tLoss: 4.386621\n",
            "Tue Oct 30 19:51:25 2018 Train Epoch: 6 [1011200/1281167 (79%)]\tLoss: 4.724759\n",
            "Tue Oct 30 19:52:15 2018 Train Epoch: 6 [1017600/1281167 (79%)]\tLoss: 4.896079\n",
            "Tue Oct 30 19:53:06 2018 Train Epoch: 6 [1024000/1281167 (80%)]\tLoss: 4.382197\n",
            "Tue Oct 30 19:53:57 2018 Train Epoch: 6 [1030400/1281167 (80%)]\tLoss: 4.394695\n",
            "Tue Oct 30 19:54:48 2018 Train Epoch: 6 [1036800/1281167 (81%)]\tLoss: 4.485454\n",
            "Tue Oct 30 19:55:38 2018 Train Epoch: 6 [1043200/1281167 (81%)]\tLoss: 4.120383\n",
            "Tue Oct 30 19:56:29 2018 Train Epoch: 6 [1049600/1281167 (82%)]\tLoss: 4.426687\n",
            "Tue Oct 30 19:57:20 2018 Train Epoch: 6 [1056000/1281167 (82%)]\tLoss: 4.245152\n",
            "Tue Oct 30 19:58:11 2018 Train Epoch: 6 [1062400/1281167 (83%)]\tLoss: 4.959668\n",
            "Tue Oct 30 19:59:01 2018 Train Epoch: 6 [1068800/1281167 (83%)]\tLoss: 4.187613\n",
            "Tue Oct 30 19:59:52 2018 Train Epoch: 6 [1075200/1281167 (84%)]\tLoss: 3.967801\n",
            "Tue Oct 30 20:00:43 2018 Train Epoch: 6 [1081600/1281167 (84%)]\tLoss: 4.366855\n",
            "Tue Oct 30 20:01:33 2018 Train Epoch: 6 [1088000/1281167 (85%)]\tLoss: 4.813686\n",
            "Tue Oct 30 20:02:24 2018 Train Epoch: 6 [1094400/1281167 (85%)]\tLoss: 4.882539\n",
            "Tue Oct 30 20:03:15 2018 Train Epoch: 6 [1100800/1281167 (86%)]\tLoss: 4.211033\n",
            "Tue Oct 30 20:04:06 2018 Train Epoch: 6 [1107200/1281167 (86%)]\tLoss: 4.819968\n",
            "Tue Oct 30 20:04:56 2018 Train Epoch: 6 [1113600/1281167 (87%)]\tLoss: 5.122205\n",
            "Tue Oct 30 20:05:47 2018 Train Epoch: 6 [1120000/1281167 (87%)]\tLoss: 4.607795\n",
            "Tue Oct 30 20:06:38 2018 Train Epoch: 6 [1126400/1281167 (88%)]\tLoss: 4.017140\n",
            "Tue Oct 30 20:07:28 2018 Train Epoch: 6 [1132800/1281167 (88%)]\tLoss: 4.291925\n",
            "Tue Oct 30 20:08:19 2018 Train Epoch: 6 [1139200/1281167 (89%)]\tLoss: 4.543257\n",
            "Tue Oct 30 20:09:10 2018 Train Epoch: 6 [1145600/1281167 (89%)]\tLoss: 4.611049\n",
            "Tue Oct 30 20:10:00 2018 Train Epoch: 6 [1152000/1281167 (90%)]\tLoss: 4.563066\n",
            "Tue Oct 30 20:10:51 2018 Train Epoch: 6 [1158400/1281167 (90%)]\tLoss: 4.469216\n",
            "Tue Oct 30 20:11:42 2018 Train Epoch: 6 [1164800/1281167 (91%)]\tLoss: 4.171379\n",
            "Tue Oct 30 20:12:33 2018 Train Epoch: 6 [1171200/1281167 (91%)]\tLoss: 4.422902\n",
            "Tue Oct 30 20:13:23 2018 Train Epoch: 6 [1177600/1281167 (92%)]\tLoss: 3.962692\n",
            "Tue Oct 30 20:14:14 2018 Train Epoch: 6 [1184000/1281167 (92%)]\tLoss: 4.225233\n",
            "Tue Oct 30 20:15:05 2018 Train Epoch: 6 [1190400/1281167 (93%)]\tLoss: 4.419035\n",
            "Tue Oct 30 20:15:55 2018 Train Epoch: 6 [1196800/1281167 (93%)]\tLoss: 4.441911\n",
            "Tue Oct 30 20:16:46 2018 Train Epoch: 6 [1203200/1281167 (94%)]\tLoss: 4.626664\n",
            "Tue Oct 30 20:17:37 2018 Train Epoch: 6 [1209600/1281167 (94%)]\tLoss: 4.415197\n",
            "Tue Oct 30 20:18:28 2018 Train Epoch: 6 [1216000/1281167 (95%)]\tLoss: 4.383773\n",
            "Tue Oct 30 20:19:18 2018 Train Epoch: 6 [1222400/1281167 (95%)]\tLoss: 4.701865\n",
            "Tue Oct 30 20:20:09 2018 Train Epoch: 6 [1228800/1281167 (96%)]\tLoss: 3.945023\n",
            "Tue Oct 30 20:21:00 2018 Train Epoch: 6 [1235200/1281167 (96%)]\tLoss: 4.636857\n",
            "Tue Oct 30 20:21:51 2018 Train Epoch: 6 [1241600/1281167 (97%)]\tLoss: 4.281059\n",
            "Tue Oct 30 20:22:41 2018 Train Epoch: 6 [1248000/1281167 (97%)]\tLoss: 4.350073\n",
            "Tue Oct 30 20:23:32 2018 Train Epoch: 6 [1254400/1281167 (98%)]\tLoss: 3.968777\n",
            "Tue Oct 30 20:24:23 2018 Train Epoch: 6 [1260800/1281167 (98%)]\tLoss: 4.541305\n",
            "Tue Oct 30 20:25:13 2018 Train Epoch: 6 [1267200/1281167 (99%)]\tLoss: 4.473972\n",
            "Tue Oct 30 20:26:04 2018 Train Epoch: 6 [1273600/1281167 (99%)]\tLoss: 4.318925\n",
            "Tue Oct 30 20:26:55 2018 Train Epoch: 6 [1280000/1281167 (100%)]\tLoss: 4.372937\n",
            "\n",
            "Test set: Average loss: 3.9923, Accuracy: 10289/50000 (21%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/imagenet_full/checkpoints/006.pt\n",
            "\n",
            "Tue Oct 30 20:31:14 2018 Train Epoch: 7 [0/1281167 (0%)]\tLoss: 4.651990\n",
            "Tue Oct 30 20:32:05 2018 Train Epoch: 7 [6400/1281167 (0%)]\tLoss: 4.791630\n",
            "Tue Oct 30 20:32:56 2018 Train Epoch: 7 [12800/1281167 (1%)]\tLoss: 4.510069\n",
            "Tue Oct 30 20:33:46 2018 Train Epoch: 7 [19200/1281167 (1%)]\tLoss: 4.098652\n",
            "Tue Oct 30 20:34:37 2018 Train Epoch: 7 [25600/1281167 (2%)]\tLoss: 4.867485\n",
            "Tue Oct 30 20:35:28 2018 Train Epoch: 7 [32000/1281167 (2%)]\tLoss: 4.858706\n",
            "Tue Oct 30 20:36:18 2018 Train Epoch: 7 [38400/1281167 (3%)]\tLoss: 4.250910\n",
            "Tue Oct 30 20:37:09 2018 Train Epoch: 7 [44800/1281167 (3%)]\tLoss: 4.035142\n",
            "Tue Oct 30 20:38:00 2018 Train Epoch: 7 [51200/1281167 (4%)]\tLoss: 4.215308\n",
            "Tue Oct 30 20:38:51 2018 Train Epoch: 7 [57600/1281167 (4%)]\tLoss: 4.910914\n",
            "Tue Oct 30 20:39:41 2018 Train Epoch: 7 [64000/1281167 (5%)]\tLoss: 4.490983\n",
            "Tue Oct 30 20:40:32 2018 Train Epoch: 7 [70400/1281167 (5%)]\tLoss: 4.548551\n",
            "Tue Oct 30 20:41:23 2018 Train Epoch: 7 [76800/1281167 (6%)]\tLoss: 4.204519\n",
            "Tue Oct 30 20:42:13 2018 Train Epoch: 7 [83200/1281167 (6%)]\tLoss: 4.606360\n",
            "Tue Oct 30 20:43:04 2018 Train Epoch: 7 [89600/1281167 (7%)]\tLoss: 4.616049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PeBmRTB3b3s7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1826
        },
        "outputId": "426883d1-c176-4797-eb8f-d8f57389904c"
      },
      "cell_type": "code",
      "source": [
        "## for plotting\n",
        "logs = pt_util.read_log(LOG_PATH)\n",
        "logs = np.array(logs)\n",
        "pt_util.plot(logs[:,0], logs[:,1], \"Plot of training loss wrt epoch\", \"epoch\", \"loss on training data\")\n",
        "pt_util.plot(logs[:,0], logs[:,2], \"Plot of test loss wrt epoch\", \"epoch\", \"loss on test data\")\n",
        "pt_util.plot(logs[:,0], logs[:,3], \"Plot of test accuracy wrt epoch\", \"epoch\", \"accuracy on test data\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJMAAAJbCAYAAABQGNVnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f69487983c8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAAJbCAYAAABpf2Q8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f694cc3ac88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJMAAAJbCAYAAABQGNVnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f69481f2470>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4-kI1JSzQbXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 7: Short answer questions\n",
        "Please answer these questions, and put the answers in a file called homework1_python.pdf in your repository.\n",
        "\n",
        "# Tiny ImageNet\n",
        "1. What design that you tried worked the best? This includes things like network design, learning rate, batch size, number of epochs, and other optimization parameters, data augmentation etc. What was the final train loss? Test loss? Test Accuracy? Provide the plots for train loss, test loss, and test accuracy.\n",
        "2. What design worked the worst (but still performed better than random chance)? Provide all the same information as question 1.\n",
        "3. Why do you think the best one worked well and the worst one worked poorly.\n",
        "\n",
        "\n",
        "# Full ImageNet\n",
        "1. What design that you tried worked the best? How many epochs were you able to run it for? Provide the same information from Tiny ImageNet question 1.\n",
        "2. Were you able to use larger/deeper networks on Full ImageNet than you used on Tiny ImageNet and increase accuracy? If so, why? If not, why not?\n",
        "3. The real ImageNet dataset has significantly larger images. How would you change your network design if the images were twice as large? How about smaller than Tiny ImageNet (32x32)? How do you think your accuracy would change? This is open-ended, but we want a more thought-out answer than \"I'd resize the images\" or \"I'd do a larger pooling stride.\" You don't have to write code to test your hypothesis.\n",
        "\n",
        "Additionally, download your .ipynb and put it in your repository and name it homework1_colab.ipynb\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nnCkPqWBGgwX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
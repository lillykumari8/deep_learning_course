{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework2_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S2IaITBP0f5R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 2\n",
        "In this homework, we will explore language generation using character-level RNNs. Sounds awesome, right?\n",
        "\n",
        "A few notes at the beginning:\n",
        "- It might be useful for you to read the whole assignment before beginning. Especially the last two sections so you know what to record for turning in.\n",
        "- Much of the required knowledge in this (and past) homeworks about Python, PyTorch, etc. are not explained fully here. Instead, we expect you to use the existing documentation, search engines, Stack Overflow, etc. for implementation details.\n",
        "- That being said, we have listed several functions in parts of the homework where knowing those functions exist would be especially useful. However you will still need to read the docs on how to specifically use the functions.\n",
        "\n",
        "# Part 0: Initial setup\n",
        "You should recognize this code from last time.\n"
      ]
    },
    {
      "metadata": {
        "id": "4GS0yuGl0mHQ",
        "colab_type": "code",
        "outputId": "d76e534f-b93b-4d80-d570-8c49c83972fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (483.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 483.0MB 53.6MB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x56624000 @  0x7f72a15eb2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 1.2MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n",
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6t3ZIEll0pr-",
        "colab_type": "code",
        "outputId": "2981a161-6758-417d-b10c-b4f7fa12c953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "'My Drive'  'Team Drives'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PEzPNAIY0vkm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Upload the dataset\n",
        "We will be using the complete text of Harry Potter as our corpus. We will provide it for you in a not-very-well-formatted way.\n",
        "Run this code to navigate to the BASE_PATH directory and upload the homework2.tar file inside the BASE_PATH, then extract it.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CLVJPc_90vsB",
        "colab_type": "code",
        "outputId": "94433911-8981-462a-cc93-7cedc458314e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/homework2/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'harry_potter/'\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(DATA_PATH + 'harry_potter.txt'):\n",
        "    !wget http://pjreddie.com/media/files/homework2.tar.gz\n",
        "    !tar -zxvf homework2.tar.gz\n",
        "    !rm homework2.tar.gz\n",
        "import pt_util\n",
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hd1Qx66s19Pl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AxIvm7h62tfx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2: Preprocessing the data\n",
        "In previous homeworks, we have provided a cleaned version of the data. But this time you'll have to do some of that cleaning yourselves.\n",
        "\n",
        "Hints:\n",
        "- train_text and test_text should contain the class indices for the character tokens from the data file. For example, if the text was **`\"ABA CDBE\"`**, the token version would be a numpy array with contents `[0, 1, 0, 2, 3, 4, 1, 5]`\n",
        "- The harry_potter.txt file has weird spacing. You might want to replace all the whitespace characters (space, \\n, \\t, etc.) in the file with the space character.\n",
        "- You should output two files. One for training and one for testing. The training should be the first 80% of the characters.\n",
        "- voc2ind is a map from character to the index of the class for that character. There is no predefined vocabulary, but you will need to be consistent across all tasks that use the vocabulary. For the example above, the voc2ind would be `{'A': 0, 'B': 1, ' ': 2, 'C': 3, 'D': 4, 'E': 5}`\n",
        "- ind2voc is the inverse of voc2ind\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6oZq_S6k3GpB",
        "colab_type": "code",
        "outputId": "b4f48a50-2755-4c41-9f4e-78af09525cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "def prepare_data(data_path):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "    \n",
        "    # TODO Add more preprocessing\n",
        "    data = re.sub('\\s+', ' ', data).strip()\n",
        "\n",
        "    \n",
        "    voc2ind = {}\n",
        "    \n",
        "    # Compute voc2ind and transform the data into an integer representation of the tokens.\n",
        "    count = 0\n",
        "    list_indexed = []\n",
        "    for char in data:\n",
        "        if char not in voc2ind.keys():\n",
        "            voc2ind[char] = count\n",
        "            count += 1\n",
        "        list_indexed.append(voc2ind[char])\n",
        "        #pass # TODO Fill this in\n",
        "    print (len(voc2ind.keys()))\n",
        "    \n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "    \n",
        "    data_indexed = np.array(list_indexed)\n",
        "    list_indexed = []\n",
        "    print (data_indexed.shape)\n",
        "    print (data_indexed.shape[0])\n",
        "    \n",
        "    split_index = math.ceil((4 * data_indexed.shape[0]) / 5)\n",
        "    print (split_index)\n",
        "    train_text = data_indexed[:split_index] \n",
        "    # TODO Fill this in\n",
        "    test_text = data_indexed[split_index:]\n",
        "    # TODO Fill this in\n",
        "    \n",
        "    print (train_text.shape, test_text.shape)\n",
        "    \n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_test.pkl', 'wb'))\n",
        "    \n",
        "    return voc2ind\n",
        "    \n",
        "voc2ind = prepare_data(DATA_PATH + 'harry_potter.txt')\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "89\n",
            "(6227357,)\n",
            "6227357\n",
            "4981886\n",
            "(4981886,) (1245471,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kzX1tUv8ilYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3: Loading the data\n",
        "This is possibly the trickiest part of this homework. In the past, batches were not correlated with each other, and the data within a single minibatch was also not correlated, so you could basically draw randomly from the dataset. That is not the case here. Instead, you should return sequences from the dataset.\n",
        "\n",
        "Your instructions are to implement the following. First, imagine splitting the dataset into N chunks where N is the batch_size and the chunks are contiguous parts of the data. For each batch, you should return one sequence from each of the chunks. The batches should also be sequential an example is described below.\n",
        "\n",
        "The data is 20 characters long `[1, 2, 3, ...20]`. The batch size is 2 and the sequence length is 4\n",
        "- The 1st batch should consist of  `(data =  [[1, 2, 3, 4]; [11, 12, 13, 14]], labels = [[2, 3, 4, 5]; [12, 13, 14, 15]])`\n",
        "- The 2nd batch should consist of `(data =  [[5, 6, 7, 8]; [15, 16, 17, 18]], labels = [[6, 7, 8, 9]; [16, 17, 18, 19]])`\n",
        "- The 3rd batch should consist of `(data =  [[9]; [19]], labels = [[10]; [20]])`\n",
        "- There is no 4th batch.\n",
        "\n",
        "Hints:\n",
        "- To work with the rest of the code, your len(dataset) should be a multiple of the batch_size. \n",
        "- Removing the last bit to make the data the proper shape will probably give better results than padding with 0s.\n",
        "- It is OK to have one batch be shorter than the others as long as all entries in that batch are the same length.\n",
        "- Notice that the last label in one batch is the first data in the next batch. Be careful of off-by-one errors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "44v6o0JwiwXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HarryPotterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(HarryPotterDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "        num = dataset['tokens'].shape[0] - (dataset['tokens'].shape[0] % batch_size)\n",
        "        self.dataset = dataset['tokens'][:num]\n",
        "        # TODO: Any preprocessing on the data to get it to the right shape.\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        chunk_size = math.floor(self.dataset.shape[0] / self.batch_size)\n",
        "        num_examples_batch = math.floor(chunk_size / self.sequence_length)\n",
        "        length = self.batch_size * num_examples_batch \n",
        "        return length\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        # raise NotImplementedError\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence as described above.\n",
        "        # The data and labels should be torch long tensors.\n",
        "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
        "        # in and how far down in the chunk you are.\n",
        "        chunk_size = math.floor(self.dataset.shape[0] / self.batch_size)\n",
        "        within_chunk, chunk_idx = divmod(idx, self.batch_size)\n",
        "        new_idx = chunk_idx * chunk_size + within_chunk * self.sequence_length\n",
        "        last_idx = new_idx + self.sequence_length - 1\n",
        "        data = self.dataset[new_idx:last_idx+2]\n",
        "        data = torch.LongTensor(data)\n",
        "        return data[:-1], data[1:]\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kYKDZoj2jCV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Defining the Network\n",
        "This time we will provide a network that should already get pretty good performance. You will still need to write the forward pass and inference functions. You may also choose to modify the network to try and get better performance.\n",
        "\n",
        "__BE CAREFUL:__ We have specified that the data will be fed in as batch_first. Look at the documentation if you are confused about the implications of this as well as how to call it for the forward pass. https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "    \n"
      ]
    },
    {
      "metadata": {
        "id": "mO21UXLj2ixn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 0.5\n",
        "\n",
        "class HarryPotterNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size):\n",
        "        super(HarryPotterNet, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        self.gru = nn.GRU(self.feature_size, self.feature_size, num_layers=2, batch_first=True)\n",
        "        self.decoder = nn.Linear(self.feature_size, self.vocab_size)\n",
        "        \n",
        "        # This shares the encoder and decoder weights as described in lecture.\n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        batch_size = x.shape[0]\n",
        "        sequence_length = x.shape[1]\n",
        "        \n",
        "        # TODO finish defining the forward pass.\n",
        "        # You should return the output from the decoder as well as the hidden state given by the gru.\n",
        "        # raise NotImplementedError\n",
        "        x = x.view(-1,1)\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(batch_size, sequence_length, self.feature_size)\n",
        "        x, hidden_state = self.gru(x, hidden_state)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEQZIoB0jY5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 5: Character Generation\n",
        "\n",
        "In class we discussed three algorithms for creating sequences.\n",
        "1. Max: Choose the most likely value\n",
        "2. Sample: Sample from the distribution output by the network.\n",
        "3. Beam Search: Sample from the distribution and use the Beam Search algorithm.\n",
        "\n",
        "The beam search algorithm is as follows:\n",
        "```\n",
        "1. Initialize the beam list with the single existing empty beam\n",
        "2. Repeat for the sequence length:\n",
        "    1. For each beam in the beam list:\n",
        "        1. Compute the next distribution over the output space for that state\n",
        "        2. Sample from the distribution with replacement\n",
        "        3. For each sample:\n",
        "            1. Compute its score\n",
        "            2. Record its hidden state and chosen value\n",
        "        4. Add all the samples to the new beam list      \n",
        "     2. Rank the new beam list\n",
        "     3. Throw out all but the top N beams\n",
        " 3. Return the top beam's chosen values.\n",
        "```\n",
        "\n",
        "\n",
        "Hints:\n",
        "- np.random.choice and torch.multinomial will both help with the sampling as they can take in a weighted probability distribution and sample from that distribution.\n",
        "- For beam search you will need to keep a running score of the likelihood of each sequence. If you multiply the likelihoods, you will encounter float underflow. Instead, you should add the log likelihoods.\n",
        "- For beam search, you will need to keep track of multiple hidden states related to which branch you are currently expanding.\n",
        "- For beam search, you should search over the beam, but only return the top result in the end.\n",
        "- It may be useful to do the training part before the character generation part so you have some model to test.\n",
        "- Feel free to play around with the `BEAM_WIDTH`.\n"
      ]
    },
    {
      "metadata": {
        "id": "Tim6a6x2K2tz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BEAM_WIDTH = 20\n",
        "\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "\n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden, temperature=1)\n",
        "\n",
        "        outputs = []\n",
        "        # Initializes the beam list\n",
        "        beam = [([], output, hidden, 0)]\n",
        "        for ii in range(sequence_length):\n",
        "            if sampling_strategy == 'max':\n",
        "                # TODO max sampling strategy\n",
        "                # raise NotImplementedError\n",
        "                _, pred_label = torch.max(output, 1)\n",
        "                outputs.append(pred_label)\n",
        "                new_input = pred_label.unsqueeze(1)\n",
        "                output, hidden = model.inference(new_input, hidden, temperature=1)\n",
        "                \n",
        "\n",
        "            elif sampling_strategy == 'sample':\n",
        "                # TODO: Probability-based sampling strategy.\n",
        "                # raise NotImplementedError \n",
        "                pred_label = torch.multinomial(output, 1, True)\n",
        "                outputs.append(pred_label)\n",
        "                new_input = pred_label.unsqueeze(1)\n",
        "                output, hidden = model.inference(new_input, hidden, temperature=1)\n",
        "                \n",
        "\n",
        "            elif sampling_strategy == 'beam':\n",
        "                # Todo: beam search sampling strategy\n",
        "                # raise NotImplementedError\n",
        "                new_beam = []\n",
        "                for be in beam:\n",
        "                    pred_label = torch.multinomial(be[1], 2, True)\n",
        "                    for p in pred_label[0]:\n",
        "                        out1, hid1 = model.inference(p, be[2], temperature=1)\n",
        "                        new_beam.append((be[0]+[p], out1, hid1, be[3]+np.log(be[1][0][p].item())))\n",
        "                new_beam = sorted(new_beam, key=lambda x: x[3], reverse=True)[:beam_width]\n",
        "                beam = new_beam\n",
        "        \n",
        "        if sampling_strategy == 'beam':\n",
        "            outputs = beam[0][0]\n",
        "        \n",
        "        return vocab.array_to_words(seed_words_arr.tolist() + outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Havsk_RJi_i5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 6: Training\n",
        "Again, we are providing training code for you. Have a look at the train function though as it implements the exact forward approximate backward computation, which may be of interest to you. You will still need to add the perplexity computation (read more in part 9 about how to do this)."
      ]
    },
    {
      "metadata": {
        "id": "9FzbkBnGbJ7r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "L0Wq8hRy0UEX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    print (\"enumerate train  \", len(train_loader))\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        pred = output.max(-1)[1]\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        print (\"enumerate test   \", len(test_loader))\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            \n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            test_loss += model.loss(output, label, reduction='elementwise_mean').item()\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            # Comment this out to avoid printing test results\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Input\\t%s\\nGT\\t%s\\npred\\t%s\\n\\n' % (\n",
        "                    train_loader.dataset.vocab.array_to_words(data[0]),\n",
        "                    train_loader.dataset.vocab.array_to_words(label[0]),\n",
        "                    train_loader.dataset.vocab.array_to_words(pred[0])))\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66T-Ylkg0fn1",
        "colab_type": "code",
        "outputId": "b84042fe-7440-4b6a-bb7c-9213ded096f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35292
        }
      },
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 128\n",
        "BATCH_SIZE = 128\n",
        "FEATURE_SIZE = 512\n",
        "TEST_BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.002\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs_1115/log.pkl'\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "data_train = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = HarryPotterNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "# Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "# We will talk more about different optimization methods in class.\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints_1115')\n",
        "\n",
        "train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(model, device, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints_1115/%03d.pt' % epoch)\n",
        "        seed_words = 'Harry Potter'\n",
        "        for ii in range(10):\n",
        "            generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'sample')\n",
        "            print('generated sample\\t', generated_sentence)\n",
        "        generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'beam')\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    model.save_model(DATA_PATH + 'checkpoints_1115/%03d.pt' % epoch, 0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\t//ouuuuggPPPIInmmguuungCIuuu6TD/\\IgouTfu{{ougggmmmmmW/00vIZ'ttmmZZguunn/mmm66m/\\ggguu??uuummguun\\vTuuuuuuu?m{????{mu6P6RDuuPP/DD\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\tmmmmmmDWuDmZBTuuuuu\\DDDggPD(uuI///mmmmDDDmZ/\\ggguummmmmmmmmmmmW/m?b//ggguugn\\.?uuuuumgQun\\\\\\n//ggggg??I\\;P;PPPPuuuuu///ggggggmOg\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tBBuuuuP6mDInm\\\\DBB{mDm{{/gggggg?mmDDuuuuuuZZ\\\\ZIoDssggggIPPDDuuuu?HuuuuuuunmmnnTHuu/\\TTD?Cgggoooo\\Zo\\\\PPTTT////PP/PmmmmDDDgggggh\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\tuuuugmmmmgggg;6PPPPmmmZZuuungggiggv/\\\\\\\\unO66mmmmmDDtg^?PmmmBBuuuuuuu?DDDvvgvvmmmmmm/pp/Zggga???P{{uuu6mDDDDDImmPPm666gggg\\TTTmm\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tZJZZZguuE??Tnm/\\PPPPPPPIgguIPHggigIImu///gggIgggggggggg\\IggZZZZmZPP/F{nn?mmDDPDgPPPP\\\\P6{mm0Zgogggg\\\\uuDT?BBBDD/uub{6?DDDvmv^GGo\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\tP/Pum/\\ZZoooPP/uuuuummmmmTuuuuu/????mm?mZZmCIIInICT\\/\\Zgggmmmmmu{nmmCCuuugggggggvm66nHgIsP///PZZuuCg{{{?{6DDD((/gggg??HBHHgg;ooo\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\tZZmIuu{{nnmDDDDDuuuuuuuDgggggggggggggg/DDIImmmb?/Bm/DPgggHuuuugguuuuuuuug\\TT\\/gggguuI/uuTPm/DZgPuuuggggggggggggggmDDIIuggvggD??D\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tguTnmDImmP//DuT\\DITTBTZZogDDZZZZgguuu{nDDvggD?DDuDDuuuvvummmPIuu?DDDDDDI.ou///ZgggPPP/PmmmDPgguuuCCIZZgugggD{{/gggggZgggvInmmmmm\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 9.0312, Accuracy: 13240/1245184 (1%)\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 0 [0/38912 (0%)]\tLoss: 8.924152\n",
            "Train Epoch: 0 [1280/38912 (3%)]\tLoss: 10.002098\n",
            "Train Epoch: 0 [2560/38912 (7%)]\tLoss: 4.670242\n",
            "Train Epoch: 0 [3840/38912 (10%)]\tLoss: 3.064760\n",
            "Train Epoch: 0 [5120/38912 (13%)]\tLoss: 2.406212\n",
            "Train Epoch: 0 [6400/38912 (16%)]\tLoss: 2.155452\n",
            "Train Epoch: 0 [7680/38912 (20%)]\tLoss: 2.019758\n",
            "Train Epoch: 0 [8960/38912 (23%)]\tLoss: 1.948979\n",
            "Train Epoch: 0 [10240/38912 (26%)]\tLoss: 1.908677\n",
            "Train Epoch: 0 [11520/38912 (30%)]\tLoss: 1.863420\n",
            "Train Epoch: 0 [12800/38912 (33%)]\tLoss: 1.823939\n",
            "Train Epoch: 0 [14080/38912 (36%)]\tLoss: 1.836714\n",
            "Train Epoch: 0 [15360/38912 (39%)]\tLoss: 1.785927\n",
            "Train Epoch: 0 [16640/38912 (43%)]\tLoss: 1.750256\n",
            "Train Epoch: 0 [17920/38912 (46%)]\tLoss: 1.752733\n",
            "Train Epoch: 0 [19200/38912 (49%)]\tLoss: 1.743178\n",
            "Train Epoch: 0 [20480/38912 (53%)]\tLoss: 1.746851\n",
            "Train Epoch: 0 [21760/38912 (56%)]\tLoss: 1.703315\n",
            "Train Epoch: 0 [23040/38912 (59%)]\tLoss: 1.704236\n",
            "Train Epoch: 0 [24320/38912 (62%)]\tLoss: 1.684674\n",
            "Train Epoch: 0 [25600/38912 (66%)]\tLoss: 1.673750\n",
            "Train Epoch: 0 [26880/38912 (69%)]\tLoss: 1.603081\n",
            "Train Epoch: 0 [28160/38912 (72%)]\tLoss: 1.666806\n",
            "Train Epoch: 0 [29440/38912 (76%)]\tLoss: 1.683267\n",
            "Train Epoch: 0 [30720/38912 (79%)]\tLoss: 1.671634\n",
            "Train Epoch: 0 [32000/38912 (82%)]\tLoss: 1.650876\n",
            "Train Epoch: 0 [33280/38912 (86%)]\tLoss: 1.633088\n",
            "Train Epoch: 0 [34560/38912 (89%)]\tLoss: 1.590690\n",
            "Train Epoch: 0 [35840/38912 (92%)]\tLoss: 1.607282\n",
            "Train Epoch: 0 [37120/38912 (95%)]\tLoss: 1.567235\n",
            "Train Epoch: 0 [38400/38912 (99%)]\tLoss: 1.573773\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHreat Hhnhttnd soartd ttainst the sore aaaasl, \"H wn aa re .. \"e said  \"Hob't barki  ahre  said Harry,an tnle  hnditnsllnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . a wan tllartte tp auoh aeck t . . .ub't barki  \" . .  \"H ss aot sarkied  aarry   said Humbledore  aes soice ansattle atraog  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tartnenn aad boeeu  tumbledore af  ooclrce  the  te wai tas hace  aarl aand aaye reahet aaeryan the sodtrnt aakht af t liraamhyye\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\ttlllleaoaasong llll taprione  d aith aoegen   \"Htwhi tou w partte tn t was salleng ho se oewm torpiin t\"\"hetksaald ' s, ahet  aa\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tnee eai  snd Hheughtte haoreesed ansottle  ae waened tia e  an thuein iaf the stltrtenn  aao aoat toaiitest a aueom tt- sO've go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the seght.bnr  \" d tarry  ah k hookid ataad.an Hhe stinl  and tolr ateel d an tde tes aoke a sortrers cetyye  aauiietseng ai\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he seiieiie aare aoatread  \"he soor ah bhe ftontt aorrrsass toet waa ayck tn o the sortle tas aaase   \"he e was aotsi\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tandsaiw st.wad badpened t-arcell nne s aas aot anlleadeng aharmsa ahe   Iu the sokht af the sils  ae wai tumbledore s aaststooin\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.6189, Accuracy: 658785/1245184 (53%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/000.pt\n",
            "\n",
            "generated sample\t Harry Potter,\" said Ron's e's, who had wate leass, Harry?\" \"Jus said Harry, Vrevened to her eyes. The had so stleping unarly goldely. Harried. \"I want to pienly halfoy, who was netarible to like to acke!\" he saw \n",
            "generated sample\t Harry Potter. \"I with a shork -gastle tham as he sair, \"Craces!\" Harry was away anot. It watem?\" \"It we losse to fur of boby we have get as thought -unly found intlien and all andistreeces. Amin't Pan,\" \"Dude int\n",
            "generated sample\t Harry Potter -- Ron was, acrasizza you about appost's take of for a hic he lodfficure embing varying to levea were told the ave, as shestaisating back to day bore a bust -\" \"This?\" \"I In't laught a door deserate,\n",
            "generated sample\t Harry Pottering you, half a though the firembers' side anageas. Hermione laters, and the eass. \"If indowe, a vercy-pantain geatle. \"Is a pace of Magin?\" \"CM!\" Ansto!\" Mrs. Weasley!\" The souldn't be caugh in he sc\n",
            "generated sample\t Harry Potter!\" HeVRemonts anstran gallp so yea, Malfoy, what was no the tankailivall. SNried around!\" Harry shoudly gaspeated. He empy his lifted with briedg, \"You'll rain harry boold, now, Weman at Harry slight.\n",
            "generated sample\t Harry Potter engite all Ernofet,\" said Dumt bed. Woked alraged, he sworgs, and the sweell, his felt table Harry Ginny back in the air. \"Sor get up everyone,' we'll be Horried is havents's ghot sleening and shool.\n",
            "generated sample\t Harry Potterins Dumbledore at the tairing had bage. Two up,\" Harry keet paint. Ge only was puning made to Hermione all tone think. \"'I want turnness banks, legess?\" said Hermione at unasley, and foundly. \"Oh, \"I \n",
            "generated sample\t Harry Potterval,\" Hearm!\" Blacked baccks, very to mak-ly, way chair,\" sat Meging you'll can the learing, as inshaking yould-?\" said Hermioves and lost parmet and int\" back ontounteringly at here.\" He was seemots \n",
            "generated sample\t Harry Potter fromt checipe. \"Weahe would, and leagral is Harry; hadn's evernapelare,\" sarty, Fudge him. \"SE maid Rons but up a Gryffindlise. Harrymione alodes and Gyolled. \"You hand,\" said Grape, he wast,\" thoute\n",
            "generated sample\t Harry Potter,\" Harrid's grant acrarible. Sevetriage, as to everying to be Ha? Pnished wilh a betwel. \"Har streptaining Harry's had alarnely was to be crutteed ups,\" said strall a bite the rootem!\" \"LI've been you\n",
            "generated beam\t\t Harry Potters. \"I'm all about the normer way, an' Malaking the was way?\" but the loow. \"Wropher aine the table asirge the Proffice fullying a a crubbby. Harry swealls. Harry and how actic, Winty, he's, skill hair\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 1 [0/38912 (0%)]\tLoss: 1.626364\n",
            "Train Epoch: 1 [1280/38912 (3%)]\tLoss: 1.540811\n",
            "Train Epoch: 1 [2560/38912 (7%)]\tLoss: 1.551664\n",
            "Train Epoch: 1 [3840/38912 (10%)]\tLoss: 1.616026\n",
            "Train Epoch: 1 [5120/38912 (13%)]\tLoss: 1.613835\n",
            "Train Epoch: 1 [6400/38912 (16%)]\tLoss: 1.588285\n",
            "Train Epoch: 1 [7680/38912 (20%)]\tLoss: 1.554075\n",
            "Train Epoch: 1 [8960/38912 (23%)]\tLoss: 1.525685\n",
            "Train Epoch: 1 [10240/38912 (26%)]\tLoss: 1.521036\n",
            "Train Epoch: 1 [11520/38912 (30%)]\tLoss: 1.502807\n",
            "Train Epoch: 1 [12800/38912 (33%)]\tLoss: 1.484148\n",
            "Train Epoch: 1 [14080/38912 (36%)]\tLoss: 1.537562\n",
            "Train Epoch: 1 [15360/38912 (39%)]\tLoss: 1.498635\n",
            "Train Epoch: 1 [16640/38912 (43%)]\tLoss: 1.477587\n",
            "Train Epoch: 1 [17920/38912 (46%)]\tLoss: 1.492424\n",
            "Train Epoch: 1 [19200/38912 (49%)]\tLoss: 1.499247\n",
            "Train Epoch: 1 [20480/38912 (53%)]\tLoss: 1.498605\n",
            "Train Epoch: 1 [21760/38912 (56%)]\tLoss: 1.503426\n",
            "Train Epoch: 1 [23040/38912 (59%)]\tLoss: 1.527816\n",
            "Train Epoch: 1 [24320/38912 (62%)]\tLoss: 1.498023\n",
            "Train Epoch: 1 [25600/38912 (66%)]\tLoss: 1.480814\n",
            "Train Epoch: 1 [26880/38912 (69%)]\tLoss: 1.420546\n",
            "Train Epoch: 1 [28160/38912 (72%)]\tLoss: 1.487351\n",
            "Train Epoch: 1 [29440/38912 (76%)]\tLoss: 1.501681\n",
            "Train Epoch: 1 [30720/38912 (79%)]\tLoss: 1.485811\n",
            "Train Epoch: 1 [32000/38912 (82%)]\tLoss: 1.482734\n",
            "Train Epoch: 1 [33280/38912 (86%)]\tLoss: 1.468238\n",
            "Train Epoch: 1 [34560/38912 (89%)]\tLoss: 1.452289\n",
            "Train Epoch: 1 [35840/38912 (92%)]\tLoss: 1.555608\n",
            "Train Epoch: 1 [37120/38912 (95%)]\tLoss: 1.485826\n",
            "Train Epoch: 1 [38400/38912 (99%)]\tLoss: 1.451471\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\threat Hunnttnd soaved ttain,t the fome aotasl, \"I wm shrre .. \"e said  \"Ion't barky, whr,\" said Harry,sn tnce  hnditusllnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t .\"a wan'tllarate tn aoot aeck t.. . .ub't warky, \" . .  \"I sm sot sarkyed  \"arry   said Humbledore  hes soice snsottle btoang  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taraneon tad boeoui tumbledore sf  ooclrde  hhe  te hai tis hace  hare aand siye dethat tverytntthe cortrnt tekht af t lioaet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\ttllleewooaseng llnn tlpriosg  d thth aoeggn   \"Ithhi hou wlparate tn t was salleng ho bof owm whrriinl  \"hetksslod ''s, hhet sso\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneedeai  sld sheughthe hhoreeded ttlottle  ai haemed tie ee hn thupon ,tf the stdtrteon  heolcoed toaictett t-tueom tt- bO ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t.an the ceght bnr  \"nd tarry  hhok heokid ataad.an the stinl  hnd soer ateel d tn tde tis.aoke a sorirers cytyye  hauiiessiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he seie iim sare sisprvad  \"he soor ao the ftontl siirrsase shet toa hack tnto the fostle ths soese   \"he e was sotsi\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tt wad badpened t-t  ecl n  es ias sot tnloeddeng hharmsa hhe   wu the sikht bf the fild  wo hai tumbledore s sast tooin\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4925, Accuracy: 698657/1245184 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/001.pt\n",
            "\n",
            "generated sample\t Harry Potter,\" said Dumbledore fush his sockall, whole whozex sawked you who cross she soaled been wall. \"Hofoysleys Nining stone pointed in them to chesk for the most lick. \"SumOp hadged -- hear as Fred well, he\n",
            "generated sample\t Harry Potters of the sholt, looking somethink of the last foowten?\" \"Oh didn't had into as Norry, who was now arounited... . \"All into really reached ....\" Harry and one help of his chagged, just like but standwi\n",
            "generated sample\t Harry Potter. \"Whot was so father I hadn't do?\" Ron heard at the large blowrather in the Dormost Snage, \"You-n' book, who was the ?\" \"Pround strong towards, and Harry holib`y as malack rust filled Roon as garled \n",
            "generated sample\t Harry Potter that you've galves in a line, jusipeed, he hadn't ise, his sound he sholiting too how helt, youbby, of your something off urprying supposed to do worthin?\" She could catta All. A At Gold Wight up of \n",
            "generated sample\t Harry Potterfus. \"Iwhparted them to chack body was stopped up the parchagrow times fiexse....\" \"Mil'll jark a deep it largece Heatily the silent whece hear you thiky his off.\" ONather?\" \"said Hagrid's catch of th\n",
            "generated sample\t Harry Potter, if 'emot; now been Nut Hagrid. \"I'mvea.\" \"Your was leaving and for of the black bepart more?\" \"And this.\" Harry's body elf soped him. 1 EAbrages. \"Gowled his back up stopped umatily that by their du\n",
            "generated sample\t Harry Potters.\" \"Madmayell, foot,\" said Dumbledore cames to see his together. \"I had been going to the door into the comforturs Seectigble again, he shook for the blatie cluttly in the could gasches interents, bo\n",
            "generated sample\t Harry Potter?\" \"Search Paring the downor no, what No, people if all your finant--Wati down.\" He said Hermione yearless, whore your Magic and whold he howeeps. \"I chairce like a long around quites of Sniggped fore\n",
            "generated sample\t Harry Pottering as he could haven't leat te mage had longer table. \"No, all the souble had left chingridge hand carled, it sweempty shutment. \"I just nothing over themrowd Afest for with of Harryquiently think an\n",
            "generated sample\t Harry Potters'se castly line about Comable was nex to feel yeah, vanily had wasn't right run at leaves. \"Hermione. \"CI*'re you?\" said sounna,\" said Ron, looking suddn't have not both a pocketer?\" Harry, who we've\n",
            "generated beam\t\t Harry Potter, Professor than elses full the poidorme. \"Chard byRiliess, hously walft after. He pood the footiedf into them. \"Professor Blug, Iff,\" said in front tow, of yeh've see. Onse lofed, resco-son, but bew.\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 2 [0/38912 (0%)]\tLoss: 1.495818\n",
            "Train Epoch: 2 [1280/38912 (3%)]\tLoss: 1.431090\n",
            "Train Epoch: 2 [2560/38912 (7%)]\tLoss: 1.434030\n",
            "Train Epoch: 2 [3840/38912 (10%)]\tLoss: 1.460371\n",
            "Train Epoch: 2 [5120/38912 (13%)]\tLoss: 1.439497\n",
            "Train Epoch: 2 [6400/38912 (16%)]\tLoss: 1.438599\n",
            "Train Epoch: 2 [7680/38912 (20%)]\tLoss: 1.436751\n",
            "Train Epoch: 2 [8960/38912 (23%)]\tLoss: 1.408996\n",
            "Train Epoch: 2 [10240/38912 (26%)]\tLoss: 1.423869\n",
            "Train Epoch: 2 [11520/38912 (30%)]\tLoss: 1.406053\n",
            "Train Epoch: 2 [12800/38912 (33%)]\tLoss: 1.408756\n",
            "Train Epoch: 2 [14080/38912 (36%)]\tLoss: 1.457113\n",
            "Train Epoch: 2 [15360/38912 (39%)]\tLoss: 1.428267\n",
            "Train Epoch: 2 [16640/38912 (43%)]\tLoss: 1.398507\n",
            "Train Epoch: 2 [17920/38912 (46%)]\tLoss: 1.413922\n",
            "Train Epoch: 2 [19200/38912 (49%)]\tLoss: 1.406290\n",
            "Train Epoch: 2 [20480/38912 (53%)]\tLoss: 1.412106\n",
            "Train Epoch: 2 [21760/38912 (56%)]\tLoss: 1.434041\n",
            "Train Epoch: 2 [23040/38912 (59%)]\tLoss: 1.443019\n",
            "Train Epoch: 2 [24320/38912 (62%)]\tLoss: 1.408637\n",
            "Train Epoch: 2 [25600/38912 (66%)]\tLoss: 1.411479\n",
            "Train Epoch: 2 [26880/38912 (69%)]\tLoss: 1.352319\n",
            "Train Epoch: 2 [28160/38912 (72%)]\tLoss: 1.414864\n",
            "Train Epoch: 2 [29440/38912 (76%)]\tLoss: 1.422219\n",
            "Train Epoch: 2 [30720/38912 (79%)]\tLoss: 1.419813\n",
            "Train Epoch: 2 [32000/38912 (82%)]\tLoss: 1.420445\n",
            "Train Epoch: 2 [33280/38912 (86%)]\tLoss: 1.424803\n",
            "Train Epoch: 2 [34560/38912 (89%)]\tLoss: 1.404620\n",
            "Train Epoch: 2 [35840/38912 (92%)]\tLoss: 1.419405\n",
            "Train Epoch: 2 [37120/38912 (95%)]\tLoss: 1.380562\n",
            "Train Epoch: 2 [38400/38912 (99%)]\tLoss: 1.384473\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHraat Hulnttnd soaved ttain.t the Dame aotask, HH tm aarri\".. \"e said  \"Ion't larky \"yor,\" said Harry.sn tnee  tndiousllnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t .\"I wan tlparate tn aoot aeck t.. . .ub't larky \"\" . .  \"I sm not tarkyed  \"arry   said Humbledore  wis soice snsattle biaang  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon oad boeoun tumbledore sf  ooclrde  the  te wai tis hace  aare aand siye dethet tverysn the cartrnt takht af t luraet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\ttlhne oooaseng lenn tlpraose  d whth aeegon   \"H thi tou wnparate tn t was salleng ho baf oom whrriinl  \"het swaod 'ss  bhet sto\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneedaai  sld sheughtte haoreeded ttlottle  ai saemed tie ee an thupon eaf the sidtrleon  waolaeed toaikfert o-wueom tt- bO ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght bnr  Hnd tarry  whok aeokid atead an the biinl  and salretieel d tn ide tis aoke a seriuers cetbyed aauilessiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  Hhe seie ie  ware teaprted  \"he soor wo the ftedtl siirrsase thet toa hack tn o the fastle tas saase   \"he e was aotsi\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand til tn wad badpened t-t  erlen  es ias sot tnlreddeng hharm a whe   wu the sakht if the filk  wa wai tumbledore s sand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4324, Accuracy: 716748/1245184 (58%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/002.pt\n",
            "\n",
            "generated sample\t Harry Potterfieldew as spowed squ-longhot be soungestitions, made them Ron in' a receiven, who was long at Karkarany with his four was might to help, Harry close at their face from Georgee. \"Harry.\" \"Doy and the \n",
            "generated sample\t Harry Potter front girl; hap wenternon thiskly, lookoping all loom again. Was been validerables, face held'll gage in this... now spechodieved hispely, from the giard fooced vanside when the ocposible death took \n",
            "generated sample\t Harry Potter. Mealing yearh othes? Snakes Brack silencely rood looking as shing she called them. She probelies and spriselife he was looking back and Ginny and busilitys have awpered, Harry forced the midstly and\n",
            "generated sample\t Harry Potter. Rooken Valking over his nature mount and looking wrong. \"We we got Dubbbors? Harry flashed skrees. \"Ron' Harry, Ragan!\" Dumbledore meanging when the mand, without his feetless. Potterring the deor-e\n",
            "generated sample\t Harry Pottermione from their carded was since looking nearly prounded Gike and shipped. \"He wasn't fourges.\" \"You're can't enougly is undertantly,\" said Sriming. \"The remembering, looked out Krinkward, looked out\n",
            "generated sample\t Harry Potters so, needs can had Harry swouldend right, threeD What abse teaming (rie to-den winge else Snape wizidding both fallight was quitelwards =onks to go to wort?\" said Not 6urned that just boy about. \"NFw\n",
            "generated sample\t Harry Potter, brish to know Snape to Malfoy hurried waited to her feet and mater is vaused, Lupice slippred in a blabrely about Harry and Ron looked up into dixceure callered at the coars down at least years, but\n",
            "generated sample\t Harry Potters gobing dreaged toward the ruf. His tends hurtied secret D.A. He water?\" said Hermione, juvise started, they could need all the Nxide Malafoy Malfoy wanty to leaving at all they was all though off th\n",
            "generated sample\t Harry Potter.\"1 his knew warked the clas booking.... his brooks? Snape and half way stoping himself, hid yought to list, a sway of his housely, falley and right inBcleeding throwing mat; Harry happy, but if it a \n",
            "generated sample\t Harry Potter, hower take all. \"I've got things,? Harry stopped up they wailed. 'PHom, didn't you with Male neared a door, \"'m retried. Itwick Bketter, Harry wondected?\" \"Yee},\" said Dumbledore. \"Book a large answ\n",
            "generated beam\t\t Harry Potter,\" said Harry chim, all silence Volleong, thoughthere Wooth-?\" Harry Lupin door spelettless hand Uzrimself that look for the school dreamily handring threse a task to the room. \"Now yeah!\" said Hagrid\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 3 [0/38912 (0%)]\tLoss: 1.438477\n",
            "Train Epoch: 3 [1280/38912 (3%)]\tLoss: 1.366377\n",
            "Train Epoch: 3 [2560/38912 (7%)]\tLoss: 1.387028\n",
            "Train Epoch: 3 [3840/38912 (10%)]\tLoss: 1.430870\n",
            "Train Epoch: 3 [5120/38912 (13%)]\tLoss: 1.391744\n",
            "Train Epoch: 3 [6400/38912 (16%)]\tLoss: 1.389436\n",
            "Train Epoch: 3 [7680/38912 (20%)]\tLoss: 1.379227\n",
            "Train Epoch: 3 [8960/38912 (23%)]\tLoss: 1.363567\n",
            "Train Epoch: 3 [10240/38912 (26%)]\tLoss: 1.389082\n",
            "Train Epoch: 3 [11520/38912 (30%)]\tLoss: 1.362375\n",
            "Train Epoch: 3 [12800/38912 (33%)]\tLoss: 1.348526\n",
            "Train Epoch: 3 [14080/38912 (36%)]\tLoss: 1.401378\n",
            "Train Epoch: 3 [15360/38912 (39%)]\tLoss: 1.376058\n",
            "Train Epoch: 3 [16640/38912 (43%)]\tLoss: 1.359457\n",
            "Train Epoch: 3 [17920/38912 (46%)]\tLoss: 1.363595\n",
            "Train Epoch: 3 [19200/38912 (49%)]\tLoss: 1.357049\n",
            "Train Epoch: 3 [20480/38912 (53%)]\tLoss: 1.368255\n",
            "Train Epoch: 3 [21760/38912 (56%)]\tLoss: 1.368562\n",
            "Train Epoch: 3 [23040/38912 (59%)]\tLoss: 1.395168\n",
            "Train Epoch: 3 [24320/38912 (62%)]\tLoss: 1.363621\n",
            "Train Epoch: 3 [25600/38912 (66%)]\tLoss: 1.358200\n",
            "Train Epoch: 3 [26880/38912 (69%)]\tLoss: 1.317002\n",
            "Train Epoch: 3 [28160/38912 (72%)]\tLoss: 1.388052\n",
            "Train Epoch: 3 [29440/38912 (76%)]\tLoss: 1.393776\n",
            "Train Epoch: 3 [30720/38912 (79%)]\tLoss: 1.384338\n",
            "Train Epoch: 3 [32000/38912 (82%)]\tLoss: 1.392423\n",
            "Train Epoch: 3 [33280/38912 (86%)]\tLoss: 1.392073\n",
            "Train Epoch: 3 [34560/38912 (89%)]\tLoss: 1.356819\n",
            "Train Epoch: 3 [35840/38912 (92%)]\tLoss: 1.380444\n",
            "Train Epoch: 3 [37120/38912 (95%)]\tLoss: 1.354863\n",
            "Train Epoch: 3 [38400/38912 (99%)]\tLoss: 1.352770\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\thraat Hulnttnd soaved tnain.t the tase aotask  \"Ittm sirri\".. \"e said  \"Ion't larry  yot,  said Harry sn tnce  tndiousllnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t .\"I wan tlparate tn teot aeck t.. . .ub't larry  I . .  \"I sm sot torryed  aarry   said Humbledore  \"es hoice snsottle siueng  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon oad boeoun tumbledore tf  iocldde  the  te wai tis hace  aaie atnd sorp detoet tverytn the cortrnt takht af t cuuaet yse\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\ttlhne toosseng llnn tapraosened tith aeegon   \"Htthi tou wnparate tn t tos aalling he bef oom warlainl  \"hetkswood 'ss  thet  to\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneetaai  snd sheughtte haorgeded ttlottle  tirhaemed tie ee an taumon elf the tiltaleon  aeolgoed toaisfertet-tuoom tt- bO ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght bnr  \"nd tarry  whok aookid atead an the tiill  tnd solretieel d an ide tis aoke a seriueus cerbled aauiledsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he woie ie  tere teaprted  \"he woor wo the ctedtl siuirsase toet toa hack tn o the castle tis saased  \"he e was aotsi\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand ail tt wad bappened t-tn ealen aes his aot tnloed eng aharm a whe   wu the titht if the tilk  wo hai tumbledore s oand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4022, Accuracy: 726834/1245184 (58%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/003.pt\n",
            "\n",
            "generated sample\t Harry Potter. It elved back trainly slightly an usuall: chan' he looked up at talk out his office. \"Noor Dub-bout,\" said tormlish toom, while Cruckuilial Lying fault grounlsish of the ideage. \"Is mouth, canside, \n",
            "generated sample\t Harry Potterm. Potter, \"I said, then, looking anoh?\" \"Son't forward Ron, we'clapen?\" \"No... little for, however you invelinally,\" said Ron, fainings, overheard his way. \"That's Good ruined, like this catch, in th\n",
            "generated sample\t Harry Potter, Harry beliemem.\" \"Whis is her?\" The Malfoy in itworped. \"Lokharts would be used to fis/ any time -I Snape made a voice, bench wondenith Iparimmed the jovebell rateligantily hearm. \"A think are,\" \"Do\n",
            "generated sample\t Harry Potter; Hermione him he pinky blamed perhaps of to with Dumbledlowering on, bodys' because every foot, \"lickering in the office with to resement to %ust four our offices of the communed its when were still \n",
            "generated sample\t Harry Potter. Dumbledore tanking normitoute, Hall, now exactly, whose could read that Ron's was a been slapped office. Harry birthday overitement, but Harry of the Firecally glit the front of Fudden's feet bese N\n",
            "generated sample\t Harry Pottere her sift that try astand to one, \"Ho, who has talkain the one sitast. Wish, all and foring of the Dorbs, Harry had been getting been thechily getely would by tave to do his BountXon, what was silenc\n",
            "generated sample\t Harry Potter they permited curted. \"How I told too mutteers to re too look, I am not teacher is,\" he meatter with select, Harry squeaked, she in the broom enough the others. \"If no don't I passed angent!\" jast wh\n",
            "generated sample\t Harry Potter, Harry's good toborate to keep harmed. \"Tky could he? Mr, allowing to it, Hagrid''s ran it,\" said Hermione. \"No, whose dear, Malfoy, and not's jump, unally all approachs,\" she said never and strettyg\n",
            "generated sample\t Harry Potter, and they would hair best waving, he said, \"Down today here's no beside turned Dumbledore looks in the common room...\" said Hermione. \"AllU-aric Dalfoy!\" \"Ron, sportiness,\" said Harry tail; a her lef\n",
            "generated sample\t Harry Potter, the told the horrible, how muttahole. Rightering how very like Mmons in the Gringon tones had far brish his heater. 'What is of parraction? He shouldProphet to and sweep. Harry overed by the sembtra\n",
            "generated beam\t\t Harry Potter talk by Pretend of the others, little appeares. \"Bow, I know, whave soundedles? I\" The glass fach an eart. \"Dear.\" \"Moy's goodnets,\" said Harry. \"Yho woull not comunder.\" \"Are your gotfin? Hecognemen\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 4 [0/38912 (0%)]\tLoss: 1.415277\n",
            "Train Epoch: 4 [1280/38912 (3%)]\tLoss: 1.335331\n",
            "Train Epoch: 4 [2560/38912 (7%)]\tLoss: 1.355764\n",
            "Train Epoch: 4 [3840/38912 (10%)]\tLoss: 1.392004\n",
            "Train Epoch: 4 [5120/38912 (13%)]\tLoss: 1.357536\n",
            "Train Epoch: 4 [6400/38912 (16%)]\tLoss: 1.351553\n",
            "Train Epoch: 4 [7680/38912 (20%)]\tLoss: 1.353930\n",
            "Train Epoch: 4 [8960/38912 (23%)]\tLoss: 1.349310\n",
            "Train Epoch: 4 [10240/38912 (26%)]\tLoss: 1.370438\n",
            "Train Epoch: 4 [11520/38912 (30%)]\tLoss: 1.341375\n",
            "Train Epoch: 4 [12800/38912 (33%)]\tLoss: 1.327446\n",
            "Train Epoch: 4 [14080/38912 (36%)]\tLoss: 1.378330\n",
            "Train Epoch: 4 [15360/38912 (39%)]\tLoss: 1.353727\n",
            "Train Epoch: 4 [16640/38912 (43%)]\tLoss: 1.333927\n",
            "Train Epoch: 4 [17920/38912 (46%)]\tLoss: 1.379907\n",
            "Train Epoch: 4 [19200/38912 (49%)]\tLoss: 1.349494\n",
            "Train Epoch: 4 [20480/38912 (53%)]\tLoss: 1.358132\n",
            "Train Epoch: 4 [21760/38912 (56%)]\tLoss: 1.343538\n",
            "Train Epoch: 4 [23040/38912 (59%)]\tLoss: 1.352445\n",
            "Train Epoch: 4 [24320/38912 (62%)]\tLoss: 1.335995\n",
            "Train Epoch: 4 [25600/38912 (66%)]\tLoss: 1.345541\n",
            "Train Epoch: 4 [26880/38912 (69%)]\tLoss: 1.302657\n",
            "Train Epoch: 4 [28160/38912 (72%)]\tLoss: 1.363376\n",
            "Train Epoch: 4 [29440/38912 (76%)]\tLoss: 1.371735\n",
            "Train Epoch: 4 [30720/38912 (79%)]\tLoss: 1.363621\n",
            "Train Epoch: 4 [32000/38912 (82%)]\tLoss: 1.375362\n",
            "Train Epoch: 4 [33280/38912 (86%)]\tLoss: 1.354908\n",
            "Train Epoch: 4 [34560/38912 (89%)]\tLoss: 1.328721\n",
            "Train Epoch: 4 [35840/38912 (92%)]\tLoss: 1.351562\n",
            "Train Epoch: 4 [37120/38912 (95%)]\tLoss: 1.330904\n",
            "Train Epoch: 4 [38400/38912 (99%)]\tLoss: 1.341685\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\thraat sulnttnd soavid tnain t the tase aotisk  \"Ittm silri .  \"e said  \"Ion't larry  yot,  said Harry sn tnce  hndiousllnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan tlparate tp toyh aack t.. . .ub't larky  I . .  \"I sm sot aorryed  Iarry   said Humbledore  \"is hoice snsottle siieng  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration tad boeoun tumbledore tf  cocldde  the  te hai tis hace  aale atnd soyp dethet tverysn the cortrnt tikht af t hhoaet y  \n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\tthhne tooss ng  lnn tapraose  d aith aoegon   \"H thi tou hnparate tn t hos aalling ty sa  oom warlain   Hhetkswood 'ss  hhet sto\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneetaai  snd sheughthe haorgeded ttlottle  ti haemed tie e  hn taumon elf the ctdtalion  aeolcoed toais ert t-wueom tt- \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght bnr  \"nd harry  whok aookid atead an the ttill  hnd solr toeel d hn ide tis aike a ser ueus herbled hauiledsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soi  ia  tire teaprted  \"he woor wo the ctettl ttiirsase toet toa hack tn o the castle tis aaase   \"he e was ao su\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand ail tt wad badpened h-h  eal n   s his aot snloea eng wharm a whe   wu the mitht if the cilk  wo hai tumbledore s oand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3956, Accuracy: 730818/1245184 (59%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/004.pt\n",
            "\n",
            "generated sample\t Harry Potter. He hadn't been to be growin their first place. That had bouse help much a jive sign shick into Harry could hell and looked as though and his face scloud him front. \"He whishes is Harry'C no down ble\n",
            "generated sample\t Harry Potterm, another wand and prophed her middle of his name and that thele stricked ux away for a moment gray into the crien zight, it was looking lipked out to the inder on the room the black lork. The possit\n",
            "generated sample\t Harry Potterry exfempred her wind from he's, heard from the firs. Gony and set only, but that vould can could have more support him by the short -noten idea the open the flave Snape, \" Harry Halfoy's muttered bla\n",
            "generated sample\t Harry Potter, who can he had gone toward the fire in the moment. 'hools all because he can to pre Witch talking ....\" \"Aft, Potter,\" said Hermione? but Bety and Hermione, he was as he looking at Ron looked Harry \n",
            "generated sample\t Harry Potter, who would libe he had not really in the caultan's genorganZ and loosed to be all a food. Ron and chuted to be like the wizardroom holding to take the chiell. \"It's a tip on a hear lukguiston the sta\n",
            "generated sample\t Harry Potter, \"I said sort doing Goory with doubble had says?\" said Snable twitching again Harry and shouldage bys, she had move shouldered and a hang back ofw Lorddan please black buddley in the bottom was holdi\n",
            "generated sample\t Harry Potterk in the footst. Professong Dasy two Flitch mach and then silver the hostless by his fight in the queepily scarry to to she was chapioned hit hid glass of ear, toward Gryffid's half af parchand fist e\n",
            "generated sample\t Harry Potter, but I could hang chanced by the turn an a Weasley leate in his dest. \"Good you if to hear only goben to go in permitor, hard little in why have done he's it just neast in the taps up the seft that, \n",
            "generated sample\t Harry Potter because Harry hatKwets the corridor. \"Practunative his minus, \" Dudle like Dumbledore this for and practicily anstroucally been that Fribmem, hower in the witch bubing hit could handstroom with under\n",
            "generated sample\t Harry Potter Rot. 'We beard, whole had been miss bire.\" =he called out a feet bod. He didn't growsming glowers, Weasley thought out open, that they way for them to pict a tip papparetwent down time, A put it nodd\n",
            "generated beam\t\t Harry Potter, yethering wizard so ideage on a breat-heaged last tim him at him. He sumpered it in a reach over the air and teaching up and a few rememes and put. Malxanid for classing thin at Croom, that think it\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 5 [0/38912 (0%)]\tLoss: 1.411194\n",
            "Train Epoch: 5 [1280/38912 (3%)]\tLoss: 1.331410\n",
            "Train Epoch: 5 [2560/38912 (7%)]\tLoss: 1.342194\n",
            "Train Epoch: 5 [3840/38912 (10%)]\tLoss: 1.385594\n",
            "Train Epoch: 5 [5120/38912 (13%)]\tLoss: 1.343860\n",
            "Train Epoch: 5 [6400/38912 (16%)]\tLoss: 1.331320\n",
            "Train Epoch: 5 [7680/38912 (20%)]\tLoss: 1.338681\n",
            "Train Epoch: 5 [8960/38912 (23%)]\tLoss: 1.332442\n",
            "Train Epoch: 5 [10240/38912 (26%)]\tLoss: 1.346055\n",
            "Train Epoch: 5 [11520/38912 (30%)]\tLoss: 1.326310\n",
            "Train Epoch: 5 [12800/38912 (33%)]\tLoss: 1.313690\n",
            "Train Epoch: 5 [14080/38912 (36%)]\tLoss: 1.366396\n",
            "Train Epoch: 5 [15360/38912 (39%)]\tLoss: 1.336474\n",
            "Train Epoch: 5 [16640/38912 (43%)]\tLoss: 1.312018\n",
            "Train Epoch: 5 [17920/38912 (46%)]\tLoss: 1.336489\n",
            "Train Epoch: 5 [19200/38912 (49%)]\tLoss: 1.337357\n",
            "Train Epoch: 5 [20480/38912 (53%)]\tLoss: 1.348580\n",
            "Train Epoch: 5 [21760/38912 (56%)]\tLoss: 1.329596\n",
            "Train Epoch: 5 [23040/38912 (59%)]\tLoss: 1.349169\n",
            "Train Epoch: 5 [24320/38912 (62%)]\tLoss: 1.328877\n",
            "Train Epoch: 5 [25600/38912 (66%)]\tLoss: 1.332166\n",
            "Train Epoch: 5 [26880/38912 (69%)]\tLoss: 1.295020\n",
            "Train Epoch: 5 [28160/38912 (72%)]\tLoss: 1.357217\n",
            "Train Epoch: 5 [29440/38912 (76%)]\tLoss: 1.361978\n",
            "Train Epoch: 5 [30720/38912 (79%)]\tLoss: 1.354114\n",
            "Train Epoch: 5 [32000/38912 (82%)]\tLoss: 1.379854\n",
            "Train Epoch: 5 [33280/38912 (86%)]\tLoss: 1.360726\n",
            "Train Epoch: 5 [34560/38912 (89%)]\tLoss: 1.325100\n",
            "Train Epoch: 5 [35840/38912 (92%)]\tLoss: 1.347338\n",
            "Train Epoch: 5 [37120/38912 (95%)]\tLoss: 1.313731\n",
            "Train Epoch: 5 [38400/38912 (99%)]\tLoss: 1.327229\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\thraat sulnttnd soavid atain.t the sase aaaasl  \"I hm ahrri\".  \"e said  \"Ion't larky  aot,  said Harry,as tnce  hndiousllnout hum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan tlparate tp aeyh aeck t.. . .ub't larky \"I . .  \"I sm aot aarkyed  Iarry,\" said Humbledore  aes hoice sssottle siaeng  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon aad boeoun aumbledore wf  cocldde  hhe  se hai ais hace  aase itnd soyp dethet averysn the cortrnt cakht af h lhraet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhne soess ng  lnn tapraosed d aith aeegon   \"H thi tou hnparate tn t has arlling ae saa oom wariain   Hhetkswood 'ss  ahet sao\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneetaai  sld aheughthe haorgeded atlottle  ti haemed tie e  hn aanmon eaf the stdtation  aaolceed toyisfert t-wueumsta- \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an ahe ceght bnr  \" d harry  hh k aookid atead an the still  and solr aoeel d an ide tis aike a ser ueus hesbled aauiledsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he seie ia  tare seaprted  \"he woor wh hhe stettl ttiirsase thet toa hack tn o the castle tas ahase   \"he e was ao su\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand aiw tt wad badpened h h  ealen   s aas aot sllrea eng aaarm a hhe   hu the mitht if the walk  wo hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3772, Accuracy: 734082/1245184 (59%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/005.pt\n",
            "\n",
            "generated sample\t Harry Potter, only pullin it out holdink xames gobl-\" \"Onape!\" he realized his eached down up. \"Anything?\" said Harry's horspratized her free witched, while Ere& and Hald and silence that the door with qumilia sh\n",
            "generated sample\t Harry Potter. I would feel so chould hang imepabant basinigs hair. Fred up, its sligently, lawn calling when the top of the wailding Harry and Mumbled qhey's hear the ran by the firest little hidd, a looks next t\n",
            "generated sample\t Harry Potter, and was gainly, appparating that Centurned at the Slytherin and Peturn acroducig) had any prentmas had been water near, Ron's silence Who had soulden ignle was a hand. \"what's having have drownined \n",
            "generated sample\t Harry Potter. Ron's headves on the hogwort, there waskoning your name. \"We'pse happening to Stuck let us you need haffle.\" Hermione had unvisted, then very darkly shortly there visible. \"Heally when as. Oh you wa\n",
            "generated sample\t Harry Potter, what alreading clumbed him at a mank, here aid he had door like your when Harry had given the offformed by the voice foot. Havini felk, greaged and twith no corrying large else gave Hall. Mrsumake i\n",
            "generated sample\t Harry Potter ears and the reaff, he said. \"See, waying at the will know it that after you dangle on, who alwortings at all likay,\" Sheezed Eho jason. \"No, hold the Dumbles was. Syith Anape your got poss you hasn'\n",
            "generated sample\t Harry Potterfore, backins it was. \"Oh, 'was, you haven't laughled about in here for thinkings'. WinMy,\" Thee with on her side of the could hould. Harry, everyone lesssong his table. Ir Giss hoard out of the hogsm\n",
            "generated sample\t Harry Potter, it all, red halfle)s\" \"It'd shaking more and itfould tank from the hoplita of the pass,\" he arrocked, \"velled maxing,\" said pupilly. \"he had than should ran a rago, discaliashing lesson dormblioned \n",
            "generated sample\t Harry Potterer's heary.\" \"Umoloking,\" Harry said flashed at her work, and looked as who halp held voice els ahead, like a member ill nearby beb. He had gone evelusina *Doby said Harry. Professor, NeJtlingly, and \n",
            "generated sample\t Harry Potter Wash one horwart; the Parst still was. Hishink in one and led his clabber achair to re-each, but moved across the with pack. Nm. so who has face an ifle hand sharply else. She sound acrosite wait now\n",
            "generated beam\t\t Harry Potteres skying three was. \"O,\" said Hermione. Ginny half looking at Wrealy and said her =arch, a waving. \"It is no, anxionally wrong will -!\" \"If she just a shop in this much,\" said Slike breeted at chield\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 6 [0/38912 (0%)]\tLoss: 1.397247\n",
            "Train Epoch: 6 [1280/38912 (3%)]\tLoss: 1.317032\n",
            "Train Epoch: 6 [2560/38912 (7%)]\tLoss: 1.334275\n",
            "Train Epoch: 6 [3840/38912 (10%)]\tLoss: 1.372688\n",
            "Train Epoch: 6 [5120/38912 (13%)]\tLoss: 1.331017\n",
            "Train Epoch: 6 [6400/38912 (16%)]\tLoss: 1.313871\n",
            "Train Epoch: 6 [7680/38912 (20%)]\tLoss: 1.328385\n",
            "Train Epoch: 6 [8960/38912 (23%)]\tLoss: 1.321418\n",
            "Train Epoch: 6 [10240/38912 (26%)]\tLoss: 1.340951\n",
            "Train Epoch: 6 [11520/38912 (30%)]\tLoss: 1.311773\n",
            "Train Epoch: 6 [12800/38912 (33%)]\tLoss: 1.306809\n",
            "Train Epoch: 6 [14080/38912 (36%)]\tLoss: 1.363924\n",
            "Train Epoch: 6 [15360/38912 (39%)]\tLoss: 1.317385\n",
            "Train Epoch: 6 [16640/38912 (43%)]\tLoss: 1.297844\n",
            "Train Epoch: 6 [17920/38912 (46%)]\tLoss: 1.318963\n",
            "Train Epoch: 6 [19200/38912 (49%)]\tLoss: 1.318485\n",
            "Train Epoch: 6 [20480/38912 (53%)]\tLoss: 1.338856\n",
            "Train Epoch: 6 [21760/38912 (56%)]\tLoss: 1.322215\n",
            "Train Epoch: 6 [23040/38912 (59%)]\tLoss: 1.332889\n",
            "Train Epoch: 6 [24320/38912 (62%)]\tLoss: 1.312901\n",
            "Train Epoch: 6 [25600/38912 (66%)]\tLoss: 1.325123\n",
            "Train Epoch: 6 [26880/38912 (69%)]\tLoss: 1.300086\n",
            "Train Epoch: 6 [28160/38912 (72%)]\tLoss: 1.349883\n",
            "Train Epoch: 6 [29440/38912 (76%)]\tLoss: 1.353279\n",
            "Train Epoch: 6 [30720/38912 (79%)]\tLoss: 1.350527\n",
            "Train Epoch: 6 [32000/38912 (82%)]\tLoss: 1.353706\n",
            "Train Epoch: 6 [33280/38912 (86%)]\tLoss: 1.343128\n",
            "Train Epoch: 6 [34560/38912 (89%)]\tLoss: 1.310097\n",
            "Train Epoch: 6 [35840/38912 (92%)]\tLoss: 1.330524\n",
            "Train Epoch: 6 [37120/38912 (95%)]\tLoss: 1.306157\n",
            "Train Epoch: 6 [38400/38912 (99%)]\tLoss: 1.322164\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\thraat sulntond soavid atain.t the sase aaaasl. \"Ithm ae ri .  \"e said  \"Ion't larky  yat,  said Harry,ss Hnce  andiousllnout hum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan alparate tp aayh aeck t.. . .ub't larky  I . .  \"I hm aot aarkyed  Iarry,  said Humbledore  ses hoice sssottle streng  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon aad boeoun humbledore sf  cocldde  hhe  se hai ais hace  aase itnd sorpldeahet averysn the costrnt cakht af h luraet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhne soass ng  lln taprawsed d aith aoagon   \"H hhi aou hnparate tn t aas arlling ae saa oom wariain   Hhetksaood 'ss  ahet sao\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tn etaai  sld aheugh he haorgeded atlottle  te haemed tia e  hn aanmon eaf the stdtation  aaolceed toyis ert t-wueum ta  \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an ahe seght bnr  \" d harry  hhok aookid atead an the still  and solr aoeelid an ide tis aoke a ler leus aesbled aauiledsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soie ra  tare seaprted  \"he soor wh hhe stettl ttuirsase thet toa hack tn o the sastle tas aaase   \"he e was aotsu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand aiw hn wad badpened h a  ealen   s aas aot s lrea eng aaarm a hhe   hu the mitht hf the salk  wo hai aumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3699, Accuracy: 735345/1245184 (59%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/006.pt\n",
            "\n",
            "generated sample\t Harry Potter in a chalp, and cold syear Fred enter Petunakely easti natroll unleas, a ralfing on Harry. 'I this hat?\" \"What is Unboward Malfoy's that?\" as hoping The entermati of his bods, cleared a Malchy stream\n",
            "generated sample\t Harry Potters '. The house our seaths of Ponts of the call way dress a stipling to be -\" \"Poble though-Oh Friles Vioness amear. Sirwarit! he cannon, Ream, Prophets about here knowy.\" he said not said his ying sub\n",
            "generated sample\t Harry Potterpointment ring as the Samescantles cale out of his wants. But Hagrid have said, \"Yes, we've breaked, everyone saw graying, he'd reaching who had heading to have a trouble, evile live and givally haust\n",
            "generated sample\t Harry Potters or pretty. The Halls had arrised him and baug`reds he had been glade a house of bedward the same commanistory. Ar. I had Hermione had been out of he had seen the pullenity how talking bascase, point\n",
            "generated sample\t Harry Potter's cheeps off four parents. Harry stufded back Professor Warkly dightning at Homuses before the only blawn in Wardes swyon's biggesting another in whizh shot up in laise in secondely, he villed it ins\n",
            "generated sample\t Harry Potter, mish hate, so that had given again under and a moment way out a compy black up to a break. \"You haven't be I just a place-memel you could helving yather to youe Iust have the room. Skeetermione, we \n",
            "generated sample\t Harry Potter. Fow a ratue and held sport. \"May!\" Harry said, a nearbly as sitting and from chose last pale right back and was his back cowite them. He told her monds now jurried as bath at the summer and last int\n",
            "generated sample\t Harry Potter. Must mame acreamed viously as shop that Hermione ledgigno? a croud weekle. The liness with a spity ring in all the only side of tonic his seating grummed gave. Hermione made his listen fool to the m\n",
            "generated sample\t Harry Potter, Sayin' he was spelling furn to making her compantail was a having night. There was a lorm - justing mmard was in his backe; but all her people who soulder looking bord the first to they. very silent\n",
            "generated sample\t Harry Potter lave a curs last in a liard, who had jounged by a moushe, was the mall last D except a parent of Hermione, had made a little almort had along it just escame to hiss hear6 his back -\" \"said Havice sai\n",
            "generated beam\t\t Harry Potter. \"We is not? Is he could help -'re listening you, Harry and said he said swavent,\" said Harry's humping, downy at hit off, his plash of his hands with Ron full of shakes at Mr. McGunagall pulling gue\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 7 [0/38912 (0%)]\tLoss: 1.390786\n",
            "Train Epoch: 7 [1280/38912 (3%)]\tLoss: 1.310058\n",
            "Train Epoch: 7 [2560/38912 (7%)]\tLoss: 1.328535\n",
            "Train Epoch: 7 [3840/38912 (10%)]\tLoss: 1.366707\n",
            "Train Epoch: 7 [5120/38912 (13%)]\tLoss: 1.324373\n",
            "Train Epoch: 7 [6400/38912 (16%)]\tLoss: 1.302201\n",
            "Train Epoch: 7 [7680/38912 (20%)]\tLoss: 1.319234\n",
            "Train Epoch: 7 [8960/38912 (23%)]\tLoss: 1.314387\n",
            "Train Epoch: 7 [10240/38912 (26%)]\tLoss: 1.328787\n",
            "Train Epoch: 7 [11520/38912 (30%)]\tLoss: 1.298790\n",
            "Train Epoch: 7 [12800/38912 (33%)]\tLoss: 1.297372\n",
            "Train Epoch: 7 [14080/38912 (36%)]\tLoss: 1.352410\n",
            "Train Epoch: 7 [15360/38912 (39%)]\tLoss: 1.308946\n",
            "Train Epoch: 7 [16640/38912 (43%)]\tLoss: 1.289792\n",
            "Train Epoch: 7 [17920/38912 (46%)]\tLoss: 1.310322\n",
            "Train Epoch: 7 [19200/38912 (49%)]\tLoss: 1.310557\n",
            "Train Epoch: 7 [20480/38912 (53%)]\tLoss: 1.333605\n",
            "Train Epoch: 7 [21760/38912 (56%)]\tLoss: 1.315356\n",
            "Train Epoch: 7 [23040/38912 (59%)]\tLoss: 1.327710\n",
            "Train Epoch: 7 [24320/38912 (62%)]\tLoss: 1.307913\n",
            "Train Epoch: 7 [25600/38912 (66%)]\tLoss: 1.313807\n",
            "Train Epoch: 7 [26880/38912 (69%)]\tLoss: 1.283805\n",
            "Train Epoch: 7 [28160/38912 (72%)]\tLoss: 1.337584\n",
            "Train Epoch: 7 [29440/38912 (76%)]\tLoss: 1.339190\n",
            "Train Epoch: 7 [30720/38912 (79%)]\tLoss: 1.340006\n",
            "Train Epoch: 7 [32000/38912 (82%)]\tLoss: 1.339448\n",
            "Train Epoch: 7 [33280/38912 (86%)]\tLoss: 1.331682\n",
            "Train Epoch: 7 [34560/38912 (89%)]\tLoss: 1.297031\n",
            "Train Epoch: 7 [35840/38912 (92%)]\tLoss: 1.320325\n",
            "Train Epoch: 7 [37120/38912 (95%)]\tLoss: 1.294322\n",
            "Train Epoch: 7 [38400/38912 (99%)]\tLoss: 1.314309\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHreat sulntand soavid atain.t the sase aatasl. \"Ithm ae ri .  \"e said  \"Ion't larky  yat,  said Harry,ss Hnce  hndiousllnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I han tlparate tp aayh aeck t.. . .ub't larky  I . .  \"I hm aot aarryed  Iarry,  said Humbledore  ses hoice sssottle staeng  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon aad boeoun tumbledore sf  cocltde  the  se hai tis hace  ause itnd sorpldethet tverysn the costant cakht af h lhraet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhle soess ng  lln tapraosen d aith aeegon.  \"H hhi tou hnparate tn t has arlling te saa oom wariain   Ihetkswood 'ss  ahet syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tn et,ai  sld aheughthe haoreeded atlottle  te haemed tie e  an ahnmoniitf the stdtation  aaalceed toyistirt t-wueom tt  \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an ahe ceght bnr  \"nd harry  hhok aookid atead an the still  and solr aheelid an ide tis aike a ler leus aesbled aauiledsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soie ri  tare sasprted  \"he soor wh hhe stettl ttairsase thet taa hack tn o the castle tas aaese   \"he e was aotsu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tn wad badpened t h  ealenn  s aas aot s lrea eng taarm a hhe   hu the mitht sf the malk  wo hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3563, Accuracy: 739461/1245184 (59%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/007.pt\n",
            "\n",
            "generated sample\t Harry Potter. Hermione opened his handw atterbing and toy straights to someth it off other, wooked his back. Weasparts eyes. Ereached them and then he ichards back to the castle of the splace room, \"ob' can, when\n",
            "generated sample\t Harry Pottern too. It had any also a mometh xaimeculacy. 'vol' dounget?\" he spelled his handly. He spreing near! he couldn that he didn't sliq his brack enterescoble! Anno echoines schedmed. Mrofessor Mrs. Nower \n",
            "generated sample\t Harry Potter, for a bought casilit; Penue he had shoot like in the air, Harry listen his qones of the cast, there was a cauldry came a past contract. Havale all sucked her, they said in a loud face. Che kitchen w\n",
            "generated sample\t Harry Potter, hazpidening toward Harry, the Dumbledore was beathing at somewhering her hesitable. \"Nind... It's all could be could alrealy elch!\" \"I may matter, who anyone said, Ron't gold up remort mess oh,\" sai\n",
            "generated sample\t Harry Potter, and a hollip with hairing. Harry could leave it had before Uumbledore's laugher Bilch'ant; Not handed nothinggess her zade found the name bous. Frowide was in the room. Harry woashed. He could have \n",
            "generated sample\t Harry Potter. Alough the contact day that it halfw all pust that Harry said 3utly all that somerably worried out a three of head. He was in an idea as he was one fired shouldery fast, it's moving leap, in took st\n",
            "generated sample\t Harry Potter he premed to cyp\" her fast stiply party and unally durpling. hif as both chieldfy over the Walled of passage. \"Hagrid, Ron? Sir,\" said Hermione, looking up at her 6ear, \"He will me help! it was back \n",
            "generated sample\t Harry Potter'strying in a quiling. The des, that's he said. \"I was play in feet and havpinous less you said Dobble,\" eyes eyeming help a caped other. \"Wood busistmasite,\" he said, blue pointing in, after a whole \n",
            "generated sample\t Harry Potter, her said, \"It,\" said Mastin his cheering vaular at him. \"Well, you y have a curied in bedres? G his something the course spects basing you.\" \"I sunlighted it age?\" ask Harry still umbling a masty. \"\n",
            "generated sample\t Harry Potters the little Clooking as barelause. Malfoy had a mitter all wasching then stone shripped in a thing loose scrucked it. He watched him and skints over at Sleaky. Harry-' Silcagian wasked, sitting a hal\n",
            "generated beam\t\t Harry Potter, Firius walling to get spake them hasn't into what all g back against the short but and what. It wanted, puriuish like Sirius anvious. I've meet \" \"Ergeur having a chair are Potterbing trappling up i\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 8 [0/38912 (0%)]\tLoss: 1.377160\n",
            "Train Epoch: 8 [1280/38912 (3%)]\tLoss: 1.300721\n",
            "Train Epoch: 8 [2560/38912 (7%)]\tLoss: 1.314439\n",
            "Train Epoch: 8 [3840/38912 (10%)]\tLoss: 1.349470\n",
            "Train Epoch: 8 [5120/38912 (13%)]\tLoss: 1.317346\n",
            "Train Epoch: 8 [6400/38912 (16%)]\tLoss: 1.297706\n",
            "Train Epoch: 8 [7680/38912 (20%)]\tLoss: 1.314651\n",
            "Train Epoch: 8 [8960/38912 (23%)]\tLoss: 1.300249\n",
            "Train Epoch: 8 [10240/38912 (26%)]\tLoss: 1.317598\n",
            "Train Epoch: 8 [11520/38912 (30%)]\tLoss: 1.289130\n",
            "Train Epoch: 8 [12800/38912 (33%)]\tLoss: 1.286372\n",
            "Train Epoch: 8 [14080/38912 (36%)]\tLoss: 1.345761\n",
            "Train Epoch: 8 [15360/38912 (39%)]\tLoss: 1.302586\n",
            "Train Epoch: 8 [16640/38912 (43%)]\tLoss: 1.281125\n",
            "Train Epoch: 8 [17920/38912 (46%)]\tLoss: 1.300667\n",
            "Train Epoch: 8 [19200/38912 (49%)]\tLoss: 1.296558\n",
            "Train Epoch: 8 [20480/38912 (53%)]\tLoss: 1.324519\n",
            "Train Epoch: 8 [21760/38912 (56%)]\tLoss: 1.301098\n",
            "Train Epoch: 8 [23040/38912 (59%)]\tLoss: 1.320320\n",
            "Train Epoch: 8 [24320/38912 (62%)]\tLoss: 1.297561\n",
            "Train Epoch: 8 [25600/38912 (66%)]\tLoss: 1.303248\n",
            "Train Epoch: 8 [26880/38912 (69%)]\tLoss: 1.277894\n",
            "Train Epoch: 8 [28160/38912 (72%)]\tLoss: 1.327498\n",
            "Train Epoch: 8 [29440/38912 (76%)]\tLoss: 1.327088\n",
            "Train Epoch: 8 [30720/38912 (79%)]\tLoss: 1.330826\n",
            "Train Epoch: 8 [32000/38912 (82%)]\tLoss: 1.335912\n",
            "Train Epoch: 8 [33280/38912 (86%)]\tLoss: 1.323464\n",
            "Train Epoch: 8 [34560/38912 (89%)]\tLoss: 1.289622\n",
            "Train Epoch: 8 [35840/38912 (92%)]\tLoss: 1.308626\n",
            "Train Epoch: 8 [37120/38912 (95%)]\tLoss: 1.287923\n",
            "Train Epoch: 8 [38400/38912 (99%)]\tLoss: 1.310071\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHreat sulntand soavid atain.t the case natasl. \"Ithm aerri .  \"e said  \"Ion't barry  yht,  said Harry,ss Hnce  andiouslltout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I han tlparate tp aayh aeck t.. . .ub't tarky  I . .  \"I hm aot aarryed  Iarry,  said Humbledore, ais hoice sssattle staeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon wad bheoun humbledore wf  cocltded hhe  se hai tis hace  aase itnd soraldeahet tverysn the costant cakht af h lhreet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhle oooas ng  lln tapraosed d aith aeegon.  \"H hhi tou wnparate tn t has arlling te faaroom tariains  Ihetkswood 'ss  ahet sto\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneet,ai  sld aheughthe haoreeded atlottle  be haemed tie ee an thnponiitf the ctdtation  aaalgeed toyistirt t wueom tt  \"  ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an ahe ceght bnr. \"nd harry  whok aookid atead an the ctill  and sllreaheelid an ide tis aike a ler leus cesbled aauiletsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soie ri  ware sasprted  \"he soor wh hhe stittl ttairsase that taa hack tn o the castle tas aaese   \"he e was aotsu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tn wad badpened t h  erlenn  s has aot t lrea eng taarm a hher  hu the matht sf the malk  wo hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3514, Accuracy: 740526/1245184 (59%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/008.pt\n",
            "\n",
            "generated sample\t Harry Potterwout Reamus\" \"I'm second wheter else at them at the troby back in her use it?\" Rumabled point, Much more trust. \"OEcenset I can telling my feeling end nervoushance attempteen to have done broomstig.\" \n",
            "generated sample\t Harry Potter and seemed-all gast almazining in his bole. Never, her papred in a hurcryble to heavy groum called from the Hool poor. \"We woodered dog sobs! Stoppet's here sit... be'll have been to because yh're ha\n",
            "generated sample\t Harry Potter, his bark in the gar. It Bust Neeper very splared, heading his mishorrabody next thing hand, roaring \"Someone was a mags! Dady whose askaboy, that at about withlectly belive anital? Cosmeader is for \n",
            "generated sample\t Harry Potter, with a joist had rushed him perixed lizarias, Scabbing down cast the pone of them ope. Hercy he had neckle; and beaking a great sure opes: The Woalers urrentylethy. they wairded his face again were \n",
            "generated sample\t Harry Potter, going in away. These and there Harry hurtled, bacling a look out from whom seemed back the cuilling room biq canduring elsiht, him eyes shouldent, he said these from usually, a launcing for loudy. u\n",
            "generated sample\t Harry Potter, had been parting on the flape, her last night bodkes of the pitchet. Harry stood upon. Harry stoubled way followed exceptmatrying bowe efceedly over the a hand, the Propebrath fire or them swarmly M\n",
            "generated sample\t Harry Potter, whose back in the Batsroom what Dingus during his ways, he wasted her angrisal in she leaven askay. \"Fing, and year.\" Harry likedly, that unwarvanced. \"Oracok' said Pybbled-by in, but Lulus Burst do\n",
            "generated sample\t Harry Pottermal. Harry flushed in the boar. Theeves were burrish, but it didn't enough his eyes in a skidge passaps into her breat. \"And good what I think he didn't must not bag moment! Vaying hare hroteblemstryv\n",
            "generated sample\t Harry Potter, and a strent of croud's poullukevely and walled a cap beaming the small. bulled in his body, there, Harry said, Neveralding stuffled and hourid go. PerTy Slarm and lass crumbing loaght in them still\n",
            "generated sample\t Harry Potter duny such, looking to see them that Harry worrtelly unanledI he said. \"Lough, is what?\" said Mungus. \"And oh, look?\" greemed those businis and said. \"Sook this Slit and george,\" said Professably, and\n",
            "generated beam\t\t Harry Potter, the room. \"Ind much!!\" \"Hell... I dust gere yithready, to these of the wasting for very PickSNway forced me the fifty feathers at Mordan famous and, had those points. Harry, it was quilling puft in \n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 9 [0/38912 (0%)]\tLoss: 1.373323\n",
            "Train Epoch: 9 [1280/38912 (3%)]\tLoss: 1.292580\n",
            "Train Epoch: 9 [2560/38912 (7%)]\tLoss: 1.306014\n",
            "Train Epoch: 9 [3840/38912 (10%)]\tLoss: 1.340209\n",
            "Train Epoch: 9 [5120/38912 (13%)]\tLoss: 1.311832\n",
            "Train Epoch: 9 [6400/38912 (16%)]\tLoss: 1.291384\n",
            "Train Epoch: 9 [7680/38912 (20%)]\tLoss: 1.304515\n",
            "Train Epoch: 9 [8960/38912 (23%)]\tLoss: 1.293913\n",
            "Train Epoch: 9 [10240/38912 (26%)]\tLoss: 1.310923\n",
            "Train Epoch: 9 [11520/38912 (30%)]\tLoss: 1.281541\n",
            "Train Epoch: 9 [12800/38912 (33%)]\tLoss: 1.277650\n",
            "Train Epoch: 9 [14080/38912 (36%)]\tLoss: 1.335440\n",
            "Train Epoch: 9 [15360/38912 (39%)]\tLoss: 1.294310\n",
            "Train Epoch: 9 [16640/38912 (43%)]\tLoss: 1.276127\n",
            "Train Epoch: 9 [17920/38912 (46%)]\tLoss: 1.290084\n",
            "Train Epoch: 9 [19200/38912 (49%)]\tLoss: 1.289745\n",
            "Train Epoch: 9 [20480/38912 (53%)]\tLoss: 1.317565\n",
            "Train Epoch: 9 [21760/38912 (56%)]\tLoss: 1.293417\n",
            "Train Epoch: 9 [23040/38912 (59%)]\tLoss: 1.314064\n",
            "Train Epoch: 9 [24320/38912 (62%)]\tLoss: 1.290922\n",
            "Train Epoch: 9 [25600/38912 (66%)]\tLoss: 1.297079\n",
            "Train Epoch: 9 [26880/38912 (69%)]\tLoss: 1.265329\n",
            "Train Epoch: 9 [28160/38912 (72%)]\tLoss: 1.319589\n",
            "Train Epoch: 9 [29440/38912 (76%)]\tLoss: 1.319236\n",
            "Train Epoch: 9 [30720/38912 (79%)]\tLoss: 1.318571\n",
            "Train Epoch: 9 [32000/38912 (82%)]\tLoss: 1.322228\n",
            "Train Epoch: 9 [33280/38912 (86%)]\tLoss: 1.313376\n",
            "Train Epoch: 9 [34560/38912 (89%)]\tLoss: 1.284048\n",
            "Train Epoch: 9 [35840/38912 (92%)]\tLoss: 1.305076\n",
            "Train Epoch: 9 [37120/38912 (95%)]\tLoss: 1.283046\n",
            "Train Epoch: 9 [38400/38912 (99%)]\tLoss: 1.299842\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHreat sulntond soavid atain.t the case aatisl. \"Ithm aeari .  \"e said  \"Ion't tarry  Iht,  said Harry,ss tnce  \"ndious ltout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan'tlparate tp tayh aeck t.. . .ub't tarry  I . .  \"I hm aot tarryed  Iarry,  said Humbledore, ais foice sssottle staeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon wad bheoun humbledore wf  cocltde  hhe  se hai tis hace  aase itnd soraldeahet tverysn the cosaant oaght af h lhreet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhle oooas ng  lln tapraose  d aith aoegon.  \"H hhi tou wnparate tn t tas artling te fasroom tariain   Ihetksyood 'ss  ahet syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneet,ai  snd aheugh he haorgeded ttlottle  ae hatmed tie ei an thnponiitf the ctdtation  aaalgeed toyisfirt t wuoom ta  \"  ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght bnr  \"nd harry  whok aookid atead an the ctill  and sllreaheelid an ide tis aike a ler leus besble  aaupletsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soie ri  ware sasprted  \"he soor wh hhe stital btairsase that taa tack tn o the castle ois aaese   \"he e was aotsu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tn wad badpened t h  erlinn  s has aot t brea eng taarm a hhe   wu the mitht bf the milk  wo hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3432, Accuracy: 743541/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/009.pt\n",
            "\n",
            "generated sample\t Harry Pottered we shake, unaperont that the others -momemtagald voice graying many injastibly butled aback the DrimusKlag elvally ensemed that Trass Fred and neverie, and he'd nothing that door that he must who d\n",
            "generated sample\t Harry Potter's naster though before he rose it a pat Looks hats back. \"If he was going to see that there awokate Scape that particular,\" Hermione had already glimpled in the ambrime as though it shopled. \"focough\n",
            "generated sample\t Harry Pottermat behind him. Oil, of chains outside everyone. Harry bottled himself talled at the sounded glasson, a very decideration to Hprouch Dumple calling at her wand, eithering Seaming and grunting. \"What's\n",
            "generated sample\t Harry Pottering mork and ozerhat the Duidditch Professor McGonagall, there were creeizing at him, missing any dewn class by partinction. \"Pirty, NiVth Malas nerve I?\" \"How can't think that mish have been?? I'mgeo\n",
            "generated sample\t Harry Potter?'bowed like the beaz home. \"Ron, only along other foolish and now.\" \"We'lleg!\" said Hermione. \"Yeh, anyludian to mutterer than, what?' his breephairs tailed. Hermione was arrus at Wark. Vill wasn't j\n",
            "generated sample\t Harry Potterward There has huddedled upop it in the vair other housen and help this way, baxing about the eyammer, but Harry had finished his back that Christmas grassed the start amabtle in the lesson sauderathe\n",
            "generated sample\t Harry Potter. Harro, inside Muggley and Potions post Grither of the couple streaming. \"And charm what have been gone to a dognit mack the poom.\" And Harry was nothing moly in And qusie went down the game with Hag\n",
            "generated sample\t Harry Potter-:, Ron a great inside in Alitable from Pars. \"Cours! Kobest you oughot, he thought it t-keep if you hasping at last years antornally.\" Willing She leaving a Quior of a chiebd and all over the other Q\n",
            "generated sample\t Harry Potter. The spell was a look at the narrow clap out and say. \"It's them done ?\" Harry been Brabbe across at the fire with the Mabision, Hermione's leap. \"I don't prouring their way this naso you know all th\n",
            "generated sample\t Harry Potter, Those player, had gain numer. All retreet the winds. And balked in Dudles pinks upon the door behond litherin Whove - 2Fthered glass, \"Source kezcelled it voice, Harry, isn't be any waited, perfacti\n",
            "generated beam\t\t Harry Potter gave everything that Oh might. Milory. The way who were doing any five racket the ghints. He storped to releaming her fieldboots to she asdreaming the start thut Harryilages like so glumberally hat a\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 10 [0/38912 (0%)]\tLoss: 1.364244\n",
            "Train Epoch: 10 [1280/38912 (3%)]\tLoss: 1.283898\n",
            "Train Epoch: 10 [2560/38912 (7%)]\tLoss: 1.295326\n",
            "Train Epoch: 10 [3840/38912 (10%)]\tLoss: 1.337572\n",
            "Train Epoch: 10 [5120/38912 (13%)]\tLoss: 1.306198\n",
            "Train Epoch: 10 [6400/38912 (16%)]\tLoss: 1.284916\n",
            "Train Epoch: 10 [7680/38912 (20%)]\tLoss: 1.296659\n",
            "Train Epoch: 10 [8960/38912 (23%)]\tLoss: 1.282824\n",
            "Train Epoch: 10 [10240/38912 (26%)]\tLoss: 1.301182\n",
            "Train Epoch: 10 [11520/38912 (30%)]\tLoss: 1.275354\n",
            "Train Epoch: 10 [12800/38912 (33%)]\tLoss: 1.272246\n",
            "Train Epoch: 10 [14080/38912 (36%)]\tLoss: 1.324868\n",
            "Train Epoch: 10 [15360/38912 (39%)]\tLoss: 1.292260\n",
            "Train Epoch: 10 [16640/38912 (43%)]\tLoss: 1.273646\n",
            "Train Epoch: 10 [17920/38912 (46%)]\tLoss: 1.281772\n",
            "Train Epoch: 10 [19200/38912 (49%)]\tLoss: 1.284117\n",
            "Train Epoch: 10 [20480/38912 (53%)]\tLoss: 1.312358\n",
            "Train Epoch: 10 [21760/38912 (56%)]\tLoss: 1.288112\n",
            "Train Epoch: 10 [23040/38912 (59%)]\tLoss: 1.307705\n",
            "Train Epoch: 10 [24320/38912 (62%)]\tLoss: 1.288485\n",
            "Train Epoch: 10 [25600/38912 (66%)]\tLoss: 1.287522\n",
            "Train Epoch: 10 [26880/38912 (69%)]\tLoss: 1.255589\n",
            "Train Epoch: 10 [28160/38912 (72%)]\tLoss: 1.308122\n",
            "Train Epoch: 10 [29440/38912 (76%)]\tLoss: 1.312586\n",
            "Train Epoch: 10 [30720/38912 (79%)]\tLoss: 1.314488\n",
            "Train Epoch: 10 [32000/38912 (82%)]\tLoss: 1.314826\n",
            "Train Epoch: 10 [33280/38912 (86%)]\tLoss: 1.309694\n",
            "Train Epoch: 10 [34560/38912 (89%)]\tLoss: 1.279578\n",
            "Train Epoch: 10 [35840/38912 (92%)]\tLoss: 1.300663\n",
            "Train Epoch: 10 [37120/38912 (95%)]\tLoss: 1.279282\n",
            "Train Epoch: 10 [38400/38912 (99%)]\tLoss: 1.292722\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHreet sulntond soavid ttain.t the sase a tisl. \"Ithm aoari .  \"e said  \"Ion't tarry  Iot,  said Harry,ss tnce  andious lnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan'tlparate tp tyyh aeck t.. . .ob't tanry  I . .  \"I hm aot tarryed  Iarry,  said Humbledore, ais foice sssottle staeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon wad bheou  humbledore wf  cocltded hhe  se hai tis hace  aare itnd soyaldiahet tverysn the cosaant oaght af t lhreet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhle oooas ng tlln typraose  d aith aoegon.  \"H hhi tou wnparate tn t tos artling te faa oom toriain   Ihetksyood ess  ahet syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneettab  snd hheugh he haorgeded ttlottle  ae hatmed tie ei an thnponiitf the stdtation  aahlgeed toyisfirt t wuoomsta  \"  ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght bnr  \"nd harry  whok aookid atead an the stinl  and solr aheelid an ide tis aike a ler tets besble  aaupletsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soie ri  ware sosprted  \"he soor wh hhe stital otairsase that hoa tack tn o the castle tis aaese   \"he e was aotsu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tn wad badpened t h  eclinn  s has aot t boed ing toarm a hhe   wu the might bf the filk  wo hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3389, Accuracy: 745517/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/010.pt\n",
            "\n",
            "generated sample\t Harry Potter like the Bood housef. \"I'm going everything, black you've plendiff you wouldn't you thought that tomafle, have still nothing like that is a big wish, so that, it's his memoPrable slid sort of the cha\n",
            "generated sample\t Harry Potter. Poor was still torming. {ummerous 8ain, down them he had sugdeed parcets only through them becoming the Dark and Seamings, back into these hositatives, and undered his smile, tooknow, a the coam. A.\n",
            "generated sample\t Harry Potter, he waited, his building front Geep before they alswed. Permione copleted, lower very hangroby? Khour excited back -- bask and looking less the voice cat on the bedward on the stuffled up as he gaste\n",
            "generated sample\t Harry Potter. ...,\" he appeared to persuadementh a hosital open babber. \"Ced,\" said RHPGins! Herhaps Dumbledoor. The haggward right ounced the momorial stone shaper waith to the moment they were with a rest be sh\n",
            "generated sample\t Harry Potter, near the last that walled the mistres voice, had beneady Apparate, the ground potion soutdentons. There was happing that he said his hands as something on Uagamas' for in helpe, but there wapped up \n",
            "generated sample\t Harry Potter. \"Yis' you thought yours? I want to this be curious.\" Harry, yelled, elves as he could have seared when Harry's chair for, whose who had searched them without annoyed. \"If you have to not to knick s.\n",
            "generated sample\t Harry Potter doing down the crowd. Dumbledore Growles was mught with itsless winto his fame. \"Anj of that.\" \"So, but have you know -this beJaus,\" said Scright Dumbledore in in his shoot. Went leaning glass as shi\n",
            "generated sample\t Harry Potter. Uncle you thousand and warget to the late. And look possible was looking in any of the Balls and the Voilet seeOs Phose bas a voice enter renold a varther footstephing and better they they lizhed be\n",
            "generated sample\t Harry Potter, who had gleamed until but with Dudley had Turning Professor Empridge stupisht. \"But I thought have it dong, with Hermione's kick,\" said Proofts. MyLO2 Don's box suphour. \"We're done if ... but she h\n",
            "generated sample\t Harry Potter's waiting. \"It ha&m been Klooking at about the Dableon. And why Dumbledore had been breakon's longs. Harry! Mrs. Wook half,\" said Hedrowed Windully, \"you was\" making he done not take to her made Mr. \n",
            "generated beam\t\t Harry Potter disappoaromenty from the onion, \"oop, don't bilus askin, you make proormay.\" He had roared they pretects Dumbledore to get rapha; largue in her light sgueeted, have a large-out for a bith team on shi\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 11 [0/38912 (0%)]\tLoss: 1.358473\n",
            "Train Epoch: 11 [1280/38912 (3%)]\tLoss: 1.273551\n",
            "Train Epoch: 11 [2560/38912 (7%)]\tLoss: 1.289670\n",
            "Train Epoch: 11 [3840/38912 (10%)]\tLoss: 1.331776\n",
            "Train Epoch: 11 [5120/38912 (13%)]\tLoss: 1.299699\n",
            "Train Epoch: 11 [6400/38912 (16%)]\tLoss: 1.278797\n",
            "Train Epoch: 11 [7680/38912 (20%)]\tLoss: 1.287693\n",
            "Train Epoch: 11 [8960/38912 (23%)]\tLoss: 1.274273\n",
            "Train Epoch: 11 [10240/38912 (26%)]\tLoss: 1.293066\n",
            "Train Epoch: 11 [11520/38912 (30%)]\tLoss: 1.274264\n",
            "Train Epoch: 11 [12800/38912 (33%)]\tLoss: 1.267641\n",
            "Train Epoch: 11 [14080/38912 (36%)]\tLoss: 1.317853\n",
            "Train Epoch: 11 [15360/38912 (39%)]\tLoss: 1.286161\n",
            "Train Epoch: 11 [16640/38912 (43%)]\tLoss: 1.267799\n",
            "Train Epoch: 11 [17920/38912 (46%)]\tLoss: 1.274657\n",
            "Train Epoch: 11 [19200/38912 (49%)]\tLoss: 1.279058\n",
            "Train Epoch: 11 [20480/38912 (53%)]\tLoss: 1.304709\n",
            "Train Epoch: 11 [21760/38912 (56%)]\tLoss: 1.283678\n",
            "Train Epoch: 11 [23040/38912 (59%)]\tLoss: 1.301096\n",
            "Train Epoch: 11 [24320/38912 (62%)]\tLoss: 1.283084\n",
            "Train Epoch: 11 [25600/38912 (66%)]\tLoss: 1.282430\n",
            "Train Epoch: 11 [26880/38912 (69%)]\tLoss: 1.248502\n",
            "Train Epoch: 11 [28160/38912 (72%)]\tLoss: 1.305695\n",
            "Train Epoch: 11 [29440/38912 (76%)]\tLoss: 1.304741\n",
            "Train Epoch: 11 [30720/38912 (79%)]\tLoss: 1.310742\n",
            "Train Epoch: 11 [32000/38912 (82%)]\tLoss: 1.307585\n",
            "Train Epoch: 11 [33280/38912 (86%)]\tLoss: 1.304921\n",
            "Train Epoch: 11 [34560/38912 (89%)]\tLoss: 1.273014\n",
            "Train Epoch: 11 [35840/38912 (92%)]\tLoss: 1.296580\n",
            "Train Epoch: 11 [37120/38912 (95%)]\tLoss: 1.274613\n",
            "Train Epoch: 11 [38400/38912 (99%)]\tLoss: 1.286979\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHreet sulntond soavid tnain.t the cose y tisl, \"I tm solri .  \"e said  \"Ion't tanry  yot,  said Harry,ss tnce  \"ndious lnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan'tlparate tp tyyh aeck t.. . .on't tanry  I . .  \"I dm sot torryed  Iarry,  said Humbledore, wis foice sssottle staeng  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon wad bheou  humbledore wf ecotltded hhe  se hai tis hace  aase itnd soypldiahet tverysn the cosaant oaght af t lhoeet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhle oooas ng tlon typraose  d aith aoogons  \"H hhi you wnparate tn t tos trtling te faa oom toriain   Ihetksyood ess  ahet syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneettab  ssd Hheugh he waorgeded ttlottle  ae hatmed tia e  hn thnponietf the stdtation  aahlceed toynsfirt t wuoomsta  \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght bnr  \"nd tarry  whok aookid atead an the stinl  and solr aheelid tn ide tis aoke a lor tets besble  aaupletsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soie ri  ware sosprted  \"he soor wh hhe stital staircase that hoa tack tn o the castle tis aaese   \"he e was aotso\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tn wad badpened t \" ceclin y s has aot t boed ing toarm a hhe   wu the might sf the filk  wo hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3363, Accuracy: 747069/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/011.pt\n",
            "\n",
            "generated sample\t Harry Potter worns right, there prodes they becape. They won't watch a wood wight in a moan that Vryffilting yeal. Hermione parted each at them Findous Onape. \"No,\" said Malfor GryMchole. \"Mr... you haven't Quidd\n",
            "generated sample\t Harry Potter was cares; for her way down scraby, for a class, thinded a water, one crastic a seren into one Slytherin sprontly, a long up of her approardely what Sweating once door was from Mack{ Malfoy everythin\n",
            "generated sample\t Harry Potter, looking wither that he had just endemently severately, and it ough, my, my also not get here in 1ituations _obbertable. To that very other was back for the Alitiona. Bellations of remotions in a bul\n",
            "generated sample\t Harry Potter. Wearlhown unlyone shool up sighly; back in his lick four hellast els' some gown, she was cheating help your freeh and large, the DAfest caulda swizes rathed with the Whole them's house of his unward\n",
            "generated sample\t Harry Potter. \"I happenelf. I shop you vought before, his lights to Co meet in this armancent,\" said Ron stiffly. \"you forgive you say idea!\" said Munakely, whose coulaged szirrubly slightly was a hand door, hand\n",
            "generated sample\t Harry Potter. My them would have found her clas? \"Inaps any,\" Y_lemmerion amakated, and Harry got her Lubbing caw the pantaring Professor Tix he pushed her carrock of the impairitale and knocDed the Daily, with m\n",
            "generated sample\t Harry Potterlow regaping the toase of the house enhousay, but leaping how if they had moted his bag most. He had too with but buggled goyle has fung. he had been watching. \"ffow her'y!\" said Potter, which, Ron wa\n",
            "generated sample\t Harry Potter$ caulings. Harry gave the Sork And he thoughed, he had /ondlemed with them are realize surprise. Shick raised as though he was trying to sime he, Harry who pulled Pagman who who had masterable? \" \"Th\n",
            "generated sample\t Harry Potter, wiph grouping up what toop behidde: Snape. Popartay on the boards was nothing. . ... Nobby believed, Harry! Oht was rough haunted her days pick sudmiss the Dark Pobes Soophowed right at Hogwarts Swo\n",
            "generated sample\t Harry Potter what could into his making bullight by Dail and Gay, so this joines langer in themmem, and dolfs up the staff eyes of the enormous a real in head, shoutly as Ixmeade it, Harry was those trick of his \n",
            "generated beam\t\t Harry Potter had trust his head, he prage in a gream scake with she castled; his voices and looked awain into them. Ron had realy shook up to the Greachey. Harry, who seived a giant put the grounds consided into \n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 12 [0/38912 (0%)]\tLoss: 1.354707\n",
            "Train Epoch: 12 [1280/38912 (3%)]\tLoss: 1.265548\n",
            "Train Epoch: 12 [2560/38912 (7%)]\tLoss: 1.284762\n",
            "Train Epoch: 12 [3840/38912 (10%)]\tLoss: 1.328894\n",
            "Train Epoch: 12 [5120/38912 (13%)]\tLoss: 1.293625\n",
            "Train Epoch: 12 [6400/38912 (16%)]\tLoss: 1.271044\n",
            "Train Epoch: 12 [7680/38912 (20%)]\tLoss: 1.283159\n",
            "Train Epoch: 12 [8960/38912 (23%)]\tLoss: 1.267261\n",
            "Train Epoch: 12 [10240/38912 (26%)]\tLoss: 1.289055\n",
            "Train Epoch: 12 [11520/38912 (30%)]\tLoss: 1.270076\n",
            "Train Epoch: 12 [12800/38912 (33%)]\tLoss: 1.262819\n",
            "Train Epoch: 12 [14080/38912 (36%)]\tLoss: 1.310275\n",
            "Train Epoch: 12 [15360/38912 (39%)]\tLoss: 1.281178\n",
            "Train Epoch: 12 [16640/38912 (43%)]\tLoss: 1.263474\n",
            "Train Epoch: 12 [17920/38912 (46%)]\tLoss: 1.269660\n",
            "Train Epoch: 12 [19200/38912 (49%)]\tLoss: 1.273984\n",
            "Train Epoch: 12 [20480/38912 (53%)]\tLoss: 1.296512\n",
            "Train Epoch: 12 [21760/38912 (56%)]\tLoss: 1.279146\n",
            "Train Epoch: 12 [23040/38912 (59%)]\tLoss: 1.294667\n",
            "Train Epoch: 12 [24320/38912 (62%)]\tLoss: 1.277649\n",
            "Train Epoch: 12 [25600/38912 (66%)]\tLoss: 1.278648\n",
            "Train Epoch: 12 [26880/38912 (69%)]\tLoss: 1.243943\n",
            "Train Epoch: 12 [28160/38912 (72%)]\tLoss: 1.300279\n",
            "Train Epoch: 12 [29440/38912 (76%)]\tLoss: 1.297822\n",
            "Train Epoch: 12 [30720/38912 (79%)]\tLoss: 1.303520\n",
            "Train Epoch: 12 [32000/38912 (82%)]\tLoss: 1.301006\n",
            "Train Epoch: 12 [33280/38912 (86%)]\tLoss: 1.298216\n",
            "Train Epoch: 12 [34560/38912 (89%)]\tLoss: 1.265544\n",
            "Train Epoch: 12 [35840/38912 (92%)]\tLoss: 1.290158\n",
            "Train Epoch: 12 [37120/38912 (95%)]\tLoss: 1.268625\n",
            "Train Epoch: 12 [38400/38912 (99%)]\tLoss: 1.283665\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet sulntond soavid ttain.t the cose y tisl, \"I tm solri .  \"e said  \"Ion't tanry  yor,  said Harry,ss tnce. \"ndious lnout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan'tlparate tp tyyh aeck t.. . .on't tanry  I . .  \"I dm sot torryed  Iarry,  said Humbledore, wis foice sssottle staeng  \n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon wad booou  humbledore wf ecotltged hhe  se hai tis hace  aase itnd soypldethet tverysn the cosaant saght af t ltoeet y e\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lhle oooas ng tlon typrayse  d aith aoogons  \"I hhi you wnparate tn t tos trtling te faa oom toriain   Ihetksyood ess  yhat syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneed,ab  ssd Hheugh he waorgeded ttlottle  ae saemed tia e  wn tonponietf the stdtation  aahlceed toynsfirt t wuoomsta  \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght tnr  \" d tarry  whok aookid atead an the stinl  and solr aheelid tn ide tis aoke a sor tets cesble  aaupletsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he soie ri  ware sosprted  \"he soor wo hhe stital staircase that hoa tack tn o the castle tis aaesed  \"he e was aotso\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tn wad badpened t \"rceclin y s has aot t soed isg toarm a whe   wu the saght sf the filk  wo hai tumbledore s hand aooin\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3333, Accuracy: 748156/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/012.pt\n",
            "\n",
            "generated sample\t Harry Potter. And with Falue Sumpoff was using the stemme of a subarmed in the intrexmition Volevous, goyling and Malacon's bagmas, his body contained. \"Harry, bought by it's all wing,\" said Hasterma, Harry looke\n",
            "generated sample\t Harry Potter. \"Fours Ron, territ,\" said Minispay, in an along. The first was josite would hair before what it've with heavy in a Lorth're echoined. \"Nothing to go back to the last. I wish, annoy quiet.\" \"Harry be\n",
            "generated sample\t Harry Potter. Harry washed, pass them hailed, you who would ha-been it?\" he see though them, then through on him want they were the large Bagure were delierivally whome hemper in the paintors. Dumbledore later, \"\n",
            "generated sample\t Harry Potter in the parcement, though he said were looking in though than books followed. Pather at home, sorty, who had booked. \"He are to -tell that Ron' now notigel that Potter don' wait to that I could 6notax\n",
            "generated sample\t Harry Potter swim shut that it who went doing and saugh him to fourth... and contended (uron hapher a disap, laging - soft busief in them looked in a loud covernon to they from them, and the moment there wanted t\n",
            "generated sample\t Harry Potter, Harry remegbed all to Hermoon's in a goody at Harry and be's quick. All for rebolutes as the first big in the -dask looks foot to cont. His this pake with the cormbulas each o' Hourta Sprobable for \n",
            "generated sample\t Harry Potter had she was going to the chail table, and looked at the empty over the hatsmal breathea. Ban, Harry's instruned moods durm she was looking to the other secoal carry with the oway? Sitten, and yellow \n",
            "generated sample\t Harry Potter, you see hrow whose he were Ron's agreed, those sturning in her Figupress, I waz to be back off cold.\" \"I have been have cold,\" said Marous look as the cought dustle over the wits empty-ing his basha\n",
            "generated sample\t Harry Potter, they seem in a twass, Hermione else upjose he had most broke, grumpike a look on the lage and the cobys in the skin back down, entragizing a momem to Slughorn untive. \"I have less ever that wetrallo\n",
            "generated sample\t Harry Potter abse why thought a walk. \"Euch is so it makbed Dobby? Prt! I'm not too list of a Harry good at leaps for implediant on Dudley got echoing. \"Ih met that look for detent's was in there, this say and Pe\n",
            "generated beam\t\t Harry Potter's seat. He because asked what what for he wished, but she else smited in feet sitting unward them and then becase that had neares benearly soaked reso very a sleeve from her lunch, aside these neishe\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 13 [0/38912 (0%)]\tLoss: 1.352026\n",
            "Train Epoch: 13 [1280/38912 (3%)]\tLoss: 1.262162\n",
            "Train Epoch: 13 [2560/38912 (7%)]\tLoss: 1.280514\n",
            "Train Epoch: 13 [3840/38912 (10%)]\tLoss: 1.323506\n",
            "Train Epoch: 13 [5120/38912 (13%)]\tLoss: 1.287038\n",
            "Train Epoch: 13 [6400/38912 (16%)]\tLoss: 1.262841\n",
            "Train Epoch: 13 [7680/38912 (20%)]\tLoss: 1.279866\n",
            "Train Epoch: 13 [8960/38912 (23%)]\tLoss: 1.262834\n",
            "Train Epoch: 13 [10240/38912 (26%)]\tLoss: 1.286542\n",
            "Train Epoch: 13 [11520/38912 (30%)]\tLoss: 1.268079\n",
            "Train Epoch: 13 [12800/38912 (33%)]\tLoss: 1.257859\n",
            "Train Epoch: 13 [14080/38912 (36%)]\tLoss: 1.302296\n",
            "Train Epoch: 13 [15360/38912 (39%)]\tLoss: 1.273274\n",
            "Train Epoch: 13 [16640/38912 (43%)]\tLoss: 1.258346\n",
            "Train Epoch: 13 [17920/38912 (46%)]\tLoss: 1.267728\n",
            "Train Epoch: 13 [19200/38912 (49%)]\tLoss: 1.267247\n",
            "Train Epoch: 13 [20480/38912 (53%)]\tLoss: 1.290388\n",
            "Train Epoch: 13 [21760/38912 (56%)]\tLoss: 1.272898\n",
            "Train Epoch: 13 [23040/38912 (59%)]\tLoss: 1.291548\n",
            "Train Epoch: 13 [24320/38912 (62%)]\tLoss: 1.273028\n",
            "Train Epoch: 13 [25600/38912 (66%)]\tLoss: 1.273679\n",
            "Train Epoch: 13 [26880/38912 (69%)]\tLoss: 1.236281\n",
            "Train Epoch: 13 [28160/38912 (72%)]\tLoss: 1.294865\n",
            "Train Epoch: 13 [29440/38912 (76%)]\tLoss: 1.292401\n",
            "Train Epoch: 13 [30720/38912 (79%)]\tLoss: 1.297786\n",
            "Train Epoch: 13 [32000/38912 (82%)]\tLoss: 1.297233\n",
            "Train Epoch: 13 [33280/38912 (86%)]\tLoss: 1.292702\n",
            "Train Epoch: 13 [34560/38912 (89%)]\tLoss: 1.259397\n",
            "Train Epoch: 13 [35840/38912 (92%)]\tLoss: 1.283191\n",
            "Train Epoch: 13 [37120/38912 (95%)]\tLoss: 1.262753\n",
            "Train Epoch: 13 [38400/38912 (99%)]\tLoss: 1.279005\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet selhtond soavid ttain.t the cose y tisl, \"I tm solri .  \"e said  \"Ion't tanry  yor,\" said Harry,ss tnce. andious lsout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I wan'tlparate tp tyyh aeck t.. . .on't tanry  I . .  \"I dm sot tarryed  Iarry,  said Humbledore, wis foice sssottle staeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\tarateon wad boooun humbledore wf ecotltged hhe  se hai tis hace  aase itnd soypldethat tverysn the cosaant saght af t ltaeet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lele oooas ng tlon typraysed d aith aeogons  \"I hhi you tnparate tn t tos trtling te daa oom tariain  \"Ihetksyood ess  yhat syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneed ab  ssd Hheugh he waorgeded ttlottle  ae saemed tia ei wn tonpondetf the ftdtation  aahlceed toynsfirtet wuoomsta  \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght tnr  \" d tarry  whok aookid atead an the stynl  and solr aheelid an ide tis aoke a ser tett cesble  aaupletsiog ti\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he seie ri  ware soaprted  \"he soor wo the ftital staircase that hoa tack tn o the castle tis aaesed  \"he e was aotse\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tn wad badpened t \"rceclin yas has sot t boed isg toarm a \"he   wu the saght tf the filk  wo hai tumbledore s hand tooin\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3274, Accuracy: 749851/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/013.pt\n",
            "\n",
            "generated sample\t Harry Potter, who could done a lot out of him. Ye was spying to more. Hermione had the basy in his book. \"My SibP!\" Ceamus Hagrid attemptatiyed in a Qush before because he Mranger shook his head, stunning down to\n",
            "generated sample\t Harry Potter threeling. He charrying the door \" \"Wyon' Alack5's in Potter's though! Don't meet you was yow. Everyone books to tell you can feel you you too do it.\" \"And Harry?\" had the boy Prookens was at Hommatu\n",
            "generated sample\t Harry Potter's bell in cauldron and the blact and statief from her aimchantaches, held how glasses were varcelt, gringing first an a very scotter as four, the Death Peorge Poice, the joss to be last now paic to h\n",
            "generated sample\t Harry Potter, why he looked school sure it with hundred something weren't look at onho, sounding holding Sonce, though, yeh got down across there by the prove cup insteaz. \"Ron's near last, you don't make you tot\n",
            "generated sample\t Harry Potter, the doorwaits looking belowd' his sweet. Gasmed Malfoy had still looking down his second who was usuashed for poorfet. Hands on the toached Vass behind his way out the gates in the contragues, but i\n",
            "generated sample\t Harry Potter Xay Potter Barror's champtomy Mapical Leath. A bit. And a gloomis, as though, Professor Dumbledore bought impose reading that he crouddled, though she was on the dementor day, blinking her laughing h\n",
            "generated sample\t Harry Potter him, thought they came Harry'; exprected Place out onto the library weather he had seen cary and Ron's blabber. \"Sever master!' Offered Mastione, who have found he got endermed a Hew cause she had ju\n",
            "generated sample\t Harry Potter looking a large polich, Harry's study: [every tictured hand up at bed. \"That in contemort more firely hoc for Didgots, hard last you, now.... that is im? I made harp, his glass as your gosy, he's not\n",
            "generated sample\t Harry Potter of Hermione house, he heard her, whoke had felt inspatied' \"Sha[y! NABSY\" Those Flima stimled to be she chouldrathed hardly. \"How, only,\" he told Hermione, Right Harty was softling in Harry's wand to\n",
            "generated sample\t Harry Potter. Whose immoved but Harry's Ron's hand was to go toward the Euffle light that their back and soobed admit to be; it everyone. \"Well your cause and Woody, I though - am gobber in the Smydey of Ourself,\n",
            "generated beam\t\t Harry Potter, Harry's handayshing bag and thring, into Hob-wayd that disappeared numeriy to have to fallen, which still holding out about. \"Vood, is you're not tear memon.... no day,\" \"I've used making the hat -'\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 14 [0/38912 (0%)]\tLoss: 1.348115\n",
            "Train Epoch: 14 [1280/38912 (3%)]\tLoss: 1.261747\n",
            "Train Epoch: 14 [2560/38912 (7%)]\tLoss: 1.277225\n",
            "Train Epoch: 14 [3840/38912 (10%)]\tLoss: 1.318341\n",
            "Train Epoch: 14 [5120/38912 (13%)]\tLoss: 1.282787\n",
            "Train Epoch: 14 [6400/38912 (16%)]\tLoss: 1.256948\n",
            "Train Epoch: 14 [7680/38912 (20%)]\tLoss: 1.276560\n",
            "Train Epoch: 14 [8960/38912 (23%)]\tLoss: 1.259378\n",
            "Train Epoch: 14 [10240/38912 (26%)]\tLoss: 1.282724\n",
            "Train Epoch: 14 [11520/38912 (30%)]\tLoss: 1.261877\n",
            "Train Epoch: 14 [12800/38912 (33%)]\tLoss: 1.254132\n",
            "Train Epoch: 14 [14080/38912 (36%)]\tLoss: 1.298831\n",
            "Train Epoch: 14 [15360/38912 (39%)]\tLoss: 1.265625\n",
            "Train Epoch: 14 [16640/38912 (43%)]\tLoss: 1.254260\n",
            "Train Epoch: 14 [17920/38912 (46%)]\tLoss: 1.265340\n",
            "Train Epoch: 14 [19200/38912 (49%)]\tLoss: 1.260557\n",
            "Train Epoch: 14 [20480/38912 (53%)]\tLoss: 1.285117\n",
            "Train Epoch: 14 [21760/38912 (56%)]\tLoss: 1.269665\n",
            "Train Epoch: 14 [23040/38912 (59%)]\tLoss: 1.289191\n",
            "Train Epoch: 14 [24320/38912 (62%)]\tLoss: 1.270744\n",
            "Train Epoch: 14 [25600/38912 (66%)]\tLoss: 1.268755\n",
            "Train Epoch: 14 [26880/38912 (69%)]\tLoss: 1.229471\n",
            "Train Epoch: 14 [28160/38912 (72%)]\tLoss: 1.289240\n",
            "Train Epoch: 14 [29440/38912 (76%)]\tLoss: 1.288766\n",
            "Train Epoch: 14 [30720/38912 (79%)]\tLoss: 1.294027\n",
            "Train Epoch: 14 [32000/38912 (82%)]\tLoss: 1.293508\n",
            "Train Epoch: 14 [33280/38912 (86%)]\tLoss: 1.288541\n",
            "Train Epoch: 14 [34560/38912 (89%)]\tLoss: 1.255650\n",
            "Train Epoch: 14 [35840/38912 (92%)]\tLoss: 1.278052\n",
            "Train Epoch: 14 [37120/38912 (95%)]\tLoss: 1.258975\n",
            "Train Epoch: 14 [38400/38912 (99%)]\tLoss: 1.274261\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet selhtond soavid ttain.t the coseryatisl, \"I tm solri .  \"e said  \"Ion't tanry  Ior,\" said Harry,ss tnce. andious lsout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I dan'tlparate tp tyyh aeck t.. . .on't tanry  I . .  \"I tm sot tarryed  Iarry,  said Humbledore, wos foice sssottle staeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration wad boooun humbledore wn ecltltged hhe  se hai tis hace  wase iand soyp dethat tverysn the cosaant saght af t leaeet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lele oooas ng tlon typrissed d aith aeogons  \"I hhi you tnparate tn y tas trtling te daa oom wariain  \"Ihetksyood ess  yhat syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneed a   snd Hheugh he wairgeded ttlottle  ae saemed tia ei bn tonpondetf the ftdtation  aah ceed toynsfirtet wuoomsta  \"I me go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght tnr  \" d tarry  whok aookid atead an the ctynl  and solr aheelid an ide tis aoke a ler t tt cesble  aaupletsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he seie ri  ware coaprved  \"he soor wo the ftital staircase toat hoa tack tn o the castle tis aaesed  \"he e was aotse\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tt wad badpened t \"rcerliarias has aot t boed isg taarm a \"he   wu the faght tf the filk  wo hai tumbledore s hand tooin\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3240, Accuracy: 750532/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/014.pt\n",
            "\n",
            "generated sample\t Harry Potter's her shell, the kast hall ob his wing, clutching the carchairs, bed, because you gone, bucked, asked toward, he had been Halfing across the fame in the door oppositor....\" Harry and uncousonalized. \n",
            "generated sample\t Harry Potter, however, then deep, he added, though he had been room too fixurbly, sit how safe a second less. Dumbledore had very grouphed, and lates in the tower there. \"What had got to those what I happen me it\n",
            "generated sample\t Harry Potter, though Professor Smiriz's respective, falizing the room like, and shocked with an enory, pointing like neather in magic. \"No!\" said Harry, in front laughten, a shaltbombs clanced in his chack in the\n",
            "generated sample\t Harry Potter I'd sti's found; it's an able imable together, he looked up own equst tomorrow, he could just be badge to you. Pother's distake plain as your way than whrel - and people got all like them? It was abo\n",
            "generated sample\t Harry Potter, exchanging from juicks over a time, back, to reached thematcheciath with every steps. \"Woursely, she wasn't, Hermione,\" said Nambagi. \"If, in they looking aughing....\" \"No,' he had a shake was a boy\n",
            "generated sample\t Harry Potter, inside it shouted. \"Oh, Madage . . .\" \"What to come Scabbets Mapie wood impossible!\" Savelit's mood creatived. \"Harry, bough Come-tord!\" said slightly. \"I have to give you, Harry Potter?\" \"Oh, I tho\n",
            "generated sample\t Harry Potter. ...\" They awl it looked at her portrait under cott. \"?\" AlRna! Ginny was filegatily at Hogwarts keparass, Harry expocted that's a mean, endles. Harry Madam Mooth EuG Propeed Weeky severed and sat in\n",
            "generated sample\t Harry Potter, his hang was soaking or \"Maybe, Then it,\" said UzGYy' \"What are you quicking enough,\" said in a hourse. \"Well, you might told you up a fitsobbing castle, came so yeh,\" he said, a countly and flaced \n",
            "generated sample\t Harry Potter as a how floor, Wesperation's cahes were arrus, her tauntime handwhed he made in latter without that dayblikening, a guardit sense. Theer, sweeking being ca}t. Here boum told it into the adoutting eg\n",
            "generated sample\t Harry Potter, Professor McGonagall, said down his eyes again, bustles to be brother cat t do. He was sleeping a little pimpor. Mr. Task and Ron's statched a loud beak half and dusted down. Housmiss teapoft was Ba\n",
            "generated beam\t\t Harry Potter, Marcin was, another bange, he was a surforation, he spilled what roomstild farwed were very that Half at leath'll only pitain with his goblet and almost asteady, happidly? looked like, Scabbers, And\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 15 [0/38912 (0%)]\tLoss: 1.346434\n",
            "Train Epoch: 15 [1280/38912 (3%)]\tLoss: 1.255055\n",
            "Train Epoch: 15 [2560/38912 (7%)]\tLoss: 1.275702\n",
            "Train Epoch: 15 [3840/38912 (10%)]\tLoss: 1.313548\n",
            "Train Epoch: 15 [5120/38912 (13%)]\tLoss: 1.279416\n",
            "Train Epoch: 15 [6400/38912 (16%)]\tLoss: 1.252806\n",
            "Train Epoch: 15 [7680/38912 (20%)]\tLoss: 1.273409\n",
            "Train Epoch: 15 [8960/38912 (23%)]\tLoss: 1.256787\n",
            "Train Epoch: 15 [10240/38912 (26%)]\tLoss: 1.277241\n",
            "Train Epoch: 15 [11520/38912 (30%)]\tLoss: 1.256546\n",
            "Train Epoch: 15 [12800/38912 (33%)]\tLoss: 1.248372\n",
            "Train Epoch: 15 [14080/38912 (36%)]\tLoss: 1.297336\n",
            "Train Epoch: 15 [15360/38912 (39%)]\tLoss: 1.259968\n",
            "Train Epoch: 15 [16640/38912 (43%)]\tLoss: 1.249716\n",
            "Train Epoch: 15 [17920/38912 (46%)]\tLoss: 1.262836\n",
            "Train Epoch: 15 [19200/38912 (49%)]\tLoss: 1.254546\n",
            "Train Epoch: 15 [20480/38912 (53%)]\tLoss: 1.278707\n",
            "Train Epoch: 15 [21760/38912 (56%)]\tLoss: 1.266245\n",
            "Train Epoch: 15 [23040/38912 (59%)]\tLoss: 1.285357\n",
            "Train Epoch: 15 [24320/38912 (62%)]\tLoss: 1.268098\n",
            "Train Epoch: 15 [25600/38912 (66%)]\tLoss: 1.265914\n",
            "Train Epoch: 15 [26880/38912 (69%)]\tLoss: 1.223887\n",
            "Train Epoch: 15 [28160/38912 (72%)]\tLoss: 1.282674\n",
            "Train Epoch: 15 [29440/38912 (76%)]\tLoss: 1.285949\n",
            "Train Epoch: 15 [30720/38912 (79%)]\tLoss: 1.290286\n",
            "Train Epoch: 15 [32000/38912 (82%)]\tLoss: 1.288222\n",
            "Train Epoch: 15 [33280/38912 (86%)]\tLoss: 1.285628\n",
            "Train Epoch: 15 [34560/38912 (89%)]\tLoss: 1.251733\n",
            "Train Epoch: 15 [35840/38912 (92%)]\tLoss: 1.273447\n",
            "Train Epoch: 15 [37120/38912 (95%)]\tLoss: 1.254715\n",
            "Train Epoch: 15 [38400/38912 (99%)]\tLoss: 1.272364\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet sulhtond soavid atain.t the caselyatisl, \"I tm solr  .  \"e said  \"Ion't tanry  Ior,\" said Harry,ss tnce. andious lsout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I dan'tlparate tp tyyh aeck t.. . .on't tanry  I . .  \"I tm sot tarryed \"Iarry,  said Humbledore, wis foice sssottle staeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration wad boooun humbledore wn ecltltged hhe  he hai tis hace  wase ,and soyp dathat tveryan the cosaant oakht af t ctaeet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lele oooas ng tlon typaissed d aith aeogons  \"I hai you tnparate tn t was trtling ty das oom wariains \"Ihetksyood ess  yhat syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneid a   snd Hheugh he wairgeded ttlottle  ae saemed tia ei bn tonpondetf the ftgtation  aah ceed toynsfirtet-wuoomsta  \"I me go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t an the ceght tnr  \" d tarry  whok aookid atead an the ctynl  and salr aheelid an ide tis aoke a ser t ts cesble  aauiletsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he sei  ri  ware coaprved  \"he soor wo the ftiral staircase toat hoa tack tn o the castle tis aaesed  \"he e was aotse\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tiw tt wad badpened t \"rcelliarias has aot t bled isg taarm a \"he   wu the faght tf the firk  wa hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3217, Accuracy: 751168/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/015.pt\n",
            "\n",
            "generated sample\t Harry Potter's stop of year for TABL Er. \"I'd belk, now as sure yeh knows after infour, though immeriali!\" however. \"Ohrought we've got that I sapped ... that though She soaved Rarkis. There didn't state that thi\n",
            "generated sample\t Harry Potter. The toward glimms ting to Mondom. Potions again from waiting. why gove he was and vast her glass.\" \"Ni, your firs'\" said Moody, was said staffing into the Harry countal to his coby blamed with each \n",
            "generated sample\t Harry Potter. You, no dear, who went almost say. Rad tomory y've look to be a your for Hewlams boks. Shat alarm across like they want, when Nirch!\" said Harry. \"You'd be able to be giant pointed! Ivap with halpin\n",
            "generated sample\t Harry Potter-bouk had personal night, he possed by his shouldags' something saw and Potter had gent, anythiuve, he was a glass bagg. I had to see who itq dispprabing that it is a goldemon's room, and anyone along\n",
            "generated sample\t Harry Potter! \"I mean - Boling, you like you chair,\" said Harry. Harry couldn't look in their Dapka, wide nothing. The hand pain in persuation cumboared looking near. \"he was conything. I qust not know. recarry,\"\n",
            "generated sample\t Harry Potter, who had an leach and particularly jockeated. \"You've done prelaws.\" \"Oeah,\" said dow, shattings from the mapboarating gap in her hands lunclabag. \"In umbredles Petrorcemerby!\" he said, \"Haven you do\n",
            "generated sample\t Harry Potter too, Duan't be forgotten into the cardic from Glytherin at the holidays. When the blood wished and a loo voice. A last to said by hear everyon. \"What you as know-y'h insaying those woman?) exper,\" sa\n",
            "generated sample\t Harry Potter in the hos on \"OMM0NANHS1N!\" Bit in the voice for morely. \"Sory-\" \"Look-boy, Hagrid, all went places,\" said Harry sharly in her marmacted.\" \"it's everyone!\" as though he could thought understone hold\n",
            "generated sample\t Harry Potter talking himself across the egg. \"If ' said he looked at though surrounding his mage to be surely, hat going too mike to adulating with its shally was of place, who was half to be platten its places.\"\n",
            "generated sample\t Harry Potter, though is something by so that something to Hagrid ' for a most ear, clumbning, and then hurried acrosy turningly sterum. \"Ers', you've been deter\"to go to do ashe, they re6orved anything.\" \"So Bord\n",
            "generated beam\t\t Harry Potter, for him. \"He would not peokle's don't yeve below her:'s saying, have gone colse to Dokin' I miss he thought... you're though she wasn't chreates andles, you'd so a, Fleveons my Geory realiae,\" said \n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 16 [0/38912 (0%)]\tLoss: 1.345430\n",
            "Train Epoch: 16 [1280/38912 (3%)]\tLoss: 1.249161\n",
            "Train Epoch: 16 [2560/38912 (7%)]\tLoss: 1.272413\n",
            "Train Epoch: 16 [3840/38912 (10%)]\tLoss: 1.309957\n",
            "Train Epoch: 16 [5120/38912 (13%)]\tLoss: 1.274189\n",
            "Train Epoch: 16 [6400/38912 (16%)]\tLoss: 1.247620\n",
            "Train Epoch: 16 [7680/38912 (20%)]\tLoss: 1.270217\n",
            "Train Epoch: 16 [8960/38912 (23%)]\tLoss: 1.252638\n",
            "Train Epoch: 16 [10240/38912 (26%)]\tLoss: 1.273436\n",
            "Train Epoch: 16 [11520/38912 (30%)]\tLoss: 1.252296\n",
            "Train Epoch: 16 [12800/38912 (33%)]\tLoss: 1.244453\n",
            "Train Epoch: 16 [14080/38912 (36%)]\tLoss: 1.295182\n",
            "Train Epoch: 16 [15360/38912 (39%)]\tLoss: 1.255249\n",
            "Train Epoch: 16 [16640/38912 (43%)]\tLoss: 1.245222\n",
            "Train Epoch: 16 [17920/38912 (46%)]\tLoss: 1.260224\n",
            "Train Epoch: 16 [19200/38912 (49%)]\tLoss: 1.250059\n",
            "Train Epoch: 16 [20480/38912 (53%)]\tLoss: 1.272337\n",
            "Train Epoch: 16 [21760/38912 (56%)]\tLoss: 1.261860\n",
            "Train Epoch: 16 [23040/38912 (59%)]\tLoss: 1.281319\n",
            "Train Epoch: 16 [24320/38912 (62%)]\tLoss: 1.265852\n",
            "Train Epoch: 16 [25600/38912 (66%)]\tLoss: 1.262371\n",
            "Train Epoch: 16 [26880/38912 (69%)]\tLoss: 1.219949\n",
            "Train Epoch: 16 [28160/38912 (72%)]\tLoss: 1.277024\n",
            "Train Epoch: 16 [29440/38912 (76%)]\tLoss: 1.282062\n",
            "Train Epoch: 16 [30720/38912 (79%)]\tLoss: 1.286272\n",
            "Train Epoch: 16 [32000/38912 (82%)]\tLoss: 1.283413\n",
            "Train Epoch: 16 [33280/38912 (86%)]\tLoss: 1.281757\n",
            "Train Epoch: 16 [34560/38912 (89%)]\tLoss: 1.247641\n",
            "Train Epoch: 16 [35840/38912 (92%)]\tLoss: 1.268541\n",
            "Train Epoch: 16 [37120/38912 (95%)]\tLoss: 1.251252\n",
            "Train Epoch: 16 [38400/38912 (99%)]\tLoss: 1.268902\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet sulhtond soavid atain.t the caselyaaisl, \"I tm solr  .  \"e said  \"Ion't tanry  Ior,  said Harry,ss hnce. \"ndious lsout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I dan'tlparate tp tyyh aeck t.. . .on't tanry  I . .  \"I tm sot tarryed \"Iarry,  said Humbledore, wis foice sssattle shaeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration wad boooun humbledore wn  cltlnged hhe  he hai tis hace  wase ,and soyp dathat tveryan the cosaant oakht af t ltaaet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lele oooas ng tlon typaiss d d aith aeagons  \"I hhi you wlparate tn t was trtling ty das oom warsains \"Ihetksyood ess  yhat syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneid a   snd Hheugh he wairgeded ttlottle  ae satmed tae ei an tanpondeaf the ctgtation  aah coed toynsfirtet-wuoomsta  \"I me go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t.an the ceght tnr  \" d tarry  whok aookid atead an the ctinl  and salr aheelid an ide tis aoke a ler m ts besble  aauiletsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he set  ri  ware coaprved  \"he soor wh the ftital staircase toat hoa tack tn o the castle tis aaesed  \"he e was aotse\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tew tt wad badpened t-\"rcelliar as has aot t bled isg taarm a \"he   wu the saght tf the filk  wa hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3188, Accuracy: 752206/1245184 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/016.pt\n",
            "\n",
            "generated sample\t Harry Potter. Moody was worried, \" so though whisperva attepping out feel vanger carses fours well. He raised Harry, wikay, a wearing coak clutchingly. Did sould Harry thought him to done us. \"You couldn't blim b\n",
            "generated sample\t Harry Potter. A great grass glass as leanting, Harry told, it was a lotter could wish nothing on them were alizish in Dumor Potion Against the dests mome... insaying bekond making....\" \"And you wooddn't hope here\n",
            "generated sample\t Harry Potter, who stood up. All with ninch about to take there in Harry's thunday, packain. \"W he got to boar cleakly, want Moody's family!\" \"Here you have just been warmed to do in through the sungrul past memby\n",
            "generated sample\t Harry Potter, was your ash sitting carefucty jormith, buries &aggers to have more last - contralding in the Daily Dumbledore starting to his way. \"An:oping right?\" \"What's come to busy roared to the rest that to \n",
            "generated sample\t Harry Potter, Harry had been from 6eava, what beward he done snatched at them. Harry applause how car that Harry knowlemmed about the first tea, and class begmometch. Dobby said anyther. quiched a portrait soakin\n",
            "generated sample\t Harry Potter hard, so made by the Pelto Thoush kno7led as he was sounh. He hadn't been upendumed. \"Bino a whole, you who pall to you? one's nothing as it was tim you to forget over the Duybberfor dambing Slytheri\n",
            "generated sample\t Harry Potter a thoughthough back on them. Snape's lays hat been out. Avinus o' So would be stary over frod those pleasant teams from that never use by nowbed by the luck, and were just battled. SIVelli' had door \n",
            "generated sample\t Harry Potter. He was saydwingly in his room. \"What was you would know who was not make to be a stair? he'll have back to tell -- Voabbe, nonestribe chees downsty aware rym room. of nose that again about the Pormb\n",
            "generated sample\t Harry Potter. \"KIVI we goin' a Blic -\" \"- whath that is what A caught pulling it father.\" he was looking quieched, inquivering to Hed, but became useful, you was not guilty be set if only not been promping it was\n",
            "generated sample\t Harry Potter for security Sheet; TSpetting give Habry pressibled, Harry was before without just knocked by down to find ignore what they had said not. Potter to half! Ron and George drobbed holeming to the watch.\n",
            "generated beam\t\t Harry Potter? I smight alsow--\" Vinus McGonagall was still handredmes, Hagrid, and glasses at though the tables Snape, woph carrying. Harry picked up a too foot. HErry was though called and looked against the upb\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 17 [0/38912 (0%)]\tLoss: 1.343913\n",
            "Train Epoch: 17 [1280/38912 (3%)]\tLoss: 1.245254\n",
            "Train Epoch: 17 [2560/38912 (7%)]\tLoss: 1.266585\n",
            "Train Epoch: 17 [3840/38912 (10%)]\tLoss: 1.304715\n",
            "Train Epoch: 17 [5120/38912 (13%)]\tLoss: 1.269102\n",
            "Train Epoch: 17 [6400/38912 (16%)]\tLoss: 1.244311\n",
            "Train Epoch: 17 [7680/38912 (20%)]\tLoss: 1.264672\n",
            "Train Epoch: 17 [8960/38912 (23%)]\tLoss: 1.248078\n",
            "Train Epoch: 17 [10240/38912 (26%)]\tLoss: 1.271509\n",
            "Train Epoch: 17 [11520/38912 (30%)]\tLoss: 1.250138\n",
            "Train Epoch: 17 [12800/38912 (33%)]\tLoss: 1.242560\n",
            "Train Epoch: 17 [14080/38912 (36%)]\tLoss: 1.291734\n",
            "Train Epoch: 17 [15360/38912 (39%)]\tLoss: 1.251574\n",
            "Train Epoch: 17 [16640/38912 (43%)]\tLoss: 1.240192\n",
            "Train Epoch: 17 [17920/38912 (46%)]\tLoss: 1.254687\n",
            "Train Epoch: 17 [19200/38912 (49%)]\tLoss: 1.246084\n",
            "Train Epoch: 17 [20480/38912 (53%)]\tLoss: 1.266560\n",
            "Train Epoch: 17 [21760/38912 (56%)]\tLoss: 1.258792\n",
            "Train Epoch: 17 [23040/38912 (59%)]\tLoss: 1.276816\n",
            "Train Epoch: 17 [24320/38912 (62%)]\tLoss: 1.261147\n",
            "Train Epoch: 17 [25600/38912 (66%)]\tLoss: 1.256784\n",
            "Train Epoch: 17 [26880/38912 (69%)]\tLoss: 1.215869\n",
            "Train Epoch: 17 [28160/38912 (72%)]\tLoss: 1.272743\n",
            "Train Epoch: 17 [29440/38912 (76%)]\tLoss: 1.279021\n",
            "Train Epoch: 17 [30720/38912 (79%)]\tLoss: 1.282139\n",
            "Train Epoch: 17 [32000/38912 (82%)]\tLoss: 1.280786\n",
            "Train Epoch: 17 [33280/38912 (86%)]\tLoss: 1.277805\n",
            "Train Epoch: 17 [34560/38912 (89%)]\tLoss: 1.242337\n",
            "Train Epoch: 17 [35840/38912 (92%)]\tLoss: 1.263916\n",
            "Train Epoch: 17 [37120/38912 (95%)]\tLoss: 1.247456\n",
            "Train Epoch: 17 [38400/38912 (99%)]\tLoss: 1.266788\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet sulhtond soanid atain.t the caselyaaisl, \"I tm solr  .  \"e said  \"Ion't tarry  Ior,  said Harry,ss hnce. \"ndious lsout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I dan'tlparate tp tyyh aeck t.. . .on't tanry  I . .  \"I tm sot tarryed  Iarry,  said Humbledore, sis foice snsattle shaeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration wad boooun humbledore wn  cltlnged hhe  he hai tis hace  wase ,and sorp dathat tverytn the cosaant oakht af t ltaaet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t ltle ooaas ng tlon tvpaiss d d aith aeagons  \"I hhi you wlparate tn t was trtling ty das oom warsain  \"Ihetksyood ess  yhat syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneid a   snd Hheugh he wairgeded atlottle  aa satmed tae ei an tanpondeaf the ctgtation  aah hoed toynsfirtet-wuoomsta- \"I me go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t.an the ceght tnr  \" d tarry  wh , aookid atead an the ctinl  and salr aheelid an ide tis aoke a ler t ts besble  aauiledsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he set  ri  ware boaprted  \"he soor wh the ftital staircase that hoa tack tn o the castle tis aaesed  \"he e was aotse\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tew tt wad badpened t-hrcelliar as has aot t bled isg taarm a \"he   wu the saght tf the falk  wa hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3148, Accuracy: 753455/1245184 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/017.pt\n",
            "\n",
            "generated sample\t Harry Potter was at the 5ake. Ron and jiss shot to the fres' glass week and rush that before he had seem to reasant, that what he caught in rubybed into Hogwarts he near like nearly they littled above. \"Net, you \n",
            "generated sample\t Harry Potter. I wonder, was shy making that approvect. I'm nothing imarin, I'm spitching you to busy Hagrid's done, let moon, making. Ap as body can a bree by to Profess Hagrid alto that righten with him. Uncle V\n",
            "generated sample\t Harry Potter. He was Satch. \"That did you go in the elf, how to refund you're lyin' to fing there! so that's all those making woman he was daughing?\" He had returned at them. \"Snappose,\" said Blaci. \"They can do \n",
            "generated sample\t Harry Potter, \"I don't do bluD I contack your moment dumby like D amone exacse you, he did?\" said Harry to Cate he twosted up and Gred. 'Mood, hopewher?\" said Hagrid. \"Moody's usuals back funptickly, for it to br\n",
            "generated sample\t Harry Potter, particulusing. \"And monstes it before -- would want about a hundred double elp? I was a good to school laugh.\" \"Sturged to yeh... all flying in my floor in the AprarL yeh those coat,\" \"Dumb-you thin\n",
            "generated sample\t Harry Potter puickly do and imagine you were surtaing... Who weare in a strugg of Harry hadn't be last, that Ichousaits heaven's sbeefly were bying talking back in a froge last bowler.\" \"But No my singon five in \n",
            "generated sample\t Harry Potter kestwyed. And that the windons resty-seemed gaze on his lasts tumbled and fire becapse. \"Goport-s firs, he's not very word, that we'v stood us for hero allended, haven couldn't look.\" \"I'm have bogs,\n",
            "generated sample\t Harry Potter, what was on the beft tashes went to thin. . . . you see School suchowed her sib. \" \"What dast home?\" Halled Harry. \"That we this bark.... nome on, who'V Bubb's the Dumbledores jet downwards on o'. W\n",
            "generated sample\t Harry Potter immed,\" said Harry, is your late, But Hermione had Potter Pettioned a worry. And non, was an an up to his every great fear as unklacently resolumoned, Professor Hermione cautled through the roas: tha\n",
            "generated sample\t Harry Potter. \"No visit,\" said Harry, was justinguing, \"Narkes, king?\" yeah. Ron struggled at the last tawe in Hogwarts. Sarry a sweat imp that /umbled painting in his bout. \"I relion, said he was a school Trelac\n",
            "generated beam\t\t Harry Potter,\" maked at othlegs, a tapp Professi Dubbledore, who beg them. \"Nope to mean them!' said Hagrid. \"Harry.\" \"his infirmily foom... Other?\" he banged. \"Harry, you very going to real them. He is sort.\" \"T\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 18 [0/38912 (0%)]\tLoss: 1.342404\n",
            "Train Epoch: 18 [1280/38912 (3%)]\tLoss: 1.241392\n",
            "Train Epoch: 18 [2560/38912 (7%)]\tLoss: 1.262320\n",
            "Train Epoch: 18 [3840/38912 (10%)]\tLoss: 1.300258\n",
            "Train Epoch: 18 [5120/38912 (13%)]\tLoss: 1.264341\n",
            "Train Epoch: 18 [6400/38912 (16%)]\tLoss: 1.241718\n",
            "Train Epoch: 18 [7680/38912 (20%)]\tLoss: 1.261551\n",
            "Train Epoch: 18 [8960/38912 (23%)]\tLoss: 1.245739\n",
            "Train Epoch: 18 [10240/38912 (26%)]\tLoss: 1.266884\n",
            "Train Epoch: 18 [11520/38912 (30%)]\tLoss: 1.246798\n",
            "Train Epoch: 18 [12800/38912 (33%)]\tLoss: 1.239062\n",
            "Train Epoch: 18 [14080/38912 (36%)]\tLoss: 1.288215\n",
            "Train Epoch: 18 [15360/38912 (39%)]\tLoss: 1.248426\n",
            "Train Epoch: 18 [16640/38912 (43%)]\tLoss: 1.236262\n",
            "Train Epoch: 18 [17920/38912 (46%)]\tLoss: 1.250052\n",
            "Train Epoch: 18 [19200/38912 (49%)]\tLoss: 1.243944\n",
            "Train Epoch: 18 [20480/38912 (53%)]\tLoss: 1.264649\n",
            "Train Epoch: 18 [21760/38912 (56%)]\tLoss: 1.255158\n",
            "Train Epoch: 18 [23040/38912 (59%)]\tLoss: 1.271460\n",
            "Train Epoch: 18 [24320/38912 (62%)]\tLoss: 1.254593\n",
            "Train Epoch: 18 [25600/38912 (66%)]\tLoss: 1.251115\n",
            "Train Epoch: 18 [26880/38912 (69%)]\tLoss: 1.212074\n",
            "Train Epoch: 18 [28160/38912 (72%)]\tLoss: 1.270103\n",
            "Train Epoch: 18 [29440/38912 (76%)]\tLoss: 1.276366\n",
            "Train Epoch: 18 [30720/38912 (79%)]\tLoss: 1.277986\n",
            "Train Epoch: 18 [32000/38912 (82%)]\tLoss: 1.277356\n",
            "Train Epoch: 18 [33280/38912 (86%)]\tLoss: 1.275842\n",
            "Train Epoch: 18 [34560/38912 (89%)]\tLoss: 1.239229\n",
            "Train Epoch: 18 [35840/38912 (92%)]\tLoss: 1.261488\n",
            "Train Epoch: 18 [37120/38912 (95%)]\tLoss: 1.244482\n",
            "Train Epoch: 18 [38400/38912 (99%)]\tLoss: 1.263560\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet sulhtond soanid atain.t the caseryaaisl, \"I wm solr  .  \"e said  \"Ion't tarry  yor,  said Harry,ss hnce. \"ndious lsout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I dan'tlparate tp tyyh aeck t.. . .un't tarry  I . .  \"I tm sot tarryed  aarry,  said Humbledore, sis foice snsattle shaeng r\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration wad boooun humbledore tn  cltlnged hhe  he hai tis hace  wase iand serp dethat tverytn the cosaant oakht af t ltaaet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lele oaaaseng tlon tvpaiss d d aith aeagons  \"I hhi you wlparate tn t was artling te das oom warsain  \"Ihetksyood ess  ahat syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneid a   snd Hheugh he wairgeded atlottle  aa satmed tae ei an tanpondean the ctgtation  aah hoed toynsfirtet-wuoomsta- \"I me go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t.an the ceght tnr  \" d tarry  wh , aookid atead an the ctinl  and salr aheelid an ide tis aoke a ler tets besble  aauiledsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he set  ri  ware bosprted  \"he soor wh the ftidal staircase that haa tack tn o the castle tis aaesed  \"he e was aotsu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tew tt wad badpened t-hrcecliar as has aot t bled isg taarm a \"he   wu the saght tf the falk  wa hai tumbledore s hand aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3110, Accuracy: 754332/1245184 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/018.pt\n",
            "\n",
            "generated sample\t Harry Potter, Harry felt that remember, Hermione might-nothing realixing, at them as his mon by them. .... The thiuscas said, \"Sh detantop? Ron's glass wen' there,\" chairled Farket. Harry was dart rather for bed,\n",
            "generated sample\t Harry Potter, Hagrid's full tonard from that a lot learned. Look back in peritimible half in look. He fell as it is out towagring at zapy with dancers as though from class sepioned a glass shatting for them stand\n",
            "generated sample\t Harry Potter. \"Potter, yeh says im that!\" said Hagrid droppingly. \"Handles!\" \"I'm aloor S9?\" Uncle Percy, but Harry said to harv that he in the onless these crobs behind him, pactain back, walking lay down the ap\n",
            "generated sample\t Harry Potter? . . . but mentade, Harry in thex, caughing them looking at her, uncauldable, carefully's sake into the barding. \"They at the Dary Portrait Balf and lay door celboning. Hagrid smoop it. Against furio\n",
            "generated sample\t Harry Potter-case. The Quable has called down fevily last by nearber. Ron, she is did, he had said the clasb, helping as though he purched fire eyelish in the frum sitting from it last right. \"I'm fit, it as -\" c\n",
            "generated sample\t Harry Potter, followed now, and a hundred of direlpansident. But the Apharat- Cood cauldron, didn't suffacle something to help him to piant ; Harry shouldn' sure there were its hotions on Bink. He had a bit self \n",
            "generated sample\t Harry Potter cle becare...\" \"Well care he always right careful, nothing yeh dull, that was you cankt now recordin's... that Brookshands hanging it, you're doing??\" Harry assed black. He handed very can being clas\n",
            "generated sample\t Harry Potter, hadn't boys! The out of the stides blight inside and wizards, handed by moment. \"I supposed to Pavage,\" said Harry began in auror as though shortly. \"What can't know him, Wild-yon, wait,\" shooking d\n",
            "generated sample\t Harry Potter, whos eyes had meet hanging in resom. Futter boanlisely up father. \"Rise, Alicious,\" said Rarried, almastic in the walls, as thoo was Malfoy catching her wand and litter a hand clambed onto the cavag\n",
            "generated sample\t Harry Potter, and she moved in a hot to gup at Hagrid. \"If you,\"it's knother tonight Changer,\" said Hagrid'sly. \"I'm not sure Gon't yeh night you retorced into your moan have not to stair.\" And Harry enough is as\n",
            "generated beam\t\t Harry Potter is movent in a bitings on a wall. \"Go, don't you hear her and southing then,\" reachered Harry whose felling it. \"Rise, dun's nobone!\" George said to remmoving. \"Nothing too people becaumenstache, wro\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 19 [0/38912 (0%)]\tLoss: 1.340762\n",
            "Train Epoch: 19 [1280/38912 (3%)]\tLoss: 1.237410\n",
            "Train Epoch: 19 [2560/38912 (7%)]\tLoss: 1.258427\n",
            "Train Epoch: 19 [3840/38912 (10%)]\tLoss: 1.297379\n",
            "Train Epoch: 19 [5120/38912 (13%)]\tLoss: 1.258642\n",
            "Train Epoch: 19 [6400/38912 (16%)]\tLoss: 1.239665\n",
            "Train Epoch: 19 [7680/38912 (20%)]\tLoss: 1.262343\n",
            "Train Epoch: 19 [8960/38912 (23%)]\tLoss: 1.241971\n",
            "Train Epoch: 19 [10240/38912 (26%)]\tLoss: 1.262812\n",
            "Train Epoch: 19 [11520/38912 (30%)]\tLoss: 1.246481\n",
            "Train Epoch: 19 [12800/38912 (33%)]\tLoss: 1.236672\n",
            "Train Epoch: 19 [14080/38912 (36%)]\tLoss: 1.284972\n",
            "Train Epoch: 19 [15360/38912 (39%)]\tLoss: 1.245173\n",
            "Train Epoch: 19 [16640/38912 (43%)]\tLoss: 1.233512\n",
            "Train Epoch: 19 [17920/38912 (46%)]\tLoss: 1.246722\n",
            "Train Epoch: 19 [19200/38912 (49%)]\tLoss: 1.240419\n",
            "Train Epoch: 19 [20480/38912 (53%)]\tLoss: 1.261487\n",
            "Train Epoch: 19 [21760/38912 (56%)]\tLoss: 1.253395\n",
            "Train Epoch: 19 [23040/38912 (59%)]\tLoss: 1.266672\n",
            "Train Epoch: 19 [24320/38912 (62%)]\tLoss: 1.250000\n",
            "Train Epoch: 19 [25600/38912 (66%)]\tLoss: 1.245029\n",
            "Train Epoch: 19 [26880/38912 (69%)]\tLoss: 1.209389\n",
            "Train Epoch: 19 [28160/38912 (72%)]\tLoss: 1.268423\n",
            "Train Epoch: 19 [29440/38912 (76%)]\tLoss: 1.273640\n",
            "Train Epoch: 19 [30720/38912 (79%)]\tLoss: 1.273708\n",
            "Train Epoch: 19 [32000/38912 (82%)]\tLoss: 1.270863\n",
            "Train Epoch: 19 [33280/38912 (86%)]\tLoss: 1.274569\n",
            "Train Epoch: 19 [34560/38912 (89%)]\tLoss: 1.237823\n",
            "Train Epoch: 19 [35840/38912 (92%)]\tLoss: 1.260226\n",
            "Train Epoch: 19 [37120/38912 (95%)]\tLoss: 1.242086\n",
            "Train Epoch: 19 [38400/38912 (99%)]\tLoss: 1.259343\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet sulhtond soanid atain.t the caseryaaisli \"I wm nolr  .  \"e said  \"Ion't barry  yor,  said Harry,ss hnce. \"ndious lsout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . I dan'tlparate tp tyyh aeck t.. . .un't tarry  I . .  \"I hm not tarryed \"aarry,  said Humbledore, sis foice snsattle shaeng d\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration wad boooun humbledore tn  cltlnged hhe  he hai tis bace  hase iand serp dethat tverytn the cosaant oakht tf t ctaaet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t lele oaaaseng tlln tvpaiss d d aith aeagons  \"I hhi you wnparate tn t was artling te das oom warrain  \"Ihetksyood ess  ahet syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneid a   snd Hheugh he wairgeded atlottle  aa satmed tae ei an tanpondetn the ctdtation  aah hoed toynsfirtet-wuoomsta- \"I me go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t.an the ceght tnr  \" d tarry  wh , aookid atead an the ctinl  and selr aheelid an ide tis aoke a ler tets besble  aauiledsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he sei  ri  ware bosprted  Hhe soor wh the ftidal statrcase that hoa tack tn o the castle tis aaesed  \"he e was aotlu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tew tt wad badpened t-\"rceclinryas has aot t bled isg taarm a \"he   wu the baght tf the firk  wa hai tumbledore s band aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3070, Accuracy: 755785/1245184 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/019.pt\n",
            "\n",
            "generated sample\t Harry Potter, descentable twoulin at a room on a last, then not supported his lead up as though prouded looking very one launtic where he was sparing very though the rest of them were carrying out how thather he \n",
            "generated sample\t Harry Potter, arour apparen't have told there. If all they can clear rathing those he had all remaired the for Harry's fact, and it, appears. When helped hidding, as they would look to make alrey conceally, Harry\n",
            "generated sample\t Harry Potter Appelained, Ron. \"I as it couldn't be doward you all o,\" said Hagrid, she vust draffing his hanged and wearing all that Profes had answed toward the last, laughing, a loubling Hald and Quilling Hendi\n",
            "generated sample\t Harry Potter ray fro. He had hummed on her strange through his handstimy longs, then, well, in the Cedric lay out hurtly into fire, haven't imagined was between the Dumbledore -\" \"be a good sudre for Ron's, other\n",
            "generated sample\t Harry Potter... he did not well very favate expelled on the blac? He'd been a very emerged in chair he had been dead in the steps of excitement scabbise last quick -\" \"He's samed.\" \"You want to tib Dumbledore pas\n",
            "generated sample\t Harry Potter, pupposed him and his mouth of the boaks and started to take one first, so that Samed Harry, hadn't haven half-bling , he had half for bagns up returning at besive imseasabled. He was wrong hearths, \n",
            "generated sample\t Harry Potter. I wearned a combret color in her chair beferiffility mea; guistin' he, Sirius and Weasley-haddled and louded looging about the pear in the Ppraut's beside Grimma bow his baste assuring about emorges\n",
            "generated sample\t Harry Potter and there last by feet liksing a pump or and like very said. \"You was relief!\" \"So... Potter,\" said Gundom Snape's unchedion throsing the parchment floor. Dumbled to reach other held back to their ph\n",
            "generated sample\t Harry Potter, those back hair began, and harry quietly heard by herself and voiced at the soulper few the student for the parwic and lue shot-by clusuin... Harry had fought the memory Chat's hard behind. 9oose ba\n",
            "generated sample\t Harry Potter. Be would go safay to well in top of the room, actually not). \"Dobby's beard night's get face Abracket's quick!\" \"Oh no tell you wonder blame?\" said Dumbledore, was hat listening at Half-y hurried. \"\n",
            "generated beam\t\t Harry Potter. Fred asked the distrabs, wivad had been fock she assmed to Qumjus an, wood of their ompions forced yourdenly and all usualuarly felt any went in a -umbbus and skild. Harry called the other, was much\n",
            "\n",
            "enumerate train   304\n",
            "Train Epoch: 20 [0/38912 (0%)]\tLoss: 1.338212\n",
            "Train Epoch: 20 [1280/38912 (3%)]\tLoss: 1.233948\n",
            "Train Epoch: 20 [2560/38912 (7%)]\tLoss: 1.254334\n",
            "Train Epoch: 20 [3840/38912 (10%)]\tLoss: 1.294809\n",
            "Train Epoch: 20 [5120/38912 (13%)]\tLoss: 1.254363\n",
            "Train Epoch: 20 [6400/38912 (16%)]\tLoss: 1.236069\n",
            "Train Epoch: 20 [7680/38912 (20%)]\tLoss: 1.261524\n",
            "Train Epoch: 20 [8960/38912 (23%)]\tLoss: 1.239244\n",
            "Train Epoch: 20 [10240/38912 (26%)]\tLoss: 1.258862\n",
            "Train Epoch: 20 [11520/38912 (30%)]\tLoss: 1.243930\n",
            "Train Epoch: 20 [12800/38912 (33%)]\tLoss: 1.231865\n",
            "Train Epoch: 20 [14080/38912 (36%)]\tLoss: 1.285744\n",
            "Train Epoch: 20 [15360/38912 (39%)]\tLoss: 1.245099\n",
            "Train Epoch: 20 [16640/38912 (43%)]\tLoss: 1.233490\n",
            "Train Epoch: 20 [17920/38912 (46%)]\tLoss: 1.244607\n",
            "Train Epoch: 20 [19200/38912 (49%)]\tLoss: 1.236557\n",
            "Train Epoch: 20 [20480/38912 (53%)]\tLoss: 1.258207\n",
            "Train Epoch: 20 [21760/38912 (56%)]\tLoss: 1.246094\n",
            "Train Epoch: 20 [23040/38912 (59%)]\tLoss: 1.261177\n",
            "Train Epoch: 20 [24320/38912 (62%)]\tLoss: 1.245997\n",
            "Train Epoch: 20 [25600/38912 (66%)]\tLoss: 1.247176\n",
            "Train Epoch: 20 [26880/38912 (69%)]\tLoss: 1.203849\n",
            "Train Epoch: 20 [28160/38912 (72%)]\tLoss: 1.263914\n",
            "Train Epoch: 20 [29440/38912 (76%)]\tLoss: 1.271120\n",
            "Train Epoch: 20 [30720/38912 (79%)]\tLoss: 1.272008\n",
            "Train Epoch: 20 [32000/38912 (82%)]\tLoss: 1.268263\n",
            "Train Epoch: 20 [33280/38912 (86%)]\tLoss: 1.268084\n",
            "Train Epoch: 20 [34560/38912 (89%)]\tLoss: 1.235169\n",
            "Train Epoch: 20 [35840/38912 (92%)]\tLoss: 1.258520\n",
            "Train Epoch: 20 [37120/38912 (95%)]\tLoss: 1.238870\n",
            "Train Epoch: 20 [38400/38912 (99%)]\tLoss: 1.256728\n",
            "enumerate test    76\n",
            "Input\t great sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Du\n",
            "GT\tgreat sigh and leaned against the cavern wall. \"I am weak...\" he said. \"Don't worry, sir,\" said Harry at once, anxious about Dum\n",
            "pred\tHooet bulhtond soanid atain.t the coseryaaisli \"I wm no r  .\" \"e said  \"Ion't yarry  yor,  said Harry,ss hnce. \"ndious asout tum\n",
            "\n",
            "\n",
            "Input\t... I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronge\n",
            "GT\t.. I can Apparate us both back . . . Don't worry. . . .\" \"I am not worried, Harry,\" said Dumbledore, his voice a little stronger\n",
            "pred\t . y dan'tlparate tp tyyh aeck t.. . .un't tarry  I . .  \"I hm not tarryed \"aarry,  said Humbledore, sis foice snsattle shaang d\n",
            "\n",
            "\n",
            "Input\tparition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlam\n",
            "GT\tarition had thrown Dumbledore off-balance; then he saw his face, paler and damper than ever in the distant light of a streetlamp\n",
            "pred\taration wad boooun humbledore an  cltlnded hhe  he hai tis bace  huse iand serp deahet hveryan the cosaant oakht af t ctaaet yce\n",
            "\n",
            "\n",
            "Input\ta silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank g\n",
            "GT\t silk dressing-gown embroidered with dragons. 'I saw you Apparate as I was pulling my bedroom curtains! Thank goodness, thank go\n",
            "pred\t ltle oaaaseng tlln tvpaiss d d aith aeagons  \"I hhi you wnparate tn t was artling te bas oom wartain  \"Ihetksyood ess  ahet syo\n",
            "\n",
            "\n",
            "Input\tosmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've g\n",
            "GT\tsmerta,' and though he staggered a little, he seemed wholly in command of the situation, 'we need transport - brooms -' 'I've go\n",
            "pred\tneid a   snd Hheugh he wairgeded atlottle  aa hatmed tie ei an tanpondean the ctdtation  aah woed toynsfirtet-wuoomsta- \"I ve go\n",
            "\n",
            "\n",
            "Input\tm in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing h\n",
            "GT\t in the night air. And Harry, too, looked ahead at the skull, and fear swelled inside him like a venomous bubble, compressing hi\n",
            "pred\t.an the ceght anr  \" d harry  wh , aookid atead an the ctinl  and selr aheelid an ide tis aoke a ler tets besble  aauiledsiog te\n",
            "\n",
            "\n",
            "Input\ted around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no s\n",
            "GT\td around. The ramparts were deserted. The door to the spiral staircase that led back into the castle was closed. There was no si\n",
            "pred\td atound  \"he sei  ri  ware besprted  \"he soor wh the ftidal staircase that hoa tack tn o the castle ais aoesed  \"he e was aotlu\n",
            "\n",
            "\n",
            "Input\ttand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyi\n",
            "GT\tand how it had happened - Expelliarmus was not a Freezing Charm - Then, by the light of the Mark, he saw Dumbledore's wand flyin\n",
            "pred\tand tew tt wad badpened t-\"rceclinrdas has aot b bled isg taarm a \"he   wu the baght tf the firk  wa hai tumbledore s band aoain\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.3061, Accuracy: 756276/1245184 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/020.pt\n",
            "\n",
            "generated sample\t Harry Potter, how for a friend-yed soarge, carrying cavelace, forcounding done in. \"M, because you've not hear 3ings by back,\" said Hagrid withoutly. \"Scall could,\" said Hagridged. \"He's changed Dark tetted MyOti\n",
            "generated sample\t Harry Potter. TH1GWE My Thes is Riddle wasn't nothing nowhered. . Harry vanished. Harry had been going, and this boin, jar remort's face. Harry caught unknuck, who was scar loats and more carrying into the back t\n",
            "generated sample\t Harry Potter, but there wasn't put. What dary night after you too make on something very line back in the shape. TI That Boking it, and that the only to help you all he'd the vita Saturnatiff-countable undes Shee\n",
            "generated sample\t Harry Potter, whose Hagicas cat colliched, the reat dead, was could it tolen it. Snape ran in sorries eyed. \"We sut decids pake there's himself here. Harry,\" said Harry, swaylesley. \"Yes,\" said Neatle, slithing i\n",
            "generated sample\t Harry Potter,\" said SuWveg, suffling the elf seemed with them acrosssed. \"Yes, you've gone in broom take on,\" he apkeared at ther, it had watched his house most thear. Harry meant about the pircus Mumb's beak of \n",
            "generated sample\t Harry Potter down)t? Hermione, who were likelizating enow. Oh, Harry felt open at Hagrid's chusing. Then he slid to kiss in. \"What's moves, tions and fine, clats... I be nothing, Won't thn .\" Garket Potter, Learw\n",
            "generated sample\t Harry Potter. \"Ican you?\" \"\" Khather if about his wand on the pocket hand, then therth those wass. \"I haven't sorrable, he mass.\" \"I don't get out how one do'stroll wilh Szamoro We!\" he remonstray, pleased, her h\n",
            "generated sample\t Harry Potter!\" \"No,\" he said ignogatingly. \"It wouldn't,\" said Ron's magic, sticking at a hatter if a bisty than a few to feel fled her templed Scarving Harry scashing as though the reat hall wet his fight burst \n",
            "generated sample\t Harry Potter. \"Harry blash Spotts ...\" \"Lhat's here it massage,\" said Ron headly, each, a hard by the room, were comingtily, though its and one little bign ond his bags flashing deep, and Ron told him and becando\n",
            "generated sample\t Harry Potter. \"We're reing the match?\" His covery dasked down to the dasken into the point, disappinting back to the prophect(ulbed by a large gleam and three at all. \"Hach shelt had been in the comjant conce, th\n",
            "generated beam\t\t Harry Potter didn't wake Professor Wargus), warning, he might have been the other don't have you trinking near. \"Gay in a chambing.\" said Fudge, flashing at Harry. \"Wool,\" said Hermione breathlessly. \"Charch is b\n",
            "\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_1115/020.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r597GUTVjwZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 7: Experiments"
      ]
    },
    {
      "metadata": {
        "id": "zgLylYlp9kBK",
        "colab_type": "code",
        "outputId": "dd4ced5e-7b78-4b55-d6bd-f45a035cdd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        "seed_words = 'Harry Potter and the'\n",
        "sequence_length = 200\n",
        "\n",
        "generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'max')\n",
        "print('generated with max\\t', generated_sentence)\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'sample')\n",
        "    print('generated with sample\\t', generated_sentence)\n",
        "    \n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with max\t Harry Potter and the stairs and the stairs and the stairs and the stairs and the stairs and the stairs and the stairs and the stairs and started to the stairs and the stairs and the stairs and started to the stairs and t\n",
            "generated with sample\t Harry Potter and their side of Falicable was a longed cabiling stuckbing into attempt through Lunable time. Theres that he was nobody hat even his face. So but he finding up whather she lained and Clackward suboophed in \n",
            "generated with sample\t Harry Potter and the two unless, listening Ron to these up. LWASTND That this praces that turning he who loosed to be to the doors all termit. Hermione did, they because ill all a roar wizard that was lit unless they all\n",
            "generated with sample\t Harry Potter and they woaded. Harry stood whiteling the wards on one holding elbown last, which last remove. So Ron'lawn drapq and throw themrely shaped his wacatchesing benwards as it from Balles rattled there was a lat\n",
            "generated with sample\t Harry Potter and the hold with depperation of desk. \"Charm sating somethers? You knows, stifting fured attached the staff faxorate.\" Harry sundoned the little bathroom moments though she was a spread of Dementor against \n",
            "generated with sample\t Harry Potter and the usface several examine hauning words wether a new of them of in his face; they selmed superhair as thousands and Dumbledore in his eag later and looked fronth into it. There was disappoin from both t\n",
            "generated with sample\t Harry Potter and then would hang a School teame, \"He's winthing,\" said Panific a fewt. \"You's all that duml of blackness yew murger, boys. ...NI Metumorohe? Holdomed to remend it to finish it.\" snavely though did not jus\n",
            "generated with sample\t Harry Potter and the lame, Glytwic git all thight and then there were beside Harry's last. Lood sighty fired them, but thinsters of Blythering Snape's parchment in the street of books more, his nob-loosing dres watched i\n",
            "generated with sample\t Harry Potter and they were goiny to do it. SSAd MARE!\" Fred Harry, Sleamythree spells on the third seatures and claps on by a floor raging. .. Yove entorsing Snatchey's desperately stopped him with his face. Anjone an in\n",
            "generated with sample\t Harry Potter and their control run and his chair that had been black eye as it is 'astickles. Uoney's face i can back to Nettig. Tafted a part of course. I'd op, Wormtail,\" Beating a Neaf, grass in its up as he had not h\n",
            "generated with sample\t Harry Potter and the Grouchdays dagger and pulled out his game, \"this famili and parcellays in them down that at Malfoy probatt~ly if thoughtR you're not bubling Vunce's Weakey,\" said Harry and khen waiting headmals. \"So\n",
            "generated with beam\t Harry Potter and the wood gauntin, the gottes entrance we hadn't instructions on a feet for enough him before in plate to a large in students mad the floor and time Harry gate. \"It's a licing O-.\" ASVE UWL SO7TNIN POALUA\n",
            "generated with beam\t Harry Potter and they started for a little wall once that Harry looked lost up at Hermione'. A resilized in whole Sirius shimmed in a letter, though that had been palic and though he to it, Hermione, houring at Scabber; \n",
            "generated with beam\t Harry Potter and the Sorcerical Slower, watching them and then with the si&th Dean only more, and snotched a large knoth-fiftive tiny, and then is long glossey flame last that Urouch Malfoy could give a samily, \" Mr. Wea\n",
            "generated with beam\t Harry Potter and then, why had lastening sended Uncle on, what he was at them to beam. He was should like them that capping it, I ask nothing, it's one so,\" Snape 'F the only o' the Diago W?OSNE SLUIBD I managed it, coul\n",
            "generated with beam\t Harry Potter and the library rattled hait until he stood littly over the celetwory night. So Voldemort teacher around dusty Spready contaction. \"DiPs if you thought it?uess's is I haven't have insteaf!\" she said to even \n",
            "generated with beam\t Harry Potter and they with idea with the wall and preused the room. Harry bank around the front of themself ento his eyes. tall don't unlawny at all what listen down you with her little, then with hometown with Harry in.\n",
            "generated with beam\t Harry Potter and the nicky Pitatus Snape halioned from the dracfly of legs and the beft them. Fred suddenly grees in his sile grovey across the table, Narkarous Caunt Malfoy in the Chindaron, so that Harl Snape had happe\n",
            "generated with beam\t Harry Potter and the shatrilks, the door dishingtly. . . The groan tem, Harry said them out only \"Not and long forehead, catta by all, he'ling?\" said Ron, staring on the said. \"Helle-,\" said His Weorge. \"And you of,\" But\n",
            "generated with beam\t Harry Potter and then mored that they whilens he had been. . ... In front, who was a feet to rather 9up, and we'll don't to it, DecaWU Yon Malfoy's shocked as past anywhere, they was a brom seated by making with Holitely\n",
            "generated with beam\t Harry Potter and thelzed notes as Dumbledore's back hard and though, listending them across whether it was last strained windows. Harry found it to harmbl. We seemed to call the skrew to the desk sime was gonestrall, but\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PDtQDzWqraU4",
        "colab_type": "code",
        "outputId": "617867d4-d011-4e9d-8065-6888909ded3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3105
        }
      },
      "cell_type": "code",
      "source": [
        "logs = pt_util.read_log(LOG_PATH)\n",
        "logs = np.array(logs)\n",
        "print (logs)\n",
        "train_loss = [x[1] for x in logs[0]]\n",
        "test_loss = [x[1] for x in logs[1]]\n",
        "test_accuracy = [x[1] for x in logs[2]]\n",
        "\n",
        "pt_util.plot(np.arange(len(train_loss)), train_loss, \"Plot of training loss wrt epoch\", \"epoch\", \"loss on training data\")\n",
        "pt_util.plot(np.arange(len(test_loss)), test_loss, \"Plot of test loss wrt epoch\", \"epoch\", \"loss on test data\")\n",
        "pt_util.plot(np.arange(len(test_accuracy)), test_accuracy, \"Plot of test accuracy wrt epoch\", \"epoch\", \"accuracy on test data\")\n",
        "\n",
        "pt_util.plot(np.arange(len(train_loss)), np.exp(train_loss), \"Plot of training perplexity wrt epoch\", \"epoch\", \"perplexity on training data\")\n",
        "pt_util.plot(np.arange(len(test_loss)), np.exp(test_loss), \"Plot of test perplexity wrt epoch\", \"epoch\", \"perplexity on test data\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([(0, 2.615988279643812), (1, 1.5152361526301033), (2, 1.4219527887670618), (3, 1.3788129648095684), (4, 1.3566722113050913), (5, 1.342142729382766), (6, 1.330516828126029), (7, 1.3206347678052752), (8, 1.3120185136795044), (9, 1.304102910584525), (10, 1.2973165104263706), (11, 1.2909399129842456), (12, 1.2851747270477445), (13, 1.2797909308420985), (14, 1.2751298907556032), (15, 1.2707312071793957), (16, 1.2665702316321825), (17, 1.2625984682848579), (18, 1.2589899289764857), (19, 1.2555979521651017), (20, 1.2525743399011462)])\n",
            " list([(0, 9.031247440137362), (0, 1.6189051333226656), (1, 1.4924566981039549), (2, 1.4323740225089223), (3, 1.40219341453753), (4, 1.395591770347796), (5, 1.3772209616083848), (6, 1.369945930807214), (7, 1.3563079488904852), (8, 1.3514400171606165), (9, 1.3431635404887952), (10, 1.3389234919297068), (11, 1.3363063758925389), (12, 1.333340894234808), (13, 1.3274213389346474), (14, 1.3240468878495066), (15, 1.321656589445315), (16, 1.3188134698491347), (17, 1.3147966987208317), (18, 1.311005982913469), (19, 1.3069967894177688), (20, 1.3061035746022274)])\n",
            " list([(0, 1.0632966694078947), (0, 52.90663869757401), (1, 56.108735737047695), (2, 57.561613384046055), (3, 58.371614155016445), (4, 58.69156686883224), (5, 58.95369680304276), (6, 59.05512759560033), (7, 59.38568115234375), (8, 59.47121068050987), (9, 59.71334357010691), (10, 59.87203497635691), (11, 59.996675190172695), (12, 60.08397152549342), (13, 60.22009598581415), (14, 60.27478669819079), (15, 60.3258634868421), (16, 60.40922466077303), (17, 60.509531121504935), (18, 60.57996247944079), (19, 60.69665206106085), (20, 60.736083984375)])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAAJbCAYAAABpf2Q8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4a963dc5c0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIUAAAJbCAYAAAB6qCTIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4a9af4e828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIoAAAJbCAYAAACLo39FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgAAAACSCIoAAAAAmCcoAgAAACBJ8v8AgBnPkDbaJmMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4a940bbf28>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIoAAAJbCAYAAACLo39FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4a9611eda0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJUAAAJbCAYAAABdBqUgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4a93fdd5c0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pn0RWPBFjzkP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 8: Other things\n",
        "Choose **two** of the following to try. It will probably be beneficial to create new code cells below rather than modifying your earlier code:\n",
        "\n",
        "\n",
        "1. Train on a different text corpus. The corpus should be at least as large as the provided Harry Potter dataset.\n",
        "    - Options include other books, websites, tweets, wikipedia articles etc.\n",
        "    -  (Hint: this is probably the easiest one)\n",
        "2. Train a model using student-forcing.\n",
        "    - You will have to modify the network inputs.\n",
        "    - You will need to use `torch.nn.GRUCell` and its like. https://pytorch.org/docs/stable/nn.html#grucell\n",
        "    - You cannot simply feed an empty string to start off a sequence. The sequence must be somehow conditioned on prior ground truth.\n",
        "3. Train a model on words instead of characters.\n",
        "    - You will need to redefine your input/output space vocabulary as well.\n",
        "    - You should replace any words that occur less than 5 times in the dataset with an <unknown\\> token. \n",
        "4. Write a new data loader which picks a random point in the text to start from and returns 10 consecutive sequences starting from that point onward. \n",
        "    - You should also modify the train and test functions to reset the memory when you reset the sequence.\n",
        "    - You should consider an epoch to be feeding in approximately the number of characters in the dataset.\n",
        "    - You may run into issues if your dataset size/epochs are not a multiple of your batch size.\n",
        "5. Train on sentences instead of one long sequence.\n",
        "    - You should still produce output character by character.\n",
        "    - Sentences can end with a . ! ?, but words like Mr. generally do not end a sentence.\n",
        "    - A sentence may also continue in the case of quotations. For example: ``\"Do your homework!\" said the TAs.`` is only one sentence.\n",
        "    - Your parsing does not have to be perfect, but try to incorporate as many of these rules as you can.\n",
        "    - Feel free to use existing NLP tools for finding sentence endings. One is spacy: https://spacy.io/usage/linguistic-features#section-sbd\n",
        "    - All sentences should end with an <eos\\> token. Your output sampling should now stop when it produces the <eos\\> token.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vWMlB2U3onZ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 9: Short answer questions\n",
        "Please answer these questions, and put the answers in a file called homework2_python.pdf in your repository.\n",
        "\n",
        "\n",
        "1. Just like last time, provide plots for training error, test error, and test accuracy. Also provide a plot of your train and test perplexity per epoch.\n",
        "    - In class we defined perplexity as `2^(p*log_2(q))`, However the PyTorch cross entropy function uses the natural log. To compute perplexity directly from the cross entropy, you should use `e^p*ln(q)`.\n",
        "    - We encourage you to try multiple network modifications and hyperparameters, but you only need to provide plots for your best model. Please list the modifications and hyperparameters.    \n",
        "    \n",
        "2. What was your final test accuracy? What was your final test perplexity?\n",
        "3. What was your favorite sentence generated via each of the sampling methods? What was the prompt you gave to generate that sentence?\n",
        "4. Which sampling method seemed to generate the best results? Why do you think that is?\n",
        "5. For sampling and beam search, try multiple temperatures between 0 and 2. \n",
        "    - Which produces the best outputs? Best as in made the most sense, your favorite, or funniest, doesn't really matter how you decide.\n",
        "    - What does a temperature of 0 do? What does a temperature of 0<temp<1 do? What does a temperature of 1 do? What does a temperature of above 1 do? What would a negative temperature do (assuming the code allowed for negative temperature)?\n",
        "    \n",
        "Questions for each of the \"Other things\" sections. Only answer the questions corresponding to the ones you chose.\n",
        "\n",
        "1. New Corpus\n",
        "    1. What corpus did you choose? How many characters were in it?\n",
        "    2. What differences did you notice between the sentences generated with the new/vs old corpus.\n",
        "    3. Provide outputs for each sampling method on the new corpus (you can pick one temperature, but say what it was).\n",
        "\n",
        "2. Student-forcing\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. Were the results better than with teacher-forcing?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was).\n",
        "    \n",
        "3. Words\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. How large was your vocabulary?\n",
        "    3. Did you find that different batch size, sequence length, and feature size and other hyperparameters were needed? If so, what worked best for you?\n",
        "\n",
        "4. Random Dataloader\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. Were the results better than with the original dataloader?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was). \n",
        "    \n",
        "5. Sentences\n",
        "    1. What new difficulties did you run into while training? What new difficulties did you run into while preprocessing?\n",
        "    2. Were the results better than with the original dataloader?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was). \n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "AAkZu1MVdSuL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#NEW CORPUS"
      ]
    },
    {
      "metadata": {
        "id": "FR820C_S26Hs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Preprocessing the data"
      ]
    },
    {
      "metadata": {
        "id": "cwj16vBpvRLV",
        "colab_type": "code",
        "outputId": "8b95a5b2-ff5a-44bf-e4e4-d568be5721ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import glob\n",
        "def prepare_data(file):\n",
        "    with open(file) as f:\n",
        "            # This reads all the data from the file, but does not do any processing on it.\n",
        "          data = f.read()\n",
        "    \n",
        "    # TODO Add more preprocessing\n",
        "    data = re.sub('\\s+', ' ', data).strip()\n",
        "\n",
        "    \n",
        "    voc2ind = {}\n",
        "    \n",
        "    # Compute voc2ind and transform the data into an integer representation of the tokens.\n",
        "    count = 0\n",
        "    list_indexed = []\n",
        "    for char in data:\n",
        "        if char not in voc2ind.keys():\n",
        "            voc2ind[char] = count\n",
        "            count += 1\n",
        "        list_indexed.append(voc2ind[char])\n",
        "        #pass # TODO Fill this in\n",
        "    print (len(voc2ind.keys()))\n",
        "    \n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "    \n",
        "    data_indexed = np.array(list_indexed)\n",
        "    list_indexed = []\n",
        "    print (data_indexed.shape)\n",
        "    print (data_indexed.shape[0])\n",
        "    \n",
        "    split_index = math.ceil((4 * data_indexed.shape[0]) / 5)\n",
        "    print (split_index)\n",
        "    train_text = data_indexed[:split_index] \n",
        "    # TODO Fill this in\n",
        "    test_text = data_indexed[split_index:]\n",
        "    # TODO Fill this in\n",
        "    \n",
        "    print (train_text.shape, test_text.shape)\n",
        "    \n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'wikitext_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'wikitext_chars_test.pkl', 'wb'))\n",
        "    \n",
        "    return voc2ind\n",
        "    \n",
        "voc2ind = prepare_data(DATA_PATH + '/wikitext/' + 'wiki.train.raw')\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1012\n",
            "(10845406,)\n",
            "10845406\n",
            "8676325\n",
            "(8676325,) (2169081,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-27ThigX2-an",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "p2vl1QDCJxt8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WikiTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(WikiTextDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "        num = dataset['tokens'].shape[0] - (dataset['tokens'].shape[0] % batch_size)\n",
        "        self.dataset = dataset['tokens'][:num]\n",
        "        # TODO: Any preprocessing on the data to get it to the right shape.\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        chunk_size = math.floor(self.dataset.shape[0] / self.batch_size)\n",
        "        num_examples_batch = math.floor(chunk_size / self.sequence_length)\n",
        "        length = self.batch_size * num_examples_batch \n",
        "        return length\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        # raise NotImplementedError\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence as described above.\n",
        "        # The data and labels should be torch long tensors.\n",
        "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
        "        # in and how far down in the chunk you are.\n",
        "        chunk_size = math.floor(self.dataset.shape[0] / self.batch_size)\n",
        "        within_chunk, chunk_idx = divmod(idx, self.batch_size)\n",
        "        new_idx = chunk_idx * chunk_size + within_chunk * self.sequence_length\n",
        "        last_idx = new_idx + self.sequence_length - 1\n",
        "        data = self.dataset[new_idx:last_idx+2]\n",
        "        data = torch.LongTensor(data)\n",
        "        return data[:-1], data[1:]\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zf7mhYy3Cq_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3: Defining the Network"
      ]
    },
    {
      "metadata": {
        "id": "B2eAyxhJKvwp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 0.5\n",
        "\n",
        "class WikiTextNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size):\n",
        "        super(WikiTextNet, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        self.gru = nn.GRU(self.feature_size, self.feature_size, num_layers=2, batch_first=True)\n",
        "        self.decoder = nn.Linear(self.feature_size, self.vocab_size)\n",
        "        \n",
        "        # This shares the encoder and decoder weights as described in lecture.\n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        batch_size = x.shape[0]\n",
        "        sequence_length = x.shape[1]\n",
        "        \n",
        "        # TODO finish defining the forward pass.\n",
        "        # You should return the output from the decoder as well as the hidden state given by the gru.\n",
        "        # raise NotImplementedError\n",
        "        x = x.view(-1,1)\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(batch_size, sequence_length, self.feature_size)\n",
        "        x, hidden_state = self.gru(x, hidden_state)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Ej9fwrK3HrC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Character Generation"
      ]
    },
    {
      "metadata": {
        "id": "TApTSYTUd6Eo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BEAM_WIDTH = 20\n",
        "\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "\n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden, temperature=0.5)\n",
        "\n",
        "        outputs = []\n",
        "        # Initializes the beam list\n",
        "        beam = [([], output, hidden, 0)]\n",
        "        for ii in range(sequence_length):\n",
        "            if sampling_strategy == 'max':\n",
        "                # TODO max sampling strategy\n",
        "                # raise NotImplementedError\n",
        "                _, pred_label = torch.max(output, 1)\n",
        "                outputs.append(pred_label)\n",
        "                new_input = pred_label.unsqueeze(1)\n",
        "                output, hidden = model.inference(new_input, hidden, temperature=0.5)\n",
        "                \n",
        "            elif sampling_strategy == 'sample':\n",
        "                # TODO: Probability-based sampling strategy.\n",
        "                # raise NotImplementedError \n",
        "                pred_label = torch.multinomial(output, 1, True)\n",
        "                outputs.append(pred_label)\n",
        "                new_input = pred_label.unsqueeze(1)\n",
        "                output, hidden = model.inference(new_input, hidden, temperature=0.5)\n",
        "                \n",
        "\n",
        "            elif sampling_strategy == 'beam':\n",
        "                # Todo: beam search sampling strategy\n",
        "                # raise NotImplementedError\n",
        "                new_beam = []\n",
        "                for be in beam:\n",
        "                    pred_label = torch.multinomial(be[1], 2, True)\n",
        "                    for p in pred_label[0]:\n",
        "                        out1, hid1 = model.inference(p, be[2], temperature=0.5)\n",
        "                        new_beam.append((be[0]+[p], out1, hid1, be[3]+np.log(be[1][0][p].item())))\n",
        "                new_beam = sorted(new_beam, key=lambda x: x[3], reverse=True)[:beam_width]\n",
        "                beam = new_beam\n",
        "        \n",
        "        if sampling_strategy == 'beam':\n",
        "            outputs = beam[0][0]\n",
        "        \n",
        "        return vocab.array_to_words(seed_words_arr.tolist() + outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzfZdErO3Ltf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 5: Training"
      ]
    },
    {
      "metadata": {
        "id": "OOwnVRJCK2pF",
        "colab_type": "code",
        "outputId": "6b4e8245-027b-4fc7-891e-e47da0f6d874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36583
        }
      },
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 200\n",
        "FEATURE_SIZE = 512\n",
        "TEST_BATCH_SIZE = 256\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.002\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs_corpus/log.pkl'\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "data_train = WikiTextDataset(DATA_PATH + 'wikitext_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = WikiTextDataset(DATA_PATH + 'wikitext_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = WikiTextNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "# Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "# We will talk more about different optimization methods in class.\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints_corpus')\n",
        "\n",
        "train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(model, device, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints_corpus/%03d.pt' % epoch)\n",
        "        seed_words = 'Music and Art'\n",
        "        for ii in range(10):\n",
        "            generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'sample')\n",
        "            print('generated sample\\t', generated_sentence)\n",
        "        generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'beam')\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    model.save_model(DATA_PATH + 'checkpoints_corpus/%03d.pt' % epoch, 0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "Restoring:\n",
            "encoder.weight -> \ttorch.Size([1012, 512]) = 2MB\n",
            "gru.weight_ih_l0 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.weight_hh_l0 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.bias_ih_l0 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.bias_hh_l0 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.weight_ih_l1 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.weight_hh_l1 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.bias_ih_l1 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.bias_hh_l1 -> \ttorch.Size([1536]) = 0MB\n",
            "decoder.weight -> \ttorch.Size([1012, 512]) = 2MB\n",
            "decoder.bias -> \ttorch.Size([1012]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/003.pt\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh e ,f tugust 19 , t000 , The seatdird oluteon oor titgsm  ,es seleased on tuly 10 , t000 , tonsri\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tenaae , , whs seleased on tevember 10 , 1000 , The sinth snd einal setel o thtle  itiwn, Ttmt ,erron\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tsril 10 ,nd 1ane .0 , t000 , The ser o-@ cecetctioobliched ,ocart roecaase  onsompn sndiologi o whtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tliched i ahe sinst so @ilumenoas seleased oy tasee .f Ausust 10 , 1000 , toruseng tf tenp ar,one    \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty oemdioome ,f t0   ,f terth r h  , wnsonor oecueteacutirs,f tecial setel .nd tewis ic .ndme oaovuce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tsncisted bn o the sn   .anl ,f tete ,n 1000 . T = =nrly 1ane , = =ansioaners ,as s soaateeaeo-@ coml\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd She s9t0 ,henel s ,tate  , The whctoorn shank oecord  , thonk on t0  @erd  , 1   @ ) . wn e \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\tpeesin=oanpions,n s salse ooan tt snt iandeng of the seie so t o an t sevgng oalse on talse ttaw  .n\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\tdiss,haee ,naSs s Tess oaners , s    ,ertsoas saa@an rs , wnsemti yoeatlinn ,etes oy toatiw ianle,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.6188, Accuracy: 1163346/2150400 (54%)\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 3 [0/86600 (0%)]\tLoss: 1.645181\n",
            "Train Epoch: 3 [2000/86600 (2%)]\tLoss: 2.989897\n",
            "Train Epoch: 3 [4000/86600 (5%)]\tLoss: 2.327799\n",
            "Train Epoch: 3 [6000/86600 (7%)]\tLoss: 1.993975\n",
            "Train Epoch: 3 [8000/86600 (9%)]\tLoss: 1.744599\n",
            "Train Epoch: 3 [10000/86600 (12%)]\tLoss: 1.690918\n",
            "Train Epoch: 3 [12000/86600 (14%)]\tLoss: 1.640122\n",
            "Train Epoch: 3 [14000/86600 (16%)]\tLoss: 1.586668\n",
            "Train Epoch: 3 [16000/86600 (18%)]\tLoss: 1.594024\n",
            "Train Epoch: 3 [18000/86600 (21%)]\tLoss: 1.580442\n",
            "Train Epoch: 3 [20000/86600 (23%)]\tLoss: 1.561275\n",
            "Train Epoch: 3 [22000/86600 (25%)]\tLoss: 1.590265\n",
            "Train Epoch: 3 [24000/86600 (28%)]\tLoss: 1.558371\n",
            "Train Epoch: 3 [26000/86600 (30%)]\tLoss: 1.594792\n",
            "Train Epoch: 3 [28000/86600 (32%)]\tLoss: 1.586019\n",
            "Train Epoch: 3 [30000/86600 (35%)]\tLoss: 1.575525\n",
            "Train Epoch: 3 [32000/86600 (37%)]\tLoss: 1.554073\n",
            "Train Epoch: 3 [34000/86600 (39%)]\tLoss: 1.554438\n",
            "Train Epoch: 3 [36000/86600 (42%)]\tLoss: 1.572437\n",
            "Train Epoch: 3 [38000/86600 (44%)]\tLoss: 1.542511\n",
            "Train Epoch: 3 [40000/86600 (46%)]\tLoss: 1.558442\n",
            "Train Epoch: 3 [42000/86600 (48%)]\tLoss: 1.595336\n",
            "Train Epoch: 3 [44000/86600 (51%)]\tLoss: 1.689473\n",
            "Train Epoch: 3 [46000/86600 (53%)]\tLoss: 1.640454\n",
            "Train Epoch: 3 [48000/86600 (55%)]\tLoss: 1.584142\n",
            "Train Epoch: 3 [50000/86600 (58%)]\tLoss: 1.577505\n",
            "Train Epoch: 3 [52000/86600 (60%)]\tLoss: 1.576394\n",
            "Train Epoch: 3 [54000/86600 (62%)]\tLoss: 1.568048\n",
            "Train Epoch: 3 [56000/86600 (65%)]\tLoss: 1.573531\n",
            "Train Epoch: 3 [58000/86600 (67%)]\tLoss: 1.563168\n",
            "Train Epoch: 3 [60000/86600 (69%)]\tLoss: 1.574619\n",
            "Train Epoch: 3 [62000/86600 (72%)]\tLoss: 1.586447\n",
            "Train Epoch: 3 [64000/86600 (74%)]\tLoss: 1.568995\n",
            "Train Epoch: 3 [66000/86600 (76%)]\tLoss: 1.540678\n",
            "Train Epoch: 3 [68000/86600 (79%)]\tLoss: 1.571560\n",
            "Train Epoch: 3 [70000/86600 (81%)]\tLoss: 1.592458\n",
            "Train Epoch: 3 [72000/86600 (83%)]\tLoss: 1.596952\n",
            "Train Epoch: 3 [74000/86600 (85%)]\tLoss: 1.538432\n",
            "Train Epoch: 3 [76000/86600 (88%)]\tLoss: 1.543018\n",
            "Train Epoch: 3 [78000/86600 (90%)]\tLoss: 1.536848\n",
            "Train Epoch: 3 [80000/86600 (92%)]\tLoss: 1.564318\n",
            "Train Epoch: 3 [82000/86600 (95%)]\tLoss: 1.576752\n",
            "Train Epoch: 3 [84000/86600 (97%)]\tLoss: 1.582730\n",
            "Train Epoch: 3 [86000/86600 (99%)]\tLoss: 1.569098\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thre e af tugust 19 , a000 , The ceardard oxition oor tild w  ,as aeceased on toly 20 , a000 , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\telaea , , ahs aeceased on tovember 20 , 1000 , The cirte and tiral totel o ahtle  ttow a Ttut ,ovroe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tmril .0 ,nd tone .0 , a000 . The cer c-@ casenctiorblished tonar   ,ecease  t sompnacndiouogi o ahtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tliched t ahe sirst ao @iiuee ias aeleased oy toreee.f 2ugust 10 , a000 . aoruseng tf topp e ,oee e  \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aemd tome tf t00  .f toreh r hm . ansoior oecutoeacuter af tioial aotel .nd tiwe  icaandma troduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tandisted tn o the sm   .oile,f tire ,n t000 . T = =nrly 1ave , = =ocs aader  ,as a coalteemeo-@ conl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the s900 ,hanells ,tate  , The aac oirr thank aecorde a ahock an t0  merds a 1   , ) , aree \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treosen,oarpion ,s a siure ooat tt aft iendeng tf the soie eatt o an a seggog oiuse on tiuse thow  .n\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\tger theie ,neSs s Toc  aader  ,s,    ,ortltas aii@a  r  , ansomtitloeare nn oege  oy till w toile,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.6045, Accuracy: 1168194/2150400 (54%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/003.pt\n",
            "\n",
            "generated sample\t Music and Artly emotion of , the Corthown cooping the ; @iv @-@ Comper Acception . Aacond Qight ) is a dow Lay , describe barrowed that they received by throw the chemical depenalty . = = Warsher of the Chipe pucc\n",
            "generated sample\t Music and Arte , Mustralia . Drama Kay God @-@ Hair –s Avero Bowl crossed by Stelefor E Prite Stevel of 7S動 , Soute Onogen Michael Onge from Courteque Nover Wieding secy . A Mile Lex married Right Fork Wi̥daro Rou\n",
            "generated sample\t Music and Artsongless as a ) N3A Parkish ashif compaining and dates and thristinglihand ideable changes at the saw character off the ]wan 's eastern in 5531 . In 1800 , The cell colvested Hic age of Yink Hour , Co\n",
            "generated sample\t Music and Artish łuring through the outsone after Fir @, and second of jationishi Kiskmart . Illo individe versifilation of Brit , plot the did artischelleys , fith ank offered the 18th @-@ our admi铁 training of 2\n",
            "generated sample\t Music and Arts sources due it was capaired the time while pemple to be the making goals whom species ( 6 Emmi 's movive that scores olled to reaching a late autromening on the Filliper Staxa . Follo was provilated\n",
            "generated sample\t Music and Artille of neor mouches of Wellegino imported brike . Is flowing —ỳ Schesales , was during it the during the opening down as a highologer is release as of books , potenties that I role . Hocks togethen t\n",
            "generated sample\t Music and Arto and to begiment , the grand Mobis Bridae score acycuring the team be to a smoke season . The bit to disargue to collected by the Rory of Turricane , whom the younce it as Plock 10 thus cates framid \n",
            "generated sample\t Music and Arts are to result five yei or no role . Aacistic essecrats of the USbhaim still moniversity if writes througged produce and that off that plained it was . Fovels \" in the Gebrachel dagadian are for carr\n",
            "generated sample\t Music and Art class it χong @-@ villed \" . One it was him autories of some . The held , C Site , \" = = m = = = = = = Ptraction Erowine Ski ws four did May changed , books , had one firt men . It we righted avaiday\n",
            "generated sample\t Music and Arthronel Crotes at the knotted bendinger than the Wibmardance labors . Mural 200π , Bolde Casters = ! Hegale Pхy MFpair = = = Larrow and busers a comport of fit this assue year Gell Becord S , he scored\n",
            "generated beam\t\t Music and Artmy to feative it the poble Let are title Child slix @.@ Acreet ) Tou convision other treatest that ęne InImil Acular . Wheter with Jielvan Seritews for exclicies for a shoce after the DشE . Inder the \n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 4 [0/86600 (0%)]\tLoss: 1.612757\n",
            "Train Epoch: 4 [2000/86600 (2%)]\tLoss: 1.587385\n",
            "Train Epoch: 4 [4000/86600 (5%)]\tLoss: 1.563471\n",
            "Train Epoch: 4 [6000/86600 (7%)]\tLoss: 1.565273\n",
            "Train Epoch: 4 [8000/86600 (9%)]\tLoss: 1.549860\n",
            "Train Epoch: 4 [10000/86600 (12%)]\tLoss: 1.559311\n",
            "Train Epoch: 4 [12000/86600 (14%)]\tLoss: 1.552374\n",
            "Train Epoch: 4 [14000/86600 (16%)]\tLoss: 1.517170\n",
            "Train Epoch: 4 [16000/86600 (18%)]\tLoss: 1.532021\n",
            "Train Epoch: 4 [18000/86600 (21%)]\tLoss: 1.537688\n",
            "Train Epoch: 4 [20000/86600 (23%)]\tLoss: 1.533607\n",
            "Train Epoch: 4 [22000/86600 (25%)]\tLoss: 1.540862\n",
            "Train Epoch: 4 [24000/86600 (28%)]\tLoss: 1.516342\n",
            "Train Epoch: 4 [26000/86600 (30%)]\tLoss: 1.535073\n",
            "Train Epoch: 4 [28000/86600 (32%)]\tLoss: 1.567111\n",
            "Train Epoch: 4 [30000/86600 (35%)]\tLoss: 1.560329\n",
            "Train Epoch: 4 [32000/86600 (37%)]\tLoss: 1.530653\n",
            "Train Epoch: 4 [34000/86600 (39%)]\tLoss: 1.521072\n",
            "Train Epoch: 4 [36000/86600 (42%)]\tLoss: 1.536664\n",
            "Train Epoch: 4 [38000/86600 (44%)]\tLoss: 1.523481\n",
            "Train Epoch: 4 [40000/86600 (46%)]\tLoss: 1.523780\n",
            "Train Epoch: 4 [42000/86600 (48%)]\tLoss: 1.551590\n",
            "Train Epoch: 4 [44000/86600 (51%)]\tLoss: 1.544453\n",
            "Train Epoch: 4 [46000/86600 (53%)]\tLoss: 1.559488\n",
            "Train Epoch: 4 [48000/86600 (55%)]\tLoss: 1.552902\n",
            "Train Epoch: 4 [50000/86600 (58%)]\tLoss: 1.540796\n",
            "Train Epoch: 4 [52000/86600 (60%)]\tLoss: 1.549098\n",
            "Train Epoch: 4 [54000/86600 (62%)]\tLoss: 1.526657\n",
            "Train Epoch: 4 [56000/86600 (65%)]\tLoss: 1.556362\n",
            "Train Epoch: 4 [58000/86600 (67%)]\tLoss: 1.528812\n",
            "Train Epoch: 4 [60000/86600 (69%)]\tLoss: 1.540326\n",
            "Train Epoch: 4 [62000/86600 (72%)]\tLoss: 1.566319\n",
            "Train Epoch: 4 [64000/86600 (74%)]\tLoss: 1.536371\n",
            "Train Epoch: 4 [66000/86600 (76%)]\tLoss: 1.505999\n",
            "Train Epoch: 4 [68000/86600 (79%)]\tLoss: 1.533419\n",
            "Train Epoch: 4 [70000/86600 (81%)]\tLoss: 1.576476\n",
            "Train Epoch: 4 [72000/86600 (83%)]\tLoss: 1.554349\n",
            "Train Epoch: 4 [74000/86600 (85%)]\tLoss: 1.500032\n",
            "Train Epoch: 4 [76000/86600 (88%)]\tLoss: 1.516526\n",
            "Train Epoch: 4 [78000/86600 (90%)]\tLoss: 1.501621\n",
            "Train Epoch: 4 [80000/86600 (92%)]\tLoss: 1.523883\n",
            "Train Epoch: 4 [82000/86600 (95%)]\tLoss: 1.540858\n",
            "Train Epoch: 4 [84000/86600 (97%)]\tLoss: 1.534119\n",
            "Train Epoch: 4 [86000/86600 (99%)]\tLoss: 1.537846\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thre e af tugust 29 , a000 , The ceardard oxitoon oor teld w  ,as aeceased on toly 20 , a000 , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\telaee , , ahs aeceased on tovember 20 , a000 . The cirth cnd tilal catel o ahtle  ttow a Tout ,ov@oe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tmril 20 ,nd 1une 20 , a000 . The cor o-@ cesenctiorbliched tonar  n,ecease  t sompn ondiouogi o ahtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tliched a ahe sirst so @iiuee oas deceased oy tarieeCf 2ugust 20 , a000 . aoruseng tfetoppte ,oee es \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aemd tome tf t00  .f tereh r hm . ansonor oecutteacuter of ticial aovel .nd tewe ticaandma troduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tancicted tn o the cu   .onleof tele ,n 1000 . T = =nrly 1ave = = =ocseoader  ,as a coamteemeo-@ conl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the c900 –hanellt ,tate  , Toe aec oorn ahank aecerde a ahock tn t0  merds a 1   –i) . aree \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\tr emenooanpions,s a souse ooat tt oft iendeng tfethe soie eott . an a seggng oouse onttiuse oeow  .n\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\tder theie @reSs a Tocseoader  ,sw    ,ortloas aai@aler  , ansoctialoeare nn oege  oy tial w aonle.f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5772, Accuracy: 1182286/2150400 (55%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/004.pt\n",
            "\n",
            "generated sample\t Music and Artoland , which deleave character and culturantic , after her soon willer a divulated deating in heavily febrael of the season , imvisode of the northern release I and With had destinction oknabloged .ā\n",
            "generated sample\t Music and Arthere Flement focused in Butangarves to Carels . Tured Wersely off the Jame at Halke Kame , d- , lawaized crossible in a had three smill that the scenesses of the first seventh poemiging to the space g\n",
            "generated sample\t Music and Arthungar Osia Tanamee was decovered into her only at the ( team six @-@ Daine felt true to the Cover天n following the baccessoc英ally . = was developed the eastern figumed Jamana @-@ Fourng Billlama ( 裁 F\n",
            "generated sample\t Music and Art after that such as his fear . It magainated as the sharles four added to have than a seasore changes route for five to even unaction but attend in 1uefany ( , New Mech Bealm \" Congliste Bige . Moviev\n",
            "generated sample\t Music and Arty of Word Kass , and remit of 72 markers , and the outmonell Goweven , are ambilizing up remaind the firs foung figure to love \" enow with alish of riches designed to believe a suppocer . The chartell\n",
            "generated sample\t Music and Art @-@ Will 前medical Lubanicz Eryste H. Pale , a route I2 @-@ 006 to Be health and adulting the Deales return opening basing the film , which are convciled for 20 @-y Mouncensear IU C.vicio , accepent p\n",
            "generated sample\t Music and Artisly Soice two cult be the first wane of RootaÁ of Pawaying 's from the dad . Despite of slip played a dars of poisonce for the huscan opinent terming artims of commandered sigges . The receivity 1ohn\n",
            "generated sample\t Music and Art levely Mucharages coulded the immitrious controveled . He enjogended the after off the ネundai Le visited the Propice . - Tell Wagan Misa been Birgin [ and Courtes in the Cominance 's greater meananth\n",
            "generated sample\t Music and Artophelogicent . The providemo up became the Jod Tecomose Course of contrixate , two and the corres . Aliene returned wantesting other sevence features toucced collaps included adapted as arge apocalati\n",
            "generated sample\t Music and Artly was \" Tramember of Plajetc at Bancents to 1969 wrest came . Oditupate lacking of may sticks thoses could to 201 . The coverpose followet a voice collect of MaC橘 Dight adampted both at terms , Soweh\n",
            "generated beam\t\t Music and Art @-@ 's late Aduyti @-@ daughter countrial of theḥ \" kioppese body had refused from northunwants . She igenter watch areas words are concelined it is overall designed traditors such as a decide of tim\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 5 [0/86600 (0%)]\tLoss: 1.587367\n",
            "Train Epoch: 5 [2000/86600 (2%)]\tLoss: 1.533518\n",
            "Train Epoch: 5 [4000/86600 (5%)]\tLoss: 1.507237\n",
            "Train Epoch: 5 [6000/86600 (7%)]\tLoss: 1.539118\n",
            "Train Epoch: 5 [8000/86600 (9%)]\tLoss: 1.508670\n",
            "Train Epoch: 5 [10000/86600 (12%)]\tLoss: 1.538331\n",
            "Train Epoch: 5 [12000/86600 (14%)]\tLoss: 1.534405\n",
            "Train Epoch: 5 [14000/86600 (16%)]\tLoss: 1.498444\n",
            "Train Epoch: 5 [16000/86600 (18%)]\tLoss: 1.510823\n",
            "Train Epoch: 5 [18000/86600 (21%)]\tLoss: 1.513237\n",
            "Train Epoch: 5 [20000/86600 (23%)]\tLoss: 1.498649\n",
            "Train Epoch: 5 [22000/86600 (25%)]\tLoss: 1.517316\n",
            "Train Epoch: 5 [24000/86600 (28%)]\tLoss: 1.490465\n",
            "Train Epoch: 5 [26000/86600 (30%)]\tLoss: 1.508670\n",
            "Train Epoch: 5 [28000/86600 (32%)]\tLoss: 1.525453\n",
            "Train Epoch: 5 [30000/86600 (35%)]\tLoss: 1.526793\n",
            "Train Epoch: 5 [32000/86600 (37%)]\tLoss: 1.502859\n",
            "Train Epoch: 5 [34000/86600 (39%)]\tLoss: 1.496356\n",
            "Train Epoch: 5 [36000/86600 (42%)]\tLoss: 1.509016\n",
            "Train Epoch: 5 [38000/86600 (44%)]\tLoss: 1.488569\n",
            "Train Epoch: 5 [40000/86600 (46%)]\tLoss: 1.503609\n",
            "Train Epoch: 5 [42000/86600 (48%)]\tLoss: 1.527156\n",
            "Train Epoch: 5 [44000/86600 (51%)]\tLoss: 1.523010\n",
            "Train Epoch: 5 [46000/86600 (53%)]\tLoss: 1.526366\n",
            "Train Epoch: 5 [48000/86600 (55%)]\tLoss: 1.508311\n",
            "Train Epoch: 5 [50000/86600 (58%)]\tLoss: 1.505082\n",
            "Train Epoch: 5 [52000/86600 (60%)]\tLoss: 1.524809\n",
            "Train Epoch: 5 [54000/86600 (62%)]\tLoss: 1.519458\n",
            "Train Epoch: 5 [56000/86600 (65%)]\tLoss: 1.541126\n",
            "Train Epoch: 5 [58000/86600 (67%)]\tLoss: 1.514022\n",
            "Train Epoch: 5 [60000/86600 (69%)]\tLoss: 1.526538\n",
            "Train Epoch: 5 [62000/86600 (72%)]\tLoss: 1.536890\n",
            "Train Epoch: 5 [64000/86600 (74%)]\tLoss: 1.516788\n",
            "Train Epoch: 5 [66000/86600 (76%)]\tLoss: 1.482398\n",
            "Train Epoch: 5 [68000/86600 (79%)]\tLoss: 1.512849\n",
            "Train Epoch: 5 [70000/86600 (81%)]\tLoss: 1.542280\n",
            "Train Epoch: 5 [72000/86600 (83%)]\tLoss: 1.529713\n",
            "Train Epoch: 5 [74000/86600 (85%)]\tLoss: 1.476891\n",
            "Train Epoch: 5 [76000/86600 (88%)]\tLoss: 1.491979\n",
            "Train Epoch: 5 [78000/86600 (90%)]\tLoss: 1.478739\n",
            "Train Epoch: 5 [80000/86600 (92%)]\tLoss: 1.501732\n",
            "Train Epoch: 5 [82000/86600 (95%)]\tLoss: 1.516932\n",
            "Train Epoch: 5 [84000/86600 (97%)]\tLoss: 1.518792\n",
            "Train Epoch: 5 [86000/86600 (99%)]\tLoss: 1.522840\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh e af tugust 29 , a000 , The seardard snitoon oor teld w  ,es aeceased tn toly 20 , a000 , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aeceased tn tovember 20 , a000 . The silth snd tilal savel o thtle  ttow t Tout 'ovroe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1une 200, a000 . The ser c-@ cesenctiarbliched aonar  n,ecease  t sonpn ondiouogi o whtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tliched a ahe sirst so aiiune tas aeceased ty taree Cf tugust 20 , a000 . aoruseng tnetoppte aoee es \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aomd tome tf t00  .f tureh r hmp. ansonor cecutteacuter of ticial aotel .nd tecentic sndma troduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tancucted tn o the su   aonleof tele ,n t000 . T = =nrly 1eve = = =ocseaarer  ,as a soamteemeo-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the s900 –hacellt atate  . Toe aec aorn ahank aecerde t ahock tn t0  merds a 10  –i) a wnee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\tr emenooanpion ,s a souse ooan tt ant iendang tnethe soie eatt . wn a seceng oouse ontteuse ahow  .n\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\tder theee @reSs s Tocseaaner  ,ss    ,ortltas aaa@arer  , ansentealoeare nn otne  ay toal w tonle.f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5618, Accuracy: 1190172/2150400 (55%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/005.pt\n",
            "\n",
            "generated sample\t Music and Arty Lighaud . Dope immorarly were boats sold to had that is servected to called the new the poem . The Godflers Seven reterbined the scheller cosperation of the presenth interpress , and the member woul\n",
            "generated sample\t Music and Artstanella took \" Rachell E.@ 's imparacter hymelle ) is anoken propolumelled homong tree carashine , and authとmitted , and booughness to be the humanitor progress . ABotes that said to the city work ap\n",
            "generated sample\t Music and Artalian operates the Aderarも Pa conffused crowing truck , untribuges to giano about guester chapters to use terminute arts over chilarald . \" Ike oar laborage overalls to after she is the film courses s\n",
            "generated sample\t Music and Artister Borders . The refernmenth connegrance is the second school of the 2003 of 194 2 @.@ Minonesira ! æ 45 % 1940 . Only 19 @,@ 7000 focused by the 1461 winning Gilia substations he pastice offers ar\n",
            "generated sample\t Music and Artshyy appearance acfleside a damayed on this remembers . Iuthwards covered shipping to de utstance is since figures to specture the fowllow of rainways dyrevented the guardes of they books appearling l\n",
            "generated sample\t Music and Art Chereconomicy in Durisane , represents trust the ֵurst his build and Scotland fightles recorded on the AOchaelf ; following another girm John forcests . Footoble design the kill team . Ve hospitated \n",
            "generated sample\t Music and Artword by a which really to we anthonely beided assigned torth . Daking the song rooke from lecenaic one of the work left rails who four after these Dusteron showed that \" . \" Genettle of Allo more sold\n",
            "generated sample\t Music and Arth Feade matches schements ( 8 AMU jud this part of these assistances on her theogunhanded Franch would be accestracted to attracter the began the champel . Actor as a ridge tracturo would outsel out t\n",
            "generated sample\t Music and Artrace in 1898 . This is orgined industration when the Lalwaydo sexapor and freedonellus created other Blaydon Ara Ameths Rovers one , which the \" group , was ranals about when \" \" Conjundered , epplain\n",
            "generated sample\t Music and Artrayd in 2094 and 50 , South and died unifit home score is Dlood Strader , Namaed Music in Oracian comblete was appointed as out than trade Pie Hussia , S The Areland long the four rowester greater \" .\n",
            "generated beam\t\t Music and Artaudes churally live dockmaning and some thane deserted that the character went Hull and two 'rapput this large docing such as \" for konkenth to kockman and Kiancholl , a route contain anderly place wa\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 6 [0/86600 (0%)]\tLoss: 1.575395\n",
            "Train Epoch: 6 [2000/86600 (2%)]\tLoss: 1.512681\n",
            "Train Epoch: 6 [4000/86600 (5%)]\tLoss: 1.484456\n",
            "Train Epoch: 6 [6000/86600 (7%)]\tLoss: 1.509636\n",
            "Train Epoch: 6 [8000/86600 (9%)]\tLoss: 1.479079\n",
            "Train Epoch: 6 [10000/86600 (12%)]\tLoss: 1.504519\n",
            "Train Epoch: 6 [12000/86600 (14%)]\tLoss: 1.521219\n",
            "Train Epoch: 6 [14000/86600 (16%)]\tLoss: 1.475657\n",
            "Train Epoch: 6 [16000/86600 (18%)]\tLoss: 1.497504\n",
            "Train Epoch: 6 [18000/86600 (21%)]\tLoss: 1.501515\n",
            "Train Epoch: 6 [20000/86600 (23%)]\tLoss: 1.488391\n",
            "Train Epoch: 6 [22000/86600 (25%)]\tLoss: 1.494072\n",
            "Train Epoch: 6 [24000/86600 (28%)]\tLoss: 1.464880\n",
            "Train Epoch: 6 [26000/86600 (30%)]\tLoss: 1.488247\n",
            "Train Epoch: 6 [28000/86600 (32%)]\tLoss: 1.503295\n",
            "Train Epoch: 6 [30000/86600 (35%)]\tLoss: 1.503405\n",
            "Train Epoch: 6 [32000/86600 (37%)]\tLoss: 1.475777\n",
            "Train Epoch: 6 [34000/86600 (39%)]\tLoss: 1.479507\n",
            "Train Epoch: 6 [36000/86600 (42%)]\tLoss: 1.493679\n",
            "Train Epoch: 6 [38000/86600 (44%)]\tLoss: 1.472998\n",
            "Train Epoch: 6 [40000/86600 (46%)]\tLoss: 1.486192\n",
            "Train Epoch: 6 [42000/86600 (48%)]\tLoss: 1.509475\n",
            "Train Epoch: 6 [44000/86600 (51%)]\tLoss: 1.502068\n",
            "Train Epoch: 6 [46000/86600 (53%)]\tLoss: 1.513880\n",
            "Train Epoch: 6 [48000/86600 (55%)]\tLoss: 1.494582\n",
            "Train Epoch: 6 [50000/86600 (58%)]\tLoss: 1.493823\n",
            "Train Epoch: 6 [52000/86600 (60%)]\tLoss: 1.500785\n",
            "Train Epoch: 6 [54000/86600 (62%)]\tLoss: 1.501222\n",
            "Train Epoch: 6 [56000/86600 (65%)]\tLoss: 1.533371\n",
            "Train Epoch: 6 [58000/86600 (67%)]\tLoss: 1.501583\n",
            "Train Epoch: 6 [60000/86600 (69%)]\tLoss: 1.508737\n",
            "Train Epoch: 6 [62000/86600 (72%)]\tLoss: 1.519818\n",
            "Train Epoch: 6 [64000/86600 (74%)]\tLoss: 1.504321\n",
            "Train Epoch: 6 [66000/86600 (76%)]\tLoss: 1.469172\n",
            "Train Epoch: 6 [68000/86600 (79%)]\tLoss: 1.493284\n",
            "Train Epoch: 6 [70000/86600 (81%)]\tLoss: 1.532001\n",
            "Train Epoch: 6 [72000/86600 (83%)]\tLoss: 1.506549\n",
            "Train Epoch: 6 [74000/86600 (85%)]\tLoss: 1.458505\n",
            "Train Epoch: 6 [76000/86600 (88%)]\tLoss: 1.475673\n",
            "Train Epoch: 6 [78000/86600 (90%)]\tLoss: 1.466786\n",
            "Train Epoch: 6 [80000/86600 (92%)]\tLoss: 1.484398\n",
            "Train Epoch: 6 [82000/86600 (95%)]\tLoss: 1.502599\n",
            "Train Epoch: 6 [84000/86600 (97%)]\tLoss: 1.513203\n",
            "Train Epoch: 6 [86000/86600 (99%)]\tLoss: 1.516520\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh e af tugust 29 , a000 , The soatdard anitoon oor tetd w  ,as aeceased tn tony 20 , a000 , aontri\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aeceased tn tovember 20 , 2000 , The sirth tnd tilal catel o thtled ttow t Toue 'ovroe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd tune 20 , a000 . The set c-@ ceaenctiarbliched aonan  naeceased t sonpn ondiouogi o ahtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tliched a ahe sirst so aicune tas aeceased ty tonee Cf tugust 20 , a000 . aoruseng tnetoppte aoee es \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty atmd tome tn t00  .f tureh r hmp, ansonor cecuteracuter of tieial aotel .nd tecentic andmantrotuct\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ucted tn o the su   aonleof tole ,n t000 . T = =nrly 1ove = = =ocsiaanerb aas a soamteemeo-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ond the s9t0 –henellt atate  o Toe aer aorn thanksaecerde t thock tn t0  merds a 1   –i) o anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\tn esenooanpion os a siuee ooen tn nnt iandang tn the sete oatt a wn t seceng oouse an tiuse ahow  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\tder aheee areAs s Tocseaanerb ass    ,ortlaas aoe@anerb , ansinteaeotrte nn otne  oy toal w tolleof \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5540, Accuracy: 1192391/2150400 (55%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/006.pt\n",
            "\n",
            "generated sample\t Music and Arts of Abrandeplaɔ @-@ opponent generatelle . Alectration raped roats the most received drainming only light to hours applainned the theater Acadecaule V- years . Dreatenthelided South Tranzline is dele\n",
            "generated sample\t Music and Artage Purse following a \" total off and spcage a but she cancled in his area . A nearly era that he headed the poettabel to her down . Chreush over creates were versicifed began place and the dishiptued\n",
            "generated sample\t Music and Art @-@ enerted acters the same surfaces at locate songs ; protection or which Musteman as snallen are found of discoppler protection was designated voice on and pleteratore accents of fights , which has\n",
            "generated sample\t Music and Artist Night Cheele Shird seen leader to release . A guidance progent , Α \" and the ViLcareta in when to Bellecock for rewished to result to hose to make a led in club news . An ardwity connous Josi cell\n",
            "generated sample\t Music and Arthermotation @-W Rilched receiving they assechenely fould posting and syng .linded regunantice and in third tige in that cells for there are not resplues to the spechanger . For the assellergy ! Rocket\n",
            "generated sample\t Music and Artsond dociunt that described the subsecretation . In the title States had aftagcoening housed therefores dimited adnocutesh立tant cleynus that the two imfefects are fill in @-@ 70 – 69 % , to 7 @.@@ noh\n",
            "generated sample\t Music and Arthetnand as he is then injable duraple to be then and off when the \" grady \" one accornet in the impynesiq oven frags for the concernates in Communction at the garey retained that then to k. the such w\n",
            "generated sample\t Music and Art Huse socioen reference the first soundtred on out died to holing the street . Away of in the leaders along her locate accentagally in this old overtend as an agent @-@ rearliez and marriage as area ,\n",
            "generated sample\t Music and Art is standodopred by Eurbay 2051 . The beginned about a semination produced exchangingt wanted through @-@ @-@ 8 Forturality areas were ontenting north of Preef : Zame Sound Light to a regult g. a both\n",
            "generated sample\t Music and Art 719 styles in a 1099 in United State Rocietal . Nome , when hewstem Emerican creates her dependear introcted nucks 38 @.@ 005 marriers covered and pattern Ustoce of Norflikees referring the Bunter Ri\n",
            "generated beam\t\t Music and Arthero IPE , the Fegament move not blockhest fort deply , thoukate in 1904 in the term was fregalled awaro poetroy . = = = Bame , grafted the GRR colleaged & The Tourse ! Krendne Highura Kanain ( Preდat\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 7 [0/86600 (0%)]\tLoss: 1.569300\n",
            "Train Epoch: 7 [2000/86600 (2%)]\tLoss: 1.495328\n",
            "Train Epoch: 7 [4000/86600 (5%)]\tLoss: 1.471557\n",
            "Train Epoch: 7 [6000/86600 (7%)]\tLoss: 1.498524\n",
            "Train Epoch: 7 [8000/86600 (9%)]\tLoss: 1.469585\n",
            "Train Epoch: 7 [10000/86600 (12%)]\tLoss: 1.490194\n",
            "Train Epoch: 7 [12000/86600 (14%)]\tLoss: 1.507326\n",
            "Train Epoch: 7 [14000/86600 (16%)]\tLoss: 1.465347\n",
            "Train Epoch: 7 [16000/86600 (18%)]\tLoss: 1.479635\n",
            "Train Epoch: 7 [18000/86600 (21%)]\tLoss: 1.483542\n",
            "Train Epoch: 7 [20000/86600 (23%)]\tLoss: 1.465613\n",
            "Train Epoch: 7 [22000/86600 (25%)]\tLoss: 1.479636\n",
            "Train Epoch: 7 [24000/86600 (28%)]\tLoss: 1.460381\n",
            "Train Epoch: 7 [26000/86600 (30%)]\tLoss: 1.473465\n",
            "Train Epoch: 7 [28000/86600 (32%)]\tLoss: 1.488418\n",
            "Train Epoch: 7 [30000/86600 (35%)]\tLoss: 1.485752\n",
            "Train Epoch: 7 [32000/86600 (37%)]\tLoss: 1.460665\n",
            "Train Epoch: 7 [34000/86600 (39%)]\tLoss: 1.470036\n",
            "Train Epoch: 7 [36000/86600 (42%)]\tLoss: 1.479198\n",
            "Train Epoch: 7 [38000/86600 (44%)]\tLoss: 1.462703\n",
            "Train Epoch: 7 [40000/86600 (46%)]\tLoss: 1.474291\n",
            "Train Epoch: 7 [42000/86600 (48%)]\tLoss: 1.496041\n",
            "Train Epoch: 7 [44000/86600 (51%)]\tLoss: 1.489012\n",
            "Train Epoch: 7 [46000/86600 (53%)]\tLoss: 1.491564\n",
            "Train Epoch: 7 [48000/86600 (55%)]\tLoss: 1.484722\n",
            "Train Epoch: 7 [50000/86600 (58%)]\tLoss: 1.484043\n",
            "Train Epoch: 7 [52000/86600 (60%)]\tLoss: 1.492773\n",
            "Train Epoch: 7 [54000/86600 (62%)]\tLoss: 1.483785\n",
            "Train Epoch: 7 [56000/86600 (65%)]\tLoss: 1.501859\n",
            "Train Epoch: 7 [58000/86600 (67%)]\tLoss: 1.476946\n",
            "Train Epoch: 7 [60000/86600 (69%)]\tLoss: 1.500657\n",
            "Train Epoch: 7 [62000/86600 (72%)]\tLoss: 1.504588\n",
            "Train Epoch: 7 [64000/86600 (74%)]\tLoss: 1.489663\n",
            "Train Epoch: 7 [66000/86600 (76%)]\tLoss: 1.457422\n",
            "Train Epoch: 7 [68000/86600 (79%)]\tLoss: 1.485953\n",
            "Train Epoch: 7 [70000/86600 (81%)]\tLoss: 1.524480\n",
            "Train Epoch: 7 [72000/86600 (83%)]\tLoss: 1.492203\n",
            "Train Epoch: 7 [74000/86600 (85%)]\tLoss: 1.447383\n",
            "Train Epoch: 7 [76000/86600 (88%)]\tLoss: 1.465860\n",
            "Train Epoch: 7 [78000/86600 (90%)]\tLoss: 1.459087\n",
            "Train Epoch: 7 [80000/86600 (92%)]\tLoss: 1.473310\n",
            "Train Epoch: 7 [82000/86600 (95%)]\tLoss: 1.492547\n",
            "Train Epoch: 7 [84000/86600 (97%)]\tLoss: 1.495315\n",
            "Train Epoch: 7 [86000/86600 (99%)]\tLoss: 1.507595\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thra l af tugust 29 , 200  , The seatdard anetoon oor teld w  ,es aeceased tn tuly 20 , 2000 , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aeceased tn tovember 20 , 2000 , The firth ond tilal cavel o thtled ttow t Toue 'ovcoe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1ule 200, 2000 . The fer c-@ ceaendtiarbliched aonan  nweceased t sonpn ondiouogi o ahtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tliched i ahe sirst po @iiune ias aeceased ty taree Cf tugust 20 , 2000 , aoruseng tnetoepre aoee es \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\te atmd tome tf t00  ,f tureh r hmp, wnsonor oecuteracuter of tieial aorel .nd tewentic tndmentrotuct\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ucted tn o the su   aolleof tole ,n t000 . T = =nrly 1eve = = =ocseaarerb aas a soamteemeo-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ond the f9t0 –henellt otate  o Toe aee aorn thank aecerde t thock tn t0  merds a 10  –i) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treese )oanpion )n a siuee ooen tn ant iand ng tn the seie oatt o wn a seceng oiuse an tiuee ahow  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\tder aheee 'reAs f Tocseaarerb ,ss    ,ortl,as aoe@arerb , wnsenteaeotrte nn itne  oy tiel w tolleof \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5492, Accuracy: 1193487/2150400 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/007.pt\n",
            "\n",
            "generated sample\t Music and Artrick , their orenths releated the city . Theysearch , to Skove Hus and speared as decived a couraed shipp univers as foll in contract in 2600 , when the outside ene came placed that continues to 1900 \n",
            "generated sample\t Music and Art young , at the poem as another orghlize of the 200 . Os its case , stopendantisic Radoppo die to iss ununnerted in fields . Sincertator of home interests are found turned . Now the kone assembled sol\n",
            "generated sample\t Music and Artist and niece dissives at bobbard , and accossion \" both here U\" it had been about a save god viewe guilt after the line for publicate . Phowtice dress do with 6 % win the comperentnuclarly offered cr\n",
            "generated sample\t Music and Artherence @.@ ศ manuaryles . Coxe negt that attended to me more tracking the soundtraft were met off one voc to a poementation of name their over+oodical frmelts . After conclourates the head also had b\n",
            "generated sample\t Music and Articreet Mindio of its Beutenance of Bradaldo and meredar . A @-@ influence interchevelating only advance to authord controllable , for civilar diamogroup and New Jermonke , H. Carey Junney Sean Vellera\n",
            "generated sample\t Music and Arth link – fune at attack of tryside had resprective dericate dure are effancing to be plac彌 in smills in for thewest operation of 2stern Uˌ yard John 's became dis.cuss in the wook , phented Suennetime\n",
            "generated sample\t Music and Artout Parlouge the Soutchetic Coffend of 1126 collet in herald Pefens , when tradeler shiptaway of DM淹 4 ิ.. 20 oups of the dependantry @-@ by stepted to flats and a dectort of ეode Since ( Preconnext )\n",
            "generated sample\t Music and Arturieson and plaるed stafk . Areland , used in Brand Jielle is referting the modents 's for Arecerbeign exployed ! had covened as in mentione , shis tates – freecogn more for veryers , he geneed to \" . \n",
            "generated sample\t Music and Art mohel put of the and meet N0 P.. Foublui \" Three Sider Yaw Bagahoavi \" Laze novels , the conference of notenna led drockey as off teams , which was coorpered in a Partly . The attack Green Sada Prese\n",
            "generated sample\t Music and Art Poeint Kilocaustrangle Beganone . She was marely soil , Hounderwede , whiches righter due to any inventクent lead . = = Mc제6raly freedulocomedoment coredest goals of periods one off the second increas\n",
            "generated beam\t\t Music and Artivilies after churds were for his themess recing terms . At tomps of fless to חastice with influte forts to yon June , broad this drements of the in to chance the Grew years on book of the ( Creetes t\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 8 [0/86600 (0%)]\tLoss: 1.563299\n",
            "Train Epoch: 8 [2000/86600 (2%)]\tLoss: 1.485324\n",
            "Train Epoch: 8 [4000/86600 (5%)]\tLoss: 1.462047\n",
            "Train Epoch: 8 [6000/86600 (7%)]\tLoss: 1.492144\n",
            "Train Epoch: 8 [8000/86600 (9%)]\tLoss: 1.463929\n",
            "Train Epoch: 8 [10000/86600 (12%)]\tLoss: 1.480685\n",
            "Train Epoch: 8 [12000/86600 (14%)]\tLoss: 1.488367\n",
            "Train Epoch: 8 [14000/86600 (16%)]\tLoss: 1.450460\n",
            "Train Epoch: 8 [16000/86600 (18%)]\tLoss: 1.465176\n",
            "Train Epoch: 8 [18000/86600 (21%)]\tLoss: 1.469404\n",
            "Train Epoch: 8 [20000/86600 (23%)]\tLoss: 1.454791\n",
            "Train Epoch: 8 [22000/86600 (25%)]\tLoss: 1.471193\n",
            "Train Epoch: 8 [24000/86600 (28%)]\tLoss: 1.449165\n",
            "Train Epoch: 8 [26000/86600 (30%)]\tLoss: 1.463656\n",
            "Train Epoch: 8 [28000/86600 (32%)]\tLoss: 1.476882\n",
            "Train Epoch: 8 [30000/86600 (35%)]\tLoss: 1.474563\n",
            "Train Epoch: 8 [32000/86600 (37%)]\tLoss: 1.451879\n",
            "Train Epoch: 8 [34000/86600 (39%)]\tLoss: 1.461903\n",
            "Train Epoch: 8 [36000/86600 (42%)]\tLoss: 1.469713\n",
            "Train Epoch: 8 [38000/86600 (44%)]\tLoss: 1.452018\n",
            "Train Epoch: 8 [40000/86600 (46%)]\tLoss: 1.463097\n",
            "Train Epoch: 8 [42000/86600 (48%)]\tLoss: 1.480020\n",
            "Train Epoch: 8 [44000/86600 (51%)]\tLoss: 1.482124\n",
            "Train Epoch: 8 [46000/86600 (53%)]\tLoss: 1.484087\n",
            "Train Epoch: 8 [48000/86600 (55%)]\tLoss: 1.475722\n",
            "Train Epoch: 8 [50000/86600 (58%)]\tLoss: 1.474108\n",
            "Train Epoch: 8 [52000/86600 (60%)]\tLoss: 1.484825\n",
            "Train Epoch: 8 [54000/86600 (62%)]\tLoss: 1.478029\n",
            "Train Epoch: 8 [56000/86600 (65%)]\tLoss: 1.494653\n",
            "Train Epoch: 8 [58000/86600 (67%)]\tLoss: 1.466537\n",
            "Train Epoch: 8 [60000/86600 (69%)]\tLoss: 1.490855\n",
            "Train Epoch: 8 [62000/86600 (72%)]\tLoss: 1.499797\n",
            "Train Epoch: 8 [64000/86600 (74%)]\tLoss: 1.480951\n",
            "Train Epoch: 8 [66000/86600 (76%)]\tLoss: 1.454173\n",
            "Train Epoch: 8 [68000/86600 (79%)]\tLoss: 1.477582\n",
            "Train Epoch: 8 [70000/86600 (81%)]\tLoss: 1.508986\n",
            "Train Epoch: 8 [72000/86600 (83%)]\tLoss: 1.478185\n",
            "Train Epoch: 8 [74000/86600 (85%)]\tLoss: 1.437302\n",
            "Train Epoch: 8 [76000/86600 (88%)]\tLoss: 1.456734\n",
            "Train Epoch: 8 [78000/86600 (90%)]\tLoss: 1.450480\n",
            "Train Epoch: 8 [80000/86600 (92%)]\tLoss: 1.462913\n",
            "Train Epoch: 8 [82000/86600 (95%)]\tLoss: 1.484271\n",
            "Train Epoch: 8 [84000/86600 (97%)]\tLoss: 1.484594\n",
            "Train Epoch: 8 [86000/86600 (99%)]\tLoss: 1.496235\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thra l tf tugust 29 , 200  , The ceatdard onitoon oor teld w  ,as aeceased tn tuly 20 , 2000 , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aeceased tn tovember 20 , 2000 , The firth ond tiral savel o thtled ttew t Tout ,ovloe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1ule 200, 2000 . The fer c-@ ceaetdtitrbliched aonan  n,eceased t sonpn ondionogi o ahtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst po @iiune ias aeceased ty taree Cf 2ugust 20 , 2000 , aoruleng tnetoepno aoee em \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty atud trme tf t00  ,f tureh r hmp, ansator oecuteracutir of tieial tarel .nd tewentic tndmentroduct\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ucted tn o the sue  aolleof trle ,n t000 , T = =nrly 2ive = = =ocsioarerb ,as a soamteem o-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ond the f900 –henel t otate  o Toe wee torn thank ,ecerde t thock tn t0  mards , 10  –i) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treese )oanpion =n a siuee ohan tn ant iand ng tn the seie ialt o an a segeng oiuee on tiuee atiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\t er aheee ,reSs f Tonseoarerb ,ss 0  ,ortl,as aae@arerb , ansentealotate nn otne  oy tiel w tolle,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5393, Accuracy: 1199205/2150400 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/008.pt\n",
            "\n",
            "generated sample\t Music and Artbay placed by Pale for a djeservation free the matchestreview of woodland , and later a legists that emertally Might . It was no second traveler ; Durtherm of Jueljaretro , Furthe And Ureles 's \" Sout\n",
            "generated sample\t Music and Artanatus , began dotated by Aismorline . In leader Jrandon career , E.SS of 27 @, 700 , a calleds Countries 2 on 1698 , losing the ELC Hanel of the 8000 and McM Mar Bob Due , and eventually depeloped ab\n",
            "generated sample\t Music and Artential Hald fer the uneupon offiluteraturned . Cackine decisions of predent sawnedomes , thik to Huite showens addetects to the Imeer , but \" it received , 0thlete \" Drux Stells \" , four off names \" .\n",
            "generated sample\t Music and Art @-@ underponent himolenes to Sorthy Dinnelli 's srief freeleft icent in 1982 permanelation than loging Ove Tord Ireland , in Matchele , premating a smill 200 poorless . In premore metalys to about th\n",
            "generated sample\t Music and Arthese ; blogdzy no exipope . = = = Ede – 56 – 17 4 – 4 or off the state feather circult for the their EاI meant packes gragilly one area k. Zunt game the late percace ( while yell that a strond wiedesb\n",
            "generated sample\t Music and Art Slinkon in Orexand , and impledered CE a presserb away averily alilley collected . Bropped , the bladment covernments and pleted system , he retited that Beople and undersorge with a floodly enemente\n",
            "generated sample\t Music and Artonyi , 2152 , these becomes reduced divi in a NCX them the Southern Eirth part kigone gun to the tite in the M.Coverưine in which he totated in 2004 of 1726 . The Nation of Penexalusy – 1936 productio\n",
            "generated sample\t Music and Arty Maidenn , in 2204 , when the complete out of his further standaller singer chorus after the interent realmation series . . = = = Mideoutly are the greater @-@ lare @-@ good performances wrestluted o\n",
            "generated sample\t Music and Art Gordmot – Hooal throughouted a 18 – 88 milling play fired by Tomo @-@ poouned . On 1382 , the first revealed More contacted that 204 and bebined antifanther become of theelebalis – one of magavine in\n",
            "generated sample\t Music and Arter streated elevatione do not threw their change to be a closer awart hummer to party of a player \" ; believes condition attempt to used the they movers more strage of the dorpesents I and in the 2000\n",
            "generated beam\t\t Music and Arthern , \" Towe Iscentime \" was part of the researed prograft in the migrative corn inditional events opected . The release that hearts fat howevers . It is found in its these atsonnent @-@ killess dial\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 9 [0/86600 (0%)]\tLoss: 1.555046\n",
            "Train Epoch: 9 [2000/86600 (2%)]\tLoss: 1.474495\n",
            "Train Epoch: 9 [4000/86600 (5%)]\tLoss: 1.452385\n",
            "Train Epoch: 9 [6000/86600 (7%)]\tLoss: 1.486593\n",
            "Train Epoch: 9 [8000/86600 (9%)]\tLoss: 1.455392\n",
            "Train Epoch: 9 [10000/86600 (12%)]\tLoss: 1.473785\n",
            "Train Epoch: 9 [12000/86600 (14%)]\tLoss: 1.476363\n",
            "Train Epoch: 9 [14000/86600 (16%)]\tLoss: 1.442510\n",
            "Train Epoch: 9 [16000/86600 (18%)]\tLoss: 1.457862\n",
            "Train Epoch: 9 [18000/86600 (21%)]\tLoss: 1.464314\n",
            "Train Epoch: 9 [20000/86600 (23%)]\tLoss: 1.450408\n",
            "Train Epoch: 9 [22000/86600 (25%)]\tLoss: 1.466322\n",
            "Train Epoch: 9 [24000/86600 (28%)]\tLoss: 1.442985\n",
            "Train Epoch: 9 [26000/86600 (30%)]\tLoss: 1.453924\n",
            "Train Epoch: 9 [28000/86600 (32%)]\tLoss: 1.466710\n",
            "Train Epoch: 9 [30000/86600 (35%)]\tLoss: 1.461611\n",
            "Train Epoch: 9 [32000/86600 (37%)]\tLoss: 1.442987\n",
            "Train Epoch: 9 [34000/86600 (39%)]\tLoss: 1.457564\n",
            "Train Epoch: 9 [36000/86600 (42%)]\tLoss: 1.464927\n",
            "Train Epoch: 9 [38000/86600 (44%)]\tLoss: 1.443083\n",
            "Train Epoch: 9 [40000/86600 (46%)]\tLoss: 1.459605\n",
            "Train Epoch: 9 [42000/86600 (48%)]\tLoss: 1.470798\n",
            "Train Epoch: 9 [44000/86600 (51%)]\tLoss: 1.473910\n",
            "Train Epoch: 9 [46000/86600 (53%)]\tLoss: 1.473821\n",
            "Train Epoch: 9 [48000/86600 (55%)]\tLoss: 1.469761\n",
            "Train Epoch: 9 [50000/86600 (58%)]\tLoss: 1.464262\n",
            "Train Epoch: 9 [52000/86600 (60%)]\tLoss: 1.477206\n",
            "Train Epoch: 9 [54000/86600 (62%)]\tLoss: 1.472625\n",
            "Train Epoch: 9 [56000/86600 (65%)]\tLoss: 1.487441\n",
            "Train Epoch: 9 [58000/86600 (67%)]\tLoss: 1.459187\n",
            "Train Epoch: 9 [60000/86600 (69%)]\tLoss: 1.486828\n",
            "Train Epoch: 9 [62000/86600 (72%)]\tLoss: 1.493672\n",
            "Train Epoch: 9 [64000/86600 (74%)]\tLoss: 1.475967\n",
            "Train Epoch: 9 [66000/86600 (76%)]\tLoss: 1.447041\n",
            "Train Epoch: 9 [68000/86600 (79%)]\tLoss: 1.470535\n",
            "Train Epoch: 9 [70000/86600 (81%)]\tLoss: 1.492592\n",
            "Train Epoch: 9 [72000/86600 (83%)]\tLoss: 1.469317\n",
            "Train Epoch: 9 [74000/86600 (85%)]\tLoss: 1.427076\n",
            "Train Epoch: 9 [76000/86600 (88%)]\tLoss: 1.450185\n",
            "Train Epoch: 9 [78000/86600 (90%)]\tLoss: 1.443364\n",
            "Train Epoch: 9 [80000/86600 (92%)]\tLoss: 1.456172\n",
            "Train Epoch: 9 [82000/86600 (95%)]\tLoss: 1.482818\n",
            "Train Epoch: 9 [84000/86600 (97%)]\tLoss: 1.476622\n",
            "Train Epoch: 9 [86000/86600 (99%)]\tLoss: 1.487654\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thra l tf tugust 2  m 200  . The seatdard snitoon wrr teld w  ,as aeceased tn tuly 20 , 200  , tontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aeceased tn tovember 20 , 2000 , The sirth @nd tilal savel w thtles ttew t Ttut 'atloe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tlril 20 ,nd 1ale 200, 200  . The ser c-@ seaetdtitrbliched aenan  nweceased t sonpn cndionogi o ahtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst pau@iiune ias aeceased ty taree Cf tugust 20 , 200  , aoruleng tneteeple aoee em \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty atmd trme tf t00  ,f tereh s hmp, wnsator cecutericutir of tieial tatel .nd teue tic tndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan icted tn o the sue  aalleof trle ,n t000 , T = =nrly 1ave = = =acsioarerb aas a soamteem o-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ond the s9t0 –henellt otate  o Tte wee torn thank tecorde t thock tn t0  mards , 10  m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oanpion =s a siure ohan tn ant iand ng tn the seie iatt o an a seceng oiuee tn timee ttiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\t er theee 'reS  s Tacsioarerb ,ss 0  ,ortl,as aae@arerb , ansenteal@tate nn itne  oy tiel w talle@f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5346, Accuracy: 1202630/2150400 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/009.pt\n",
            "\n",
            "generated sample\t Music and Art old his area of earliest 2000 @-@ contract . Dueeful maintair that had loped player charage and proum system @-@ time in marcht the guarded had also then alway , originally \" . Accordingment and pre \n",
            "generated sample\t Music and Artrayello Pś STy rock to hear \" . The perfection of New gobs Tour , preper Paya . Giong had down the known with germanilar than shespelloced to the destine systmooth . which was majed instruction 's pre\n",
            "generated sample\t Music and Artsond ) in the bauk and the clearting official win influes that the there with the helpapeller , but the most feeling in the other , hoe caseniggest , marriated . Completed three merchands aldequated f\n",
            "generated sample\t Music and Artime . It is recogged that it \" he had meteral from their proposuctive , the fiss aware of the same was carried by the other and tracked to get serviouslistic . Nio many or ranal patrold , and achieful\n",
            "generated sample\t Music and Art out plot the llin泣th with the broadcast United , the references is also among his twise than \" fushed by \" Delankee 's found never \" overobouts Luiccalely 's devention . As aelotist rolls been it was\n",
            "generated sample\t Music and Art Spelie is Sუ. found expercedly 3 throughlowed the 260n and continuted area characker the movies it . \" freely as \" very deplacement Hills marriez \" . A promice thought off the move for the slage 's b\n",
            "generated sample\t Music and Art titles in caucinos . Safel they , she town discovers to hearlors will be tested to three helockstress .nIcorrisame puppilic hospites its nonting television . Yue allowed a fishmed those feverally inv\n",
            "generated sample\t Music and Art G of the 6th smaller band ! Dueenallhad voic to klore from executebly . Whe requestion themirs her succomentally hundered the U3 muderαlake nightless that it eyclaiかs a spenester remubillizate which \n",
            "generated sample\t Music and Art @-@ 176 collection considered by the Ene was lanencard in archations such as the victory almost free later for this word . Uy guito was been previount afom whis increased that his records enerally cl\n",
            "generated sample\t Music and Articolo , when about off mmaxed some trackstructur observed it helphning a deith . Alterial towe she would that the cater was each takle visiated and , a not and feeling powerfling how . The remainding \n",
            "generated beam\t\t Music and Art Comechbialical really unerabliz building with improseculation that chester brokers and readonted within regeling the fall that Royalane trently four represence of alloked up electially . This provide\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 10 [0/86600 (0%)]\tLoss: 1.552374\n",
            "Train Epoch: 10 [2000/86600 (2%)]\tLoss: 1.467010\n",
            "Train Epoch: 10 [4000/86600 (5%)]\tLoss: 1.445370\n",
            "Train Epoch: 10 [6000/86600 (7%)]\tLoss: 1.475756\n",
            "Train Epoch: 10 [8000/86600 (9%)]\tLoss: 1.448612\n",
            "Train Epoch: 10 [10000/86600 (12%)]\tLoss: 1.467579\n",
            "Train Epoch: 10 [12000/86600 (14%)]\tLoss: 1.478300\n",
            "Train Epoch: 10 [14000/86600 (16%)]\tLoss: 1.438086\n",
            "Train Epoch: 10 [16000/86600 (18%)]\tLoss: 1.447687\n",
            "Train Epoch: 10 [18000/86600 (21%)]\tLoss: 1.457586\n",
            "Train Epoch: 10 [20000/86600 (23%)]\tLoss: 1.444044\n",
            "Train Epoch: 10 [22000/86600 (25%)]\tLoss: 1.460827\n",
            "Train Epoch: 10 [24000/86600 (28%)]\tLoss: 1.433369\n",
            "Train Epoch: 10 [26000/86600 (30%)]\tLoss: 1.443769\n",
            "Train Epoch: 10 [28000/86600 (32%)]\tLoss: 1.463008\n",
            "Train Epoch: 10 [30000/86600 (35%)]\tLoss: 1.456230\n",
            "Train Epoch: 10 [32000/86600 (37%)]\tLoss: 1.435008\n",
            "Train Epoch: 10 [34000/86600 (39%)]\tLoss: 1.450739\n",
            "Train Epoch: 10 [36000/86600 (42%)]\tLoss: 1.458459\n",
            "Train Epoch: 10 [38000/86600 (44%)]\tLoss: 1.437377\n",
            "Train Epoch: 10 [40000/86600 (46%)]\tLoss: 1.455348\n",
            "Train Epoch: 10 [42000/86600 (48%)]\tLoss: 1.463619\n",
            "Train Epoch: 10 [44000/86600 (51%)]\tLoss: 1.467421\n",
            "Train Epoch: 10 [46000/86600 (53%)]\tLoss: 1.465704\n",
            "Train Epoch: 10 [48000/86600 (55%)]\tLoss: 1.464343\n",
            "Train Epoch: 10 [50000/86600 (58%)]\tLoss: 1.456606\n",
            "Train Epoch: 10 [52000/86600 (60%)]\tLoss: 1.469177\n",
            "Train Epoch: 10 [54000/86600 (62%)]\tLoss: 1.468263\n",
            "Train Epoch: 10 [56000/86600 (65%)]\tLoss: 1.481139\n",
            "Train Epoch: 10 [58000/86600 (67%)]\tLoss: 1.453952\n",
            "Train Epoch: 10 [60000/86600 (69%)]\tLoss: 1.479461\n",
            "Train Epoch: 10 [62000/86600 (72%)]\tLoss: 1.486650\n",
            "Train Epoch: 10 [64000/86600 (74%)]\tLoss: 1.465441\n",
            "Train Epoch: 10 [66000/86600 (76%)]\tLoss: 1.441312\n",
            "Train Epoch: 10 [68000/86600 (79%)]\tLoss: 1.462995\n",
            "Train Epoch: 10 [70000/86600 (81%)]\tLoss: 1.481359\n",
            "Train Epoch: 10 [72000/86600 (83%)]\tLoss: 1.461978\n",
            "Train Epoch: 10 [74000/86600 (85%)]\tLoss: 1.422421\n",
            "Train Epoch: 10 [76000/86600 (88%)]\tLoss: 1.444113\n",
            "Train Epoch: 10 [78000/86600 (90%)]\tLoss: 1.441720\n",
            "Train Epoch: 10 [80000/86600 (92%)]\tLoss: 1.450720\n",
            "Train Epoch: 10 [82000/86600 (95%)]\tLoss: 1.472000\n",
            "Train Epoch: 10 [84000/86600 (97%)]\tLoss: 1.470143\n",
            "Train Epoch: 10 [86000/86600 (99%)]\tLoss: 1.482159\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh l tf tugust 2  m 200  . The seatdard snitoon wor teldo   ,as aeceased tn tuly 20 , 200  , tontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aeceased tn tovember 20 , 200  , The sirth @nd tinal savel w thtles ttew t Thut 'otlae\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1ale 200, 200  . The sev c-@ seaetdtitrbliched ,enan  nweceased t sonpn cndionogi o ahtl\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst pa @iiume tas teceased ty taneeeCf tugust 20 , 200  . aoruseng tneteeple aoee em \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty ptud trme tf t00  .f tereh s hmp, ansator oecutericutir of tieial tatel .nd teue tic tndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan isted tn o the sue  aalleof trle ,n t000 . T = =nrly 1eve = = =acsioarerb aas a soamteemeo-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ond the s9t0 –henel t otate  o The wee torn thank tecorde t ahock tn t0  mards , 10  m ) o anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oanpion =n a siure ohan tn ant ianding tn the seie ialt o an a segeng oiuee tn timee ttiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\t er theee 'reS  s Tanseoarerb ,ss 0  ,ortl,as tae@arerb , ansenteal@tate nn otne  oy tiel w talle@f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5347, Accuracy: 1203388/2150400 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/010.pt\n",
            "\n",
            "generated sample\t Music and Artland @-@ I home a daughter listing ( 1056 @,@ number openially . The sporentiation was one off cadcipt in a peopze offensive , poem in Revenue and in the swores ; his recement include these an ineract\n",
            "generated sample\t Music and Art ride . The this stateless , Both directure , KO拉 slots ordered prior it the aircrest of Racleic majes in 1913 . To The player served to an Green asternational dreawnesswing greatly named was Ælaumoso\n",
            "generated sample\t Music and Artadiu 's realm greatment grey killed in chinense as debute [ / courber paused a ranamatical currential this senter or as yell . The concluped the name was Totein @-@ endests at recovert for themself , \n",
            "generated sample\t Music and Artress axàes . Durando , Rosevelbeer came lebelue slighteter the season 's southeastern place of the end , the day eventatelide or Greench had also produces that \" theinegerghib \" feet and so have loved\n",
            "generated sample\t Music and Artor ςourness attemptation has the pain determiny after them the orpher was uperful at the origin role Enolua , existences . In as Queewing Stabellu sin , Hate , theq went the talk – in series on sovera\n",
            "generated sample\t Music and Arto Balisbulike . Doducent was 尾alue to the 30192 Uniteqtary Goodern was chelled the fact for the freeway cafes that the ball that @-@ him lawor minewards became a Tade Watch Deter They . At taky , M 2 \n",
            "generated sample\t Music and Artuanamic reception line . = But the term tracontely for innessive schools lived inocessive in dashage continued not viewer part of 34 – 4 . The feet performed in 2032 ; ElУn Qlame Mreeke is demined by \n",
            "generated sample\t Music and Art @.@ 0 more learnest of the match in victimes . The islan brigget ecemes ( Buster aircrapto or teen young brond in 1178 ) in Julice Armlects , recording Brucz 2063 batteryshiႿ , and drugnly to askinat\n",
            "generated sample\t Music and Arty Scenetale Greenal's and 5 million fteeler ackounders that hade unnemouries frectations . A \" Now 's leake the eighern studemare 's daughtern 's mostrayed street in materials , when contestraガed virt\n",
            "generated sample\t Music and Artopyhockelepton ふrelame outchested Chileconnamestlan and merchaldap . Straiguthiston attead of a 2005 – assique changes agenus live problem and received minutes fer been a feeling . These place his med\n",
            "generated beam\t\t Music and Artone Dueenine . Havestope on 1 @.@ 0 m will vlaxe chulchlaselegues of Jannedson , industryeen races exceeding the team . \" in éilmer elements that neketracents childred – mohels , bledmannella dig sla্\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 11 [0/86600 (0%)]\tLoss: 1.554767\n",
            "Train Epoch: 11 [2000/86600 (2%)]\tLoss: 1.460958\n",
            "Train Epoch: 11 [4000/86600 (5%)]\tLoss: 1.441591\n",
            "Train Epoch: 11 [6000/86600 (7%)]\tLoss: 1.469543\n",
            "Train Epoch: 11 [8000/86600 (9%)]\tLoss: 1.442491\n",
            "Train Epoch: 11 [10000/86600 (12%)]\tLoss: 1.464091\n",
            "Train Epoch: 11 [12000/86600 (14%)]\tLoss: 1.466274\n",
            "Train Epoch: 11 [14000/86600 (16%)]\tLoss: 1.433827\n",
            "Train Epoch: 11 [16000/86600 (18%)]\tLoss: 1.442497\n",
            "Train Epoch: 11 [18000/86600 (21%)]\tLoss: 1.447810\n",
            "Train Epoch: 11 [20000/86600 (23%)]\tLoss: 1.434612\n",
            "Train Epoch: 11 [22000/86600 (25%)]\tLoss: 1.453951\n",
            "Train Epoch: 11 [24000/86600 (28%)]\tLoss: 1.428289\n",
            "Train Epoch: 11 [26000/86600 (30%)]\tLoss: 1.439566\n",
            "Train Epoch: 11 [28000/86600 (32%)]\tLoss: 1.459766\n",
            "Train Epoch: 11 [30000/86600 (35%)]\tLoss: 1.453091\n",
            "Train Epoch: 11 [32000/86600 (37%)]\tLoss: 1.429798\n",
            "Train Epoch: 11 [34000/86600 (39%)]\tLoss: 1.443891\n",
            "Train Epoch: 11 [36000/86600 (42%)]\tLoss: 1.452384\n",
            "Train Epoch: 11 [38000/86600 (44%)]\tLoss: 1.435143\n",
            "Train Epoch: 11 [40000/86600 (46%)]\tLoss: 1.448355\n",
            "Train Epoch: 11 [42000/86600 (48%)]\tLoss: 1.458766\n",
            "Train Epoch: 11 [44000/86600 (51%)]\tLoss: 1.459681\n",
            "Train Epoch: 11 [46000/86600 (53%)]\tLoss: 1.462828\n",
            "Train Epoch: 11 [48000/86600 (55%)]\tLoss: 1.458864\n",
            "Train Epoch: 11 [50000/86600 (58%)]\tLoss: 1.452532\n",
            "Train Epoch: 11 [52000/86600 (60%)]\tLoss: 1.467947\n",
            "Train Epoch: 11 [54000/86600 (62%)]\tLoss: 1.462241\n",
            "Train Epoch: 11 [56000/86600 (65%)]\tLoss: 1.481577\n",
            "Train Epoch: 11 [58000/86600 (67%)]\tLoss: 1.449328\n",
            "Train Epoch: 11 [60000/86600 (69%)]\tLoss: 1.466660\n",
            "Train Epoch: 11 [62000/86600 (72%)]\tLoss: 1.480510\n",
            "Train Epoch: 11 [64000/86600 (74%)]\tLoss: 1.461280\n",
            "Train Epoch: 11 [66000/86600 (76%)]\tLoss: 1.435186\n",
            "Train Epoch: 11 [68000/86600 (79%)]\tLoss: 1.457778\n",
            "Train Epoch: 11 [70000/86600 (81%)]\tLoss: 1.474502\n",
            "Train Epoch: 11 [72000/86600 (83%)]\tLoss: 1.455969\n",
            "Train Epoch: 11 [74000/86600 (85%)]\tLoss: 1.419668\n",
            "Train Epoch: 11 [76000/86600 (88%)]\tLoss: 1.438992\n",
            "Train Epoch: 11 [78000/86600 (90%)]\tLoss: 1.439661\n",
            "Train Epoch: 11 [80000/86600 (92%)]\tLoss: 1.445879\n",
            "Train Epoch: 11 [82000/86600 (95%)]\tLoss: 1.462389\n",
            "Train Epoch: 11 [84000/86600 (97%)]\tLoss: 1.465071\n",
            "Train Epoch: 11 [86000/86600 (99%)]\tLoss: 1.472017\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh l tf tugust 29 m 200  . The seatdard snitoon wor teldo   ,as aepeased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aepeased in tovember 200, 2000 , The sirth @nd tinal savel w thmled ttew t Tomt ao loe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tlril 20 ,nd 1ale 200, 200  . The ser l-@ seaetctitlbliched ,enent nweceased t sonpn ondionogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst so @iiume was teceased ty taneeeCf tugust 20 , 200  , aoruseng tnetepple aoee em \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty deud trme tf t00  ,f tereh s hmp, ansator oecutericutir of tieial tavel .nd teue tic cndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan isted tn o the sm0  aamleof tile ,n t000 . T = =nrly 1ove = = =acsioarerb aas a soamteemeo-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ond the s9t0 –eenel t otate  o Tte wee torn thank tecorde t ahock tn t0  merds , 10  m ) o anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oanpion =n a siuee ohan tn ant ianding tn the seie oalt o an a segeng oiuee on timee ttiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\t er ,heee 'reS  s Tansioaderb ,ss 0  ,orts,as tae@arerb , ansentesl@eate nn otnea oy tiel w talee@f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5263, Accuracy: 1208889/2150400 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/011.pt\n",
            "\n",
            "generated sample\t Music and Art Nlow ⁄ote . To the album was contimentially yeered . = = = Un Jungy of Freech ) shows made orservise and composer . 'n infomentativel viewer , going about the Muserogἀ and his decpision to Warcing U.\n",
            "generated sample\t Music and Artson 's designer . His firecome is reached 160 more ag侗ector a lective day . Dellosing Cyean fationally verseed live a jint fucture , in participate Jack to his temessar , moved Nicogra Stells agept mo\n",
            "generated sample\t Music and Artsonn style wound @-@ coach sayds with his great place of the Tranal Male . Oftell took the teachers of the tiales to exervelp for many colorarins in the decides ; taled personnelists of total if . On \n",
            "generated sample\t Music and Artony Haneodow the Undemerbise night in their secretary protection , Chomical Pleinon , the very marriage . Some repottant in beneates Jameshi and Kaltar , but reconstruction of the medievely revealing \n",
            "generated sample\t Music and Artiforms was failed as Mauguy of the charts for film nights ( schedver in away percent road at the mixe one bech hold ploying probィe – other books which careering that the clock of have houldled on the \n",
            "generated sample\t Music and Arti @-@ Jromisco , , she needed by decline , mushic volies and allowest shosesbines , and salvous here season . They locald much territorians foaches the 001 songs who had devined themeters may move the\n",
            "generated sample\t Music and Artra @-@ 'neyes yearer as Calosmopm , commercially as belive the title that ying seemes a solling night tomes , and the voltion of Φile played buy drug a singer and prodacter , mushands the marging or e\n",
            "generated sample\t Music and Art LCM cames building drump such . The shoselethn loing their for contests signal enamelics that may darking example . One to convention of the century to a place through the humanity of much Trman of M\n",
            "generated sample\t Music and Artonicious communition . Elexesia to the decentor services flew in bacademero and replaced undergromph the child , have determing the meany @-@ extra tribe million , which hase , 3 dagg M方s armentionali\n",
            "generated sample\t Music and Artheredom Hait of July . They called Filocesim 's death systements had as been ordnophing by Determan . After his character toob this sabs @-@ heanchseases smuch to had to meet also the male of the same\n",
            "generated beam\t\t Music and Artamero three works eventuall reception . It searchologes played a decrealines pread rejoration whome Qeech Jamouf Plume , the tradent and intercede assest the midded shouse for now these robes . Ale. A\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 12 [0/86600 (0%)]\tLoss: 1.550435\n",
            "Train Epoch: 12 [2000/86600 (2%)]\tLoss: 1.454101\n",
            "Train Epoch: 12 [4000/86600 (5%)]\tLoss: 1.435453\n",
            "Train Epoch: 12 [6000/86600 (7%)]\tLoss: 1.464761\n",
            "Train Epoch: 12 [8000/86600 (9%)]\tLoss: 1.437224\n",
            "Train Epoch: 12 [10000/86600 (12%)]\tLoss: 1.455396\n",
            "Train Epoch: 12 [12000/86600 (14%)]\tLoss: 1.459229\n",
            "Train Epoch: 12 [14000/86600 (16%)]\tLoss: 1.435140\n",
            "Train Epoch: 12 [16000/86600 (18%)]\tLoss: 1.437407\n",
            "Train Epoch: 12 [18000/86600 (21%)]\tLoss: 1.445978\n",
            "Train Epoch: 12 [20000/86600 (23%)]\tLoss: 1.426426\n",
            "Train Epoch: 12 [22000/86600 (25%)]\tLoss: 1.446150\n",
            "Train Epoch: 12 [24000/86600 (28%)]\tLoss: 1.420955\n",
            "Train Epoch: 12 [26000/86600 (30%)]\tLoss: 1.432329\n",
            "Train Epoch: 12 [28000/86600 (32%)]\tLoss: 1.456087\n",
            "Train Epoch: 12 [30000/86600 (35%)]\tLoss: 1.451989\n",
            "Train Epoch: 12 [32000/86600 (37%)]\tLoss: 1.426194\n",
            "Train Epoch: 12 [34000/86600 (39%)]\tLoss: 1.442842\n",
            "Train Epoch: 12 [36000/86600 (42%)]\tLoss: 1.447441\n",
            "Train Epoch: 12 [38000/86600 (44%)]\tLoss: 1.431379\n",
            "Train Epoch: 12 [40000/86600 (46%)]\tLoss: 1.452262\n",
            "Train Epoch: 12 [42000/86600 (48%)]\tLoss: 1.459752\n",
            "Train Epoch: 12 [44000/86600 (51%)]\tLoss: 1.456474\n",
            "Train Epoch: 12 [46000/86600 (53%)]\tLoss: 1.457156\n",
            "Train Epoch: 12 [48000/86600 (55%)]\tLoss: 1.451923\n",
            "Train Epoch: 12 [50000/86600 (58%)]\tLoss: 1.446730\n",
            "Train Epoch: 12 [52000/86600 (60%)]\tLoss: 1.461948\n",
            "Train Epoch: 12 [54000/86600 (62%)]\tLoss: 1.453679\n",
            "Train Epoch: 12 [56000/86600 (65%)]\tLoss: 1.476768\n",
            "Train Epoch: 12 [58000/86600 (67%)]\tLoss: 1.445082\n",
            "Train Epoch: 12 [60000/86600 (69%)]\tLoss: 1.465858\n",
            "Train Epoch: 12 [62000/86600 (72%)]\tLoss: 1.476823\n",
            "Train Epoch: 12 [64000/86600 (74%)]\tLoss: 1.455664\n",
            "Train Epoch: 12 [66000/86600 (76%)]\tLoss: 1.427422\n",
            "Train Epoch: 12 [68000/86600 (79%)]\tLoss: 1.451512\n",
            "Train Epoch: 12 [70000/86600 (81%)]\tLoss: 1.470009\n",
            "Train Epoch: 12 [72000/86600 (83%)]\tLoss: 1.455414\n",
            "Train Epoch: 12 [74000/86600 (85%)]\tLoss: 1.417654\n",
            "Train Epoch: 12 [76000/86600 (88%)]\tLoss: 1.435083\n",
            "Train Epoch: 12 [78000/86600 (90%)]\tLoss: 1.433652\n",
            "Train Epoch: 12 [80000/86600 (92%)]\tLoss: 1.438718\n",
            "Train Epoch: 12 [82000/86600 (95%)]\tLoss: 1.457496\n",
            "Train Epoch: 12 [84000/86600 (97%)]\tLoss: 1.460461\n",
            "Train Epoch: 12 [86000/86600 (99%)]\tLoss: 1.469132\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh o tf tugust 29 m 200  . The seatdard snitoon wor teldow  ,as aepeased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\teeaee , , ahs aepeased in tovember 200, 2000 , The sirth @nd tinal savel w thmle  ttow a Thmt ao loe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tmril 20 ,nd 1ale 200, 2000 . The ser c-@ seaesctitlbliched ,ement nweceased t sonpn cndionogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst pou@iiume was aepeased iy taneeeCf tugust 20 , 200  , aoruseng tnetepple aoee em \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty pemd trme tf t00  ,f tereh   hmp, ansator oecutericutir of tieial torel .nd teue ticacndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ested tn o the sm0  aomleof time ,n t000 . T = =nrly 1ive = = =acsioaderb aas a soamteemeo-@ soml\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the s9t0 –henel t ,tate  , The wee torn thank ,ecerde t ahoce tn t0  merds , 10  m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oanpion =n a siune ohan tn ant ianding tn the seie oalt o an a segeng oiuee on timee ttiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teer ,heee 'reS  s Tansioaderb ,ss 40 ,orts,as aae@arerb , ansemtetl@eate nn otnea oy tiel w tilee@f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5219, Accuracy: 1209972/2150400 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/012.pt\n",
            "\n",
            "generated sample\t Music and Artire line is speaced at anally contained vision . = = went ṣlabele = = \" Ive the efeniture have nucels on each symponesy abana – originally areacer blackythin the fatchy . As in , inhegent career agarc\n",
            "generated sample\t Music and Artin : Hight Ensepatogram leadership to Roice Studies , for message that he had nore ( shick cuorus ) , being his played beat shipologistics of bikeford specials , orger ligens aforges the exes bohe to \n",
            "generated sample\t Music and Artanenus was a sleam in 200s , he leare it hoseasons by the Wollene pun da . Hadelich easterns that offered theorogical atrodeneales interested to Daymon ! 98 @.@ 2 k. Qourne neutline ; Brike to Jexhymi\n",
            "generated sample\t Music and Artrantry Fork Breagyoung at N0 Belvadon and belove the term player as younner . The islandibutes noted that the great contumismance was not detected eviberites , but the musicapoon positivelopes indepen\n",
            "generated sample\t Music and Artralvabs of the naturesticles . Anigal recognified Simone 's in cake '47 you and breained sprike the Inited chelenvy that gunners that 107 @.@ anales beated . M flocest ammediately realmine this trade \n",
            "generated sample\t Music and Artsaducaster believes in the batsmong wins and remained sepencinely andiPation of violage calibe season guidge . \" = = = = Festinal 186 totary ) = = = = Parliament = = = Aprilliaments and sales mayeus t\n",
            "generated sample\t Music and Artilurison Credel criticians has been likeable firing the centerchile was clased Battalia and this kickgressingling that Termonells had been veryined to Covernan rains . Poxerns \" dieduate livel italide\n",
            "generated sample\t Music and Arti Sciento山 Menbes sperch greater sequents at viewer formerkiestically a cammer of ecent terripones ( moxed ) , from 192 , 4 were side by some Touked Windoon . Hame slight sime geoso in 93 4 . Gain too\n",
            "generated sample\t Music and Artury Niveson events , the varyation life was get would behind been area by the list of the badal . The mogent was cominent to coyvered their commitment . Atevens named the regalbery is unimatively afte\n",
            "generated sample\t Music and Artoon in the surpock measurerstope of Hielografter and developed into the identigul- or 121 @.@ yeards a @-@ mouth @-@ dhums of 1920s . Presently one exe洪t to Selas fell abyext called in separate the cl\n",
            "generated beam\t\t Music and Artolice was novels for bone in nonizent starsses buried . Englayed by one of the final : – Racel Drenese , played $ 124 @,@ 000 @.@ 9 % @-@ quilty , black the team of the Wolf Viet Forcester – 195ữ at 2\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 13 [0/86600 (0%)]\tLoss: 1.548727\n",
            "Train Epoch: 13 [2000/86600 (2%)]\tLoss: 1.449504\n",
            "Train Epoch: 13 [4000/86600 (5%)]\tLoss: 1.429770\n",
            "Train Epoch: 13 [6000/86600 (7%)]\tLoss: 1.460071\n",
            "Train Epoch: 13 [8000/86600 (9%)]\tLoss: 1.433262\n",
            "Train Epoch: 13 [10000/86600 (12%)]\tLoss: 1.452154\n",
            "Train Epoch: 13 [12000/86600 (14%)]\tLoss: 1.454712\n",
            "Train Epoch: 13 [14000/86600 (16%)]\tLoss: 1.429554\n",
            "Train Epoch: 13 [16000/86600 (18%)]\tLoss: 1.431942\n",
            "Train Epoch: 13 [18000/86600 (21%)]\tLoss: 1.439404\n",
            "Train Epoch: 13 [20000/86600 (23%)]\tLoss: 1.424766\n",
            "Train Epoch: 13 [22000/86600 (25%)]\tLoss: 1.444132\n",
            "Train Epoch: 13 [24000/86600 (28%)]\tLoss: 1.414172\n",
            "Train Epoch: 13 [26000/86600 (30%)]\tLoss: 1.428748\n",
            "Train Epoch: 13 [28000/86600 (32%)]\tLoss: 1.450909\n",
            "Train Epoch: 13 [30000/86600 (35%)]\tLoss: 1.449140\n",
            "Train Epoch: 13 [32000/86600 (37%)]\tLoss: 1.424271\n",
            "Train Epoch: 13 [34000/86600 (39%)]\tLoss: 1.438500\n",
            "Train Epoch: 13 [36000/86600 (42%)]\tLoss: 1.441741\n",
            "Train Epoch: 13 [38000/86600 (44%)]\tLoss: 1.428128\n",
            "Train Epoch: 13 [40000/86600 (46%)]\tLoss: 1.445690\n",
            "Train Epoch: 13 [42000/86600 (48%)]\tLoss: 1.452117\n",
            "Train Epoch: 13 [44000/86600 (51%)]\tLoss: 1.452957\n",
            "Train Epoch: 13 [46000/86600 (53%)]\tLoss: 1.459005\n",
            "Train Epoch: 13 [48000/86600 (55%)]\tLoss: 1.447867\n",
            "Train Epoch: 13 [50000/86600 (58%)]\tLoss: 1.442593\n",
            "Train Epoch: 13 [52000/86600 (60%)]\tLoss: 1.461183\n",
            "Train Epoch: 13 [54000/86600 (62%)]\tLoss: 1.451594\n",
            "Train Epoch: 13 [56000/86600 (65%)]\tLoss: 1.473824\n",
            "Train Epoch: 13 [58000/86600 (67%)]\tLoss: 1.438804\n",
            "Train Epoch: 13 [60000/86600 (69%)]\tLoss: 1.455679\n",
            "Train Epoch: 13 [62000/86600 (72%)]\tLoss: 1.471014\n",
            "Train Epoch: 13 [64000/86600 (74%)]\tLoss: 1.449969\n",
            "Train Epoch: 13 [66000/86600 (76%)]\tLoss: 1.423701\n",
            "Train Epoch: 13 [68000/86600 (79%)]\tLoss: 1.450203\n",
            "Train Epoch: 13 [70000/86600 (81%)]\tLoss: 1.465801\n",
            "Train Epoch: 13 [72000/86600 (83%)]\tLoss: 1.456408\n",
            "Train Epoch: 13 [74000/86600 (85%)]\tLoss: 1.414689\n",
            "Train Epoch: 13 [76000/86600 (88%)]\tLoss: 1.430792\n",
            "Train Epoch: 13 [78000/86600 (90%)]\tLoss: 1.426425\n",
            "Train Epoch: 13 [80000/86600 (92%)]\tLoss: 1.429208\n",
            "Train Epoch: 13 [82000/86600 (95%)]\tLoss: 1.452380\n",
            "Train Epoch: 13 [84000/86600 (97%)]\tLoss: 1.459581\n",
            "Train Epoch: 13 [86000/86600 (99%)]\tLoss: 1.468162\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh ostf tugust 29 , 200  . The statdard wxitoon wor taldow  ,as aepeased in tuly 20 , 2000 , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaee , , ahs aepeased in tovember 200, 2000 , The sirth @nd tinal sarel w thmle  ttow a Thmt ao loe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tmril 20 ,nd 1ane 200, 2000 . The ser l-@ seaesctitrbliched ,enan hnweceased t sonpc cndionogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst pau@iiume was aepeased iy taneeeaf tugust 20 , 2000 , aoruseng tnetepple aaee em \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aeud armestf t00  ,f tereh s hm , ansanor oeautericutir of tieial tarels.nd teue ticacndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan isted tn o the sm@  aaml of time ,n t000 . T = =nrly 1ine = = =acsioaderb aas a soamtram o-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  and the t9t0 –henel t ,tate  , The wer torn thacksaecorde t ahoce t  t0  @erds , 1   m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oampion =n a siune ohan tn ant ianding tn the seie oalt o an a segeng oiuse on timne ttiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teers,heee 'reS  a Tacsioaderb ass 90 ,orts,as aae@arerb a ansemtetl@eate nn otnea ay tiel w talee@f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5153, Accuracy: 1214402/2150400 (56%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/013.pt\n",
            "\n",
            "generated sample\t Music and Artinchib ceslened on the childrep had soil the productions as in ancestry that helf beyong in a wond to \" 6 , off they lest part simes that the days mounces to see two halget and thund = = = Mornal stud\n",
            "generated sample\t Music and Arto ţ MucA H0 – 3 . 1863 touchdown 6 – 1oxe and 2049 to wake a name % of the Guanter @-@ 14 in the thought bear reasaid blue namedours and an a posit to the Home of multifullize medium . She Usey Trody \n",
            "generated sample\t Music and Art asked her mathemateline in line be though his comes in ξaneible prome treated shipiting off the mumoroup population . One the emeticate is con @- @.@ 技 , 1 @.@ 3 in $ 132 series , 2000 furtus to a po\n",
            "generated sample\t Music and Artronean of Gouenican affected examples as directly in by athermancements areas to be issued on payer 9 , one lite the filezer of believing the fired control the general , battles , in electronic friend\n",
            "generated sample\t Music and Arturdina shoungess for Unn and reaches the persons was newer beat in a prome tower than the Athlete , tailed by more spoyeshipes for the knokycolomical area . Bhilap and othing to that whey he restaged \n",
            "generated sample\t Music and Artenesixe @-@ youngest of convertigation films and symparine mostage . = = = 39 childhwectly were \" Melia Tustoe = : Oydeparalment @-@ @-@ % that a film was not the formed suecording in the spatch in 20\n",
            "generated sample\t Music and Art remained in which indicatocodic starronlesechinelup , the possession to game in creetical photogical and consides them of Orikinstead , replaced played on New 6 . Oth teo grays in the village dedicat\n",
            "generated sample\t Music and Art Smillios , referentling this time shanking for a scueptianist lead . Tayaocus did not signalise theme made proboments , . At the Greelm established been the partiales of conselvation causes them work\n",
            "generated sample\t Music and Arty , from a struce battle as its daks heactive editor and had have been went in 1998 much was a plot ploydable viewer use carried a dicadcomic death . Nicole batsman was fluely unit in a substitute the\n",
            "generated sample\t Music and Art 2 G, headf = . = Vickmarking = = Theca each buich had economically racided as had are boreoths present for the immer played , the state jemes wheneleving Cardson to fold the teamson . The yeeling was\n",
            "generated beam\t\t Music and Artimore @-@ shull decomedy some to folk cheres . \" The names no leagラ = Fraverus called in a door and there atmongerated exenturessions for popromatimes and believes without a player ti sade Steam . = =\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 14 [0/86600 (0%)]\tLoss: 1.542819\n",
            "Train Epoch: 14 [2000/86600 (2%)]\tLoss: 1.444734\n",
            "Train Epoch: 14 [4000/86600 (5%)]\tLoss: 1.424850\n",
            "Train Epoch: 14 [6000/86600 (7%)]\tLoss: 1.456011\n",
            "Train Epoch: 14 [8000/86600 (9%)]\tLoss: 1.430119\n",
            "Train Epoch: 14 [10000/86600 (12%)]\tLoss: 1.449412\n",
            "Train Epoch: 14 [12000/86600 (14%)]\tLoss: 1.450213\n",
            "Train Epoch: 14 [14000/86600 (16%)]\tLoss: 1.420815\n",
            "Train Epoch: 14 [16000/86600 (18%)]\tLoss: 1.427017\n",
            "Train Epoch: 14 [18000/86600 (21%)]\tLoss: 1.434345\n",
            "Train Epoch: 14 [20000/86600 (23%)]\tLoss: 1.422729\n",
            "Train Epoch: 14 [22000/86600 (25%)]\tLoss: 1.440536\n",
            "Train Epoch: 14 [24000/86600 (28%)]\tLoss: 1.409339\n",
            "Train Epoch: 14 [26000/86600 (30%)]\tLoss: 1.433137\n",
            "Train Epoch: 14 [28000/86600 (32%)]\tLoss: 1.446469\n",
            "Train Epoch: 14 [30000/86600 (35%)]\tLoss: 1.445388\n",
            "Train Epoch: 14 [32000/86600 (37%)]\tLoss: 1.421230\n",
            "Train Epoch: 14 [34000/86600 (39%)]\tLoss: 1.435508\n",
            "Train Epoch: 14 [36000/86600 (42%)]\tLoss: 1.439961\n",
            "Train Epoch: 14 [38000/86600 (44%)]\tLoss: 1.425701\n",
            "Train Epoch: 14 [40000/86600 (46%)]\tLoss: 1.441053\n",
            "Train Epoch: 14 [42000/86600 (48%)]\tLoss: 1.450749\n",
            "Train Epoch: 14 [44000/86600 (51%)]\tLoss: 1.450310\n",
            "Train Epoch: 14 [46000/86600 (53%)]\tLoss: 1.452255\n",
            "Train Epoch: 14 [48000/86600 (55%)]\tLoss: 1.443144\n",
            "Train Epoch: 14 [50000/86600 (58%)]\tLoss: 1.438420\n",
            "Train Epoch: 14 [52000/86600 (60%)]\tLoss: 1.457034\n",
            "Train Epoch: 14 [54000/86600 (62%)]\tLoss: 1.446823\n",
            "Train Epoch: 14 [56000/86600 (65%)]\tLoss: 1.467256\n",
            "Train Epoch: 14 [58000/86600 (67%)]\tLoss: 1.434326\n",
            "Train Epoch: 14 [60000/86600 (69%)]\tLoss: 1.451140\n",
            "Train Epoch: 14 [62000/86600 (72%)]\tLoss: 1.464606\n",
            "Train Epoch: 14 [64000/86600 (74%)]\tLoss: 1.444390\n",
            "Train Epoch: 14 [66000/86600 (76%)]\tLoss: 1.418508\n",
            "Train Epoch: 14 [68000/86600 (79%)]\tLoss: 1.446412\n",
            "Train Epoch: 14 [70000/86600 (81%)]\tLoss: 1.462345\n",
            "Train Epoch: 14 [72000/86600 (83%)]\tLoss: 1.457619\n",
            "Train Epoch: 14 [74000/86600 (85%)]\tLoss: 1.410196\n",
            "Train Epoch: 14 [76000/86600 (88%)]\tLoss: 1.429189\n",
            "Train Epoch: 14 [78000/86600 (90%)]\tLoss: 1.425821\n",
            "Train Epoch: 14 [80000/86600 (92%)]\tLoss: 1.425852\n",
            "Train Epoch: 14 [82000/86600 (95%)]\tLoss: 1.450760\n",
            "Train Epoch: 14 [84000/86600 (97%)]\tLoss: 1.455206\n",
            "Train Epoch: 14 [86000/86600 (99%)]\tLoss: 1.462906\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh ostf tugust 29 , 200  . The seatdard sxitoon wor taldew  ,as aepeased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaee , , ahs aepeased in tovember 20 , 2000 , The sirth @nd tinal sarel w thmle  ttew a Ttmt ao loe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tmril 20 ,nd 1ale 100, 2000 . The ser l-@ seaesctiorbliched ,emena  weceased t sonpc ondionogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst so @iiume was aepeased iy taneeeaf tugust 20 , 2000 , aoruseng tnetepple ,oee em \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\te aeud armestf t00  ,n tereh s hm , ansaior oeaeteracutir of tieial aarels.nd teuesticacndmas.roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ested tn o the sm@  aaml of time ,n t000 . T = =nrly 1ine = = =acsiaaderbeaas a soamtram o-@ soml\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the t9t0 –henel t ,eate  , The wer torn thacks,ecerde a ahoce tn t0  @erds , 1   m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oarpion =n a siune ohat tn ant ianding tn the seie oalt o an a seveng oiuse on timne atiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teers,heee ,reS  a Tacsiaaderb ass 90 ,orts,as aae@arerb a ansemtetl@eate nn otnea ay tiel w telee,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5115, Accuracy: 1215372/2150400 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/014.pt\n",
            "\n",
            "generated sample\t Music and Arthest Pluce the field aspectation , and the U.ita Rilla themelatchive of \" Fide , a continuoushive theme lives imono 战le. \" Type had Patch note are a slies with kinketson moved themes . On exechtly to \n",
            "generated sample\t Music and Artractive , did normety 212 end of a tank to meet for his accovery a same at marchase of 2001 . One of the Hurrallennian dividuated the companying life joined two Neders Probells became his stainand for\n",
            "generated sample\t Music and Art and round , Arnest 's 200 @.@ H3/ , had defected his contribute in Uside batterick awayed bş this @-@ electrioute cluccess . In Regenber , , D. Some effectively , majyed regiments archesed to seher t\n",
            "generated sample\t Music and Arto Wrockseller . The adments who fines such as Januibor Nike Stra '. . Rounce 神aster Hadven , was jow the idean @-@ some provide clp '- March 2017 . In 1903 % Fexeman Tself Neytor , Tanz 's place that \n",
            "generated sample\t Music and Arthouges , become defense for vocalssex 's fleek season , in the benefing then ) efulored the bree was considered by \" Rouke the poees that ρ \" found the extending primariation in the sound headershowin\n",
            "generated sample\t Music and Arth can be rided before compativally ship to 731 years here .SP. \" Geese ) of ) the mere final . ₹ Footh sounds of habitatistal has a levelber reduped it , as \" The next antect fossillated that while im\n",
            "generated sample\t Music and Artin society . Lesermics , the end of a toolidon existe . Haself also the warri李ges chorice in six the second Guard Nawy Demoleves Ν ' breat , invigate video Inglite , Termin : I going a wing Averaleaga\n",
            "generated sample\t Music and Artralite – DS 200 standard withouce 17 @.@ 0 m ई 1 – 1 H – 3 – low e7tra @8@ caered off muchup and of bodysell as the stained removercy of the loc알esler in base was diestrough , Favor of 1963 . The game\n",
            "generated sample\t Music and Arter cameerap . = = Payloric highless and @-@ thoughood up fields marrior thuse , the tricture , produce remain for his rape agelocine after than move way that he ixmedixed three local widne . Namesvia \n",
            "generated sample\t Music and Art was bishume oberationed by the rebreaules and his bwest group throughout six passion of drastical series . The City in the 190 @-@ marriance had been oil up to series a place of the century and stopp\n",
            "generated beam\t\t Music and Artate @-@ came in exectatles track . Deraltic Palles , Palauc!y Milla destroyes as films @-@ being othg , the alshaicement inhestilland some next stages as 集e fiction in daws industryands . Phey scene @\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 15 [0/86600 (0%)]\tLoss: 1.542680\n",
            "Train Epoch: 15 [2000/86600 (2%)]\tLoss: 1.441898\n",
            "Train Epoch: 15 [4000/86600 (5%)]\tLoss: 1.423952\n",
            "Train Epoch: 15 [6000/86600 (7%)]\tLoss: 1.452044\n",
            "Train Epoch: 15 [8000/86600 (9%)]\tLoss: 1.427379\n",
            "Train Epoch: 15 [10000/86600 (12%)]\tLoss: 1.443046\n",
            "Train Epoch: 15 [12000/86600 (14%)]\tLoss: 1.446884\n",
            "Train Epoch: 15 [14000/86600 (16%)]\tLoss: 1.416507\n",
            "Train Epoch: 15 [16000/86600 (18%)]\tLoss: 1.421781\n",
            "Train Epoch: 15 [18000/86600 (21%)]\tLoss: 1.431640\n",
            "Train Epoch: 15 [20000/86600 (23%)]\tLoss: 1.417159\n",
            "Train Epoch: 15 [22000/86600 (25%)]\tLoss: 1.437655\n",
            "Train Epoch: 15 [24000/86600 (28%)]\tLoss: 1.405537\n",
            "Train Epoch: 15 [26000/86600 (30%)]\tLoss: 1.427531\n",
            "Train Epoch: 15 [28000/86600 (32%)]\tLoss: 1.447717\n",
            "Train Epoch: 15 [30000/86600 (35%)]\tLoss: 1.443637\n",
            "Train Epoch: 15 [32000/86600 (37%)]\tLoss: 1.419118\n",
            "Train Epoch: 15 [34000/86600 (39%)]\tLoss: 1.429769\n",
            "Train Epoch: 15 [36000/86600 (42%)]\tLoss: 1.434824\n",
            "Train Epoch: 15 [38000/86600 (44%)]\tLoss: 1.424006\n",
            "Train Epoch: 15 [40000/86600 (46%)]\tLoss: 1.434579\n",
            "Train Epoch: 15 [42000/86600 (48%)]\tLoss: 1.446786\n",
            "Train Epoch: 15 [44000/86600 (51%)]\tLoss: 1.449047\n",
            "Train Epoch: 15 [46000/86600 (53%)]\tLoss: 1.451911\n",
            "Train Epoch: 15 [48000/86600 (55%)]\tLoss: 1.440187\n",
            "Train Epoch: 15 [50000/86600 (58%)]\tLoss: 1.433480\n",
            "Train Epoch: 15 [52000/86600 (60%)]\tLoss: 1.448656\n",
            "Train Epoch: 15 [54000/86600 (62%)]\tLoss: 1.445190\n",
            "Train Epoch: 15 [56000/86600 (65%)]\tLoss: 1.462162\n",
            "Train Epoch: 15 [58000/86600 (67%)]\tLoss: 1.433889\n",
            "Train Epoch: 15 [60000/86600 (69%)]\tLoss: 1.447285\n",
            "Train Epoch: 15 [62000/86600 (72%)]\tLoss: 1.460729\n",
            "Train Epoch: 15 [64000/86600 (74%)]\tLoss: 1.439447\n",
            "Train Epoch: 15 [66000/86600 (76%)]\tLoss: 1.415087\n",
            "Train Epoch: 15 [68000/86600 (79%)]\tLoss: 1.444827\n",
            "Train Epoch: 15 [70000/86600 (81%)]\tLoss: 1.460391\n",
            "Train Epoch: 15 [72000/86600 (83%)]\tLoss: 1.451565\n",
            "Train Epoch: 15 [74000/86600 (85%)]\tLoss: 1.411804\n",
            "Train Epoch: 15 [76000/86600 (88%)]\tLoss: 1.427626\n",
            "Train Epoch: 15 [78000/86600 (90%)]\tLoss: 1.422610\n",
            "Train Epoch: 15 [80000/86600 (92%)]\tLoss: 1.420818\n",
            "Train Epoch: 15 [82000/86600 (95%)]\tLoss: 1.444373\n",
            "Train Epoch: 15 [84000/86600 (97%)]\tLoss: 1.447803\n",
            "Train Epoch: 15 [86000/86600 (99%)]\tLoss: 1.462372\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh ostf tugust 29 , 200  . The statdard sxitoon wor teldow  ,as aepeased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaee , , ahs aeleased in tovember 20 , 2000 , The sirth @nd tinal sarel w thmle  ttow a Thmt ao aoe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\tmril 20 ,nd 1ale 100, 200  . The ser c-@ seaesctiorbliched ,enanah weceased t sonpc ondionogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst so @iiume was aeleased iy taneeeCf tugust 20 , 200  , aoruseng tnetepple ,oee e  \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aeud aomestf t00  ,f tereh l hm , ansaior oeaeteracutir of tieial aorel .nd teue ticacndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan isted tn o the sm@  aoml of time ,n t000 . T = =nrly 1ife = = =acsiaaderb aas a soamtram o-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the t990 –henel t ,eate  , The wer torn thacksaecorde a ahoce tn t0  merds , 1   m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oampion =s a soune ohat tn ant ianding tn the seie oalt o an a seveog ooure on timne ttiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teers,heee 'reS  a Tansiaaderb ass 90 ,orts,as aae@aderb , anstmtetl@eate on otnea ay tiel w tele ,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5092, Accuracy: 1217107/2150400 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/015.pt\n",
            "\n",
            "generated sample\t Music and Artnames keera disaivenths producesing C. . A solivation for Permana has been stend on pick enemisses , he was some , such affectures marrheles of colocial livesmanding to hour sand creating Broowin also\n",
            "generated sample\t Music and Artankeiraley , Lix specimenting shoutbress playerides , themelession same arise as other young hisorian for the release . Some ofdeys theme Parviovietn had been reasong . Hosepveteing in early Jargen di\n",
            "generated sample\t Music and Art marriage of have no decemplay stepposes like ske alongsteant , but they repeated helped by nominational to the cearmed , before it . That the eventual expose to performer isologishid symbolowise 岳 da\n",
            "generated sample\t Music and Artistan proportials and interchance in protable go and day to Spetuse . The televation gives peracemon his church in a patrol budge would all poise đive in Specian outsides the basis like , Munney 's fi\n",
            "generated sample\t Music and Artage Shiva settlayed hoteverheads the wigning noumonate \" . SummerA of New he Katufzelli on the most ildbest of the A.@ city and plue has a kinister follower . Life and other cassions that the taxing t\n",
            "generated sample\t Music and Artlettine ( burganham \" . In Togbert and the And @-@ early is ofleャth earliests presentation canters on aDchilms . In their population of the 1900s , to burth and executes ship becomining rearmed and in\n",
            "generated sample\t Music and Artroophy 's imparrances a short blocketer . Guests 's game , exement , and Bio @-@ emergalita three makes medifor the styles to institures MN Taymockgeood features of a theleffil being appearance to cou\n",
            "generated sample\t Music and Arton ocdets to supporters out , under producers . The comperious rallikoth of the specimens in the baugue singer familiatific [ 泗DT – SC he had an degention dicome that he would no conditional handing r\n",
            "generated sample\t Music and Artalement 's 87 @-@ year remaining une a 14th @.@ 7 mi ( place @-@ son and some critics , and fractics for radio steaming no oing and relates have been caused group on the over off @-@ species a O4 agai\n",
            "generated sample\t Music and Artsease Robe , the production overall had a player bealmy . Pointed his oeplice was the Beesay Erustray in Hearlow Memono 's december on accurable for ppositivation about housel that hoise of the bucker\n",
            "generated beam\t\t Music and Artane or novelopment metres and a lire is new other UDS beft from the single Bringe Patroon UPS . = = = = = = = TC1 become clinks in Reduce Saidorcells was the upon fiction ) was no known with his promo\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 16 [0/86600 (0%)]\tLoss: 1.542302\n",
            "Train Epoch: 16 [2000/86600 (2%)]\tLoss: 1.435330\n",
            "Train Epoch: 16 [4000/86600 (5%)]\tLoss: 1.424117\n",
            "Train Epoch: 16 [6000/86600 (7%)]\tLoss: 1.449684\n",
            "Train Epoch: 16 [8000/86600 (9%)]\tLoss: 1.425522\n",
            "Train Epoch: 16 [10000/86600 (12%)]\tLoss: 1.441091\n",
            "Train Epoch: 16 [12000/86600 (14%)]\tLoss: 1.445000\n",
            "Train Epoch: 16 [14000/86600 (16%)]\tLoss: 1.414306\n",
            "Train Epoch: 16 [16000/86600 (18%)]\tLoss: 1.419415\n",
            "Train Epoch: 16 [18000/86600 (21%)]\tLoss: 1.428258\n",
            "Train Epoch: 16 [20000/86600 (23%)]\tLoss: 1.414989\n",
            "Train Epoch: 16 [22000/86600 (25%)]\tLoss: 1.433859\n",
            "Train Epoch: 16 [24000/86600 (28%)]\tLoss: 1.398943\n",
            "Train Epoch: 16 [26000/86600 (30%)]\tLoss: 1.424991\n",
            "Train Epoch: 16 [28000/86600 (32%)]\tLoss: 1.446590\n",
            "Train Epoch: 16 [30000/86600 (35%)]\tLoss: 1.440984\n",
            "Train Epoch: 16 [32000/86600 (37%)]\tLoss: 1.413145\n",
            "Train Epoch: 16 [34000/86600 (39%)]\tLoss: 1.425091\n",
            "Train Epoch: 16 [36000/86600 (42%)]\tLoss: 1.431456\n",
            "Train Epoch: 16 [38000/86600 (44%)]\tLoss: 1.417378\n",
            "Train Epoch: 16 [40000/86600 (46%)]\tLoss: 1.427549\n",
            "Train Epoch: 16 [42000/86600 (48%)]\tLoss: 1.442815\n",
            "Train Epoch: 16 [44000/86600 (51%)]\tLoss: 1.444001\n",
            "Train Epoch: 16 [46000/86600 (53%)]\tLoss: 1.444536\n",
            "Train Epoch: 16 [48000/86600 (55%)]\tLoss: 1.440967\n",
            "Train Epoch: 16 [50000/86600 (58%)]\tLoss: 1.437501\n",
            "Train Epoch: 16 [52000/86600 (60%)]\tLoss: 1.447572\n",
            "Train Epoch: 16 [54000/86600 (62%)]\tLoss: 1.442920\n",
            "Train Epoch: 16 [56000/86600 (65%)]\tLoss: 1.461375\n",
            "Train Epoch: 16 [58000/86600 (67%)]\tLoss: 1.430381\n",
            "Train Epoch: 16 [60000/86600 (69%)]\tLoss: 1.443227\n",
            "Train Epoch: 16 [62000/86600 (72%)]\tLoss: 1.455506\n",
            "Train Epoch: 16 [64000/86600 (74%)]\tLoss: 1.433596\n",
            "Train Epoch: 16 [66000/86600 (76%)]\tLoss: 1.408389\n",
            "Train Epoch: 16 [68000/86600 (79%)]\tLoss: 1.439515\n",
            "Train Epoch: 16 [70000/86600 (81%)]\tLoss: 1.456467\n",
            "Train Epoch: 16 [72000/86600 (83%)]\tLoss: 1.449625\n",
            "Train Epoch: 16 [74000/86600 (85%)]\tLoss: 1.406899\n",
            "Train Epoch: 16 [76000/86600 (88%)]\tLoss: 1.425887\n",
            "Train Epoch: 16 [78000/86600 (90%)]\tLoss: 1.420232\n",
            "Train Epoch: 16 [80000/86600 (92%)]\tLoss: 1.420121\n",
            "Train Epoch: 16 [82000/86600 (95%)]\tLoss: 1.445771\n",
            "Train Epoch: 16 [84000/86600 (97%)]\tLoss: 1.443650\n",
            "Train Epoch: 16 [86000/86600 (99%)]\tLoss: 1.457398\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh ortf tugust 29 , 200  . The statdard sxitoon oor taldew  ,as aeleased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaei , , ahs aeleased in tovember 20 , 200  , The sirth @nd tinal sarel w thmle  ttow a Thmt ao eoe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1ale 200, 200  . The ser @-@ seaesctiorbliched ,enanah aeceased t sonpc ondionogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst so @iiume was aeleased iy taneeeaf tugust 20 , 200  , aoruseng tnetepple aoee e  \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aemd armestf t00  ,f terth l hm , ansaior oeaeteracutir of tieial aerel .nd teue ticacndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan isted tn o the sm   aoml of time ,n t000 . T = =nrly 1ive = = =acsiaaderb aas a soamtram o-@ yonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  and the t990 –eenel t oeate  , The wer torn thacksaeaorde a ahoce t  t0  merds , 1   m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oarpion =s a soure ohat tn ant ianding tn the seie oalt o an a seveog ooure on timne ttiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teers,heee areA  t Tansiaaderb ass 8  ,orts,as aae@aderb a anstmterl@eate on otnea ay tiel witere ,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5060, Accuracy: 1219645/2150400 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/016.pt\n",
            "\n",
            "generated sample\t Music and Artaities 3 @.@ 36 km ) . The event of Male : Guite appearmatics almost a became even meterial baland in teration . Whe the six hunderers wanted to Raliabovo as a \" against Aprillian 1 – ) of a time – 2 \n",
            "generated sample\t Music and Artstorian Jone . Teschelish i it reiffeews for the days of with a agent came re nofel months , in losabstaמ eximents , in the secomotes , felt the exprome over 20th emergement , the Lanes period Mole an\n",
            "generated sample\t Music and Art semality II meatures a back life , the Stock 's integationalities sound prepertanyly thankit proceeded to the \" %low moselosenearine \" . Kaducaths was severally on seriestrops a voice that the nigati\n",
            "generated sample\t Music and Artaiεea , uninguided in the 0dd8 to balt him . = Aredam frame = = The mainstroarcall film meation of palation amouncess becauses a spreacheard sking , though Nue was influentulateleryles nop to an @-@ p\n",
            "generated sample\t Music and Artube Ampisa of farmorarily . Goddenke said that the come past hibself , belies wrone task at a performance of a Un @-@ sayura , the court , Buster along the bridge Career and Areutnet atvelt to sumerra\n",
            "generated sample\t Music and Arthenice . The name , and the was sime industributes of findiates ' drivership , which originally also receives in early , by and spentimes that \" the same jour contained him 誰ode make has been futuated\n",
            "generated sample\t Music and Arte knowledge to an albrest in Sold . A suppoine also calledged . These bomes , in Wationally To Concinel Ceever 196 , B from John 平homen of James 's weell at a literout 1 @.@ N of 200 releases using th\n",
            "generated sample\t Music and Articonties young on copies by Ditmacr Bradycell , and the exprement newlink , dyladoon booth , but the game deteibres and the Creenia to explaye vice condituring the errith . Broadcashia a ceratop beter\n",
            "generated sample\t Music and Artjum Us death . The features are dTesting to mean Christian Youke ; which consisted the shippoin \" , but The builtioner . = = = He 's most surage career = = = = = Resture = = Allowing 201 kmilled isobo\n",
            "generated sample\t Music and Artan ributos ( a reprodumer % \" and the 1871 cells behind Theory Taakily . B2 tellion 10 minds meets 2 cm 4 @.@ 2 fu 2 ( 1 @.@ 9 some peofeerial refervents of a worlinged from wayers , identifed beginni\n",
            "generated beam\t\t Music and Arthide , ship this plot even paunting the meleural . Are clearing a poor mained rate by the main signalling in the literar season , as well promide no main poet , while marriage in Iiff that younda is t\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 17 [0/86600 (0%)]\tLoss: 1.541268\n",
            "Train Epoch: 17 [2000/86600 (2%)]\tLoss: 1.432512\n",
            "Train Epoch: 17 [4000/86600 (5%)]\tLoss: 1.417680\n",
            "Train Epoch: 17 [6000/86600 (7%)]\tLoss: 1.446855\n",
            "Train Epoch: 17 [8000/86600 (9%)]\tLoss: 1.427538\n",
            "Train Epoch: 17 [10000/86600 (12%)]\tLoss: 1.435062\n",
            "Train Epoch: 17 [12000/86600 (14%)]\tLoss: 1.441561\n",
            "Train Epoch: 17 [14000/86600 (16%)]\tLoss: 1.414266\n",
            "Train Epoch: 17 [16000/86600 (18%)]\tLoss: 1.417591\n",
            "Train Epoch: 17 [18000/86600 (21%)]\tLoss: 1.423870\n",
            "Train Epoch: 17 [20000/86600 (23%)]\tLoss: 1.409637\n",
            "Train Epoch: 17 [22000/86600 (25%)]\tLoss: 1.427896\n",
            "Train Epoch: 17 [24000/86600 (28%)]\tLoss: 1.399417\n",
            "Train Epoch: 17 [26000/86600 (30%)]\tLoss: 1.421985\n",
            "Train Epoch: 17 [28000/86600 (32%)]\tLoss: 1.447372\n",
            "Train Epoch: 17 [30000/86600 (35%)]\tLoss: 1.436898\n",
            "Train Epoch: 17 [32000/86600 (37%)]\tLoss: 1.412631\n",
            "Train Epoch: 17 [34000/86600 (39%)]\tLoss: 1.424663\n",
            "Train Epoch: 17 [36000/86600 (42%)]\tLoss: 1.431495\n",
            "Train Epoch: 17 [38000/86600 (44%)]\tLoss: 1.416789\n",
            "Train Epoch: 17 [40000/86600 (46%)]\tLoss: 1.425278\n",
            "Train Epoch: 17 [42000/86600 (48%)]\tLoss: 1.436493\n",
            "Train Epoch: 17 [44000/86600 (51%)]\tLoss: 1.448239\n",
            "Train Epoch: 17 [46000/86600 (53%)]\tLoss: 1.444932\n",
            "Train Epoch: 17 [48000/86600 (55%)]\tLoss: 1.435396\n",
            "Train Epoch: 17 [50000/86600 (58%)]\tLoss: 1.434591\n",
            "Train Epoch: 17 [52000/86600 (60%)]\tLoss: 1.443975\n",
            "Train Epoch: 17 [54000/86600 (62%)]\tLoss: 1.438495\n",
            "Train Epoch: 17 [56000/86600 (65%)]\tLoss: 1.463764\n",
            "Train Epoch: 17 [58000/86600 (67%)]\tLoss: 1.430956\n",
            "Train Epoch: 17 [60000/86600 (69%)]\tLoss: 1.441845\n",
            "Train Epoch: 17 [62000/86600 (72%)]\tLoss: 1.454695\n",
            "Train Epoch: 17 [64000/86600 (74%)]\tLoss: 1.431017\n",
            "Train Epoch: 17 [66000/86600 (76%)]\tLoss: 1.407004\n",
            "Train Epoch: 17 [68000/86600 (79%)]\tLoss: 1.436249\n",
            "Train Epoch: 17 [70000/86600 (81%)]\tLoss: 1.454453\n",
            "Train Epoch: 17 [72000/86600 (83%)]\tLoss: 1.451704\n",
            "Train Epoch: 17 [74000/86600 (85%)]\tLoss: 1.404286\n",
            "Train Epoch: 17 [76000/86600 (88%)]\tLoss: 1.419640\n",
            "Train Epoch: 17 [78000/86600 (90%)]\tLoss: 1.416630\n",
            "Train Epoch: 17 [80000/86600 (92%)]\tLoss: 1.417759\n",
            "Train Epoch: 17 [82000/86600 (95%)]\tLoss: 1.438410\n",
            "Train Epoch: 17 [84000/86600 (97%)]\tLoss: 1.443122\n",
            "Train Epoch: 17 [86000/86600 (99%)]\tLoss: 1.454676\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh ortf tugust 2  , 200  . The statdard sxitoon wor taldew  ,as aepeased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaei , , ahs aeleased in tovember 20 , 2000 , The sirth @nd tinal sarel w thmle  ttaw o Ttmt ao aoe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1ale 20 , 200  . The ser @-@ seaesctiorbliched ,enanah aeceased t sonpc ondionogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst sa @iiune was aeleased iy taneeeCf tugust 20 , 200  , aoruseng tnetepple aoeene  \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aemd aamestf t00  ,n terth l hm , ansanor oeaeteraautir of tieial aerels.nd tewe ticacndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ested tn o the sm   aall of teme ,n 1000 . T = =nrly =eve = = =acsiaaderb aas a soamtram o-@ sonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the t990 –eenel t ,eate  , The wer torn thacksaeaorde a ahoce in t   merds , 1   m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oarpion =s a siune ohat tn ant ianding tn the seie oalt o an a seveng oaure on timne atow  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teers,heoe ,reA  a Tansiaaderb ass 9  ,orts,as aae@aderb , anstntarl@eate nn otnea ay tiel witere ,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4991, Accuracy: 1224862/2150400 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/017.pt\n",
            "\n",
            "generated sample\t Music and Arto Sa Pato 's life . Green evisone aggent to mething them executive 167 metre , a drab of intertained later ; Prinjin 's to an 121 @g@ yruine used . In 1906 , Feather 2100 feeds at importance in each o\n",
            "generated sample\t Music and Artan . The knew he he woyenned to his halicallo decent based had the people estable , Sme played by the other season wind paless for elotion . The reason of tracks and roles may unevate the poor down ov\n",
            "generated sample\t Music and Artageoun and the War Kward @.@ Raita , Asse and Markanhes favoues of Heire . The took Unimornia , those figstic hards had schoonely for his oast bean @-@ mains 's funderitor became in twick . It wrote m\n",
            "generated sample\t Music and Artrica and alebeats . = Yoreated for its defensively is filmed by they gaven manking at huete — enemohian spaces or encound 's caulling of a concrective of the three , includes out eventةe . It has a ri\n",
            "generated sample\t Music and Arton @-@ Nie ) , is moved for Horn Everso , and had conginent presade to Eanuversteiquee . Ded a Congre assert he poethy he atsound misked be omateches , cadmiualel the trade is moment of Sue to share e\n",
            "generated sample\t Music and Art Wicka . \" The topically , \" sixtherel a marries he broaghly \" over the intertagonal season , he was performed by the plantine of episode with Chadlains passes frem a moter of making the finally dable\n",
            "generated sample\t Music and Artragine Jasel was the basald , and recording the true fell 放 pastically comeland in charactic sitces a strange of the best figme ; they mang it the substry of gasself . . = range naturealy original = =\n",
            "generated sample\t Music and Arthoughestskeaconfice relational theorgogicality navalist actever . One of the later below been , on as Maxouch stated Middo ks the game Cayer and Lickynes in Othement , which set mellabilities of the C\n",
            "generated sample\t Music and Arthur conölation eventually , would represent , with a Teame stage . But wrote candle infreventually sukpartisthallative , it had si anker used created that while for these sulfans of the -odar ' was ev\n",
            "generated sample\t Music and Arty , in 1964 been crew . Pussibline producing the coverzing one effects in white a second offscore , and stated the hurround efcect replaced one of the places the dannezest Ade was or on the 2009d day \n",
            "generated beam\t\t Music and Artham to be freing a would differential dio arenessitatics , which had made the children was above the Ravannaµ avale of the pato speciation . = = Arts or theichleckment = = = = 90= years of a gresulati\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 18 [0/86600 (0%)]\tLoss: 1.537102\n",
            "Train Epoch: 18 [2000/86600 (2%)]\tLoss: 1.429119\n",
            "Train Epoch: 18 [4000/86600 (5%)]\tLoss: 1.416215\n",
            "Train Epoch: 18 [6000/86600 (7%)]\tLoss: 1.445058\n",
            "Train Epoch: 18 [8000/86600 (9%)]\tLoss: 1.422852\n",
            "Train Epoch: 18 [10000/86600 (12%)]\tLoss: 1.431966\n",
            "Train Epoch: 18 [12000/86600 (14%)]\tLoss: 1.437067\n",
            "Train Epoch: 18 [14000/86600 (16%)]\tLoss: 1.414645\n",
            "Train Epoch: 18 [16000/86600 (18%)]\tLoss: 1.415157\n",
            "Train Epoch: 18 [18000/86600 (21%)]\tLoss: 1.425231\n",
            "Train Epoch: 18 [20000/86600 (23%)]\tLoss: 1.407661\n",
            "Train Epoch: 18 [22000/86600 (25%)]\tLoss: 1.423709\n",
            "Train Epoch: 18 [24000/86600 (28%)]\tLoss: 1.394716\n",
            "Train Epoch: 18 [26000/86600 (30%)]\tLoss: 1.417237\n",
            "Train Epoch: 18 [28000/86600 (32%)]\tLoss: 1.451253\n",
            "Train Epoch: 18 [30000/86600 (35%)]\tLoss: 1.435004\n",
            "Train Epoch: 18 [32000/86600 (37%)]\tLoss: 1.408045\n",
            "Train Epoch: 18 [34000/86600 (39%)]\tLoss: 1.422328\n",
            "Train Epoch: 18 [36000/86600 (42%)]\tLoss: 1.425617\n",
            "Train Epoch: 18 [38000/86600 (44%)]\tLoss: 1.412200\n",
            "Train Epoch: 18 [40000/86600 (46%)]\tLoss: 1.422545\n",
            "Train Epoch: 18 [42000/86600 (48%)]\tLoss: 1.434637\n",
            "Train Epoch: 18 [44000/86600 (51%)]\tLoss: 1.444437\n",
            "Train Epoch: 18 [46000/86600 (53%)]\tLoss: 1.437928\n",
            "Train Epoch: 18 [48000/86600 (55%)]\tLoss: 1.435266\n",
            "Train Epoch: 18 [50000/86600 (58%)]\tLoss: 1.432259\n",
            "Train Epoch: 18 [52000/86600 (60%)]\tLoss: 1.441151\n",
            "Train Epoch: 18 [54000/86600 (62%)]\tLoss: 1.440014\n",
            "Train Epoch: 18 [56000/86600 (65%)]\tLoss: 1.458085\n",
            "Train Epoch: 18 [58000/86600 (67%)]\tLoss: 1.428940\n",
            "Train Epoch: 18 [60000/86600 (69%)]\tLoss: 1.440112\n",
            "Train Epoch: 18 [62000/86600 (72%)]\tLoss: 1.455721\n",
            "Train Epoch: 18 [64000/86600 (74%)]\tLoss: 1.428776\n",
            "Train Epoch: 18 [66000/86600 (76%)]\tLoss: 1.405550\n",
            "Train Epoch: 18 [68000/86600 (79%)]\tLoss: 1.433881\n",
            "Train Epoch: 18 [70000/86600 (81%)]\tLoss: 1.452500\n",
            "Train Epoch: 18 [72000/86600 (83%)]\tLoss: 1.448270\n",
            "Train Epoch: 18 [74000/86600 (85%)]\tLoss: 1.398606\n",
            "Train Epoch: 18 [76000/86600 (88%)]\tLoss: 1.421701\n",
            "Train Epoch: 18 [78000/86600 (90%)]\tLoss: 1.414229\n",
            "Train Epoch: 18 [80000/86600 (92%)]\tLoss: 1.414745\n",
            "Train Epoch: 18 [82000/86600 (95%)]\tLoss: 1.434034\n",
            "Train Epoch: 18 [84000/86600 (97%)]\tLoss: 1.442760\n",
            "Train Epoch: 18 [86000/86600 (99%)]\tLoss: 1.453193\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh lrtf tugust 2  , 200  . The statdard sxitoon oor teldew  ,as aepeased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaei , , ahs aeleased in tovember 20 , 2000 , The sirth @nd tiral serel w thmle  ttiw o Ttmt aovaoe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1ale 200, 2000 . The ser @-@ seaesctiorbliched ,enanah ,eceased t sonpc ondienogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst so @iiune was aeleased iy taneeeCf tugust 20 , 2000 , aoruseng tnetepple aoeene  \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aemd armestf t00  ,f terth l hm , ansanor oeaeceraautir of tieial aerel .nd teue tic cndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ested tn o the su   aoll of teme ,n 1000 . T = =arly 1eve = = =acsiaaderb aas a soamtram o-@ yonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the s990 –eenel t ,eate  , The wer torr thacksaeaerde a ahoce in t   merds , 1   m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oarpion =s a soude ohat tn ant iatding tn the seie oalt o an a seveng oaure on temne atiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teers,heoe areA  a Tacsiaaderb a s 9  ,orts,as aae@aderb a anstuterl@eate nn otnea ay tiel w tere ,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5015, Accuracy: 1222920/2150400 (57%)\n",
            "\n",
            "generated sample\t Music and Arthumnet place athound changetts of the albe @-@ loud @M@ gless . RFf had are reilled by Hoepaudja and particularly account Glue 's mosthilariance officers , conserving toher faites , and in depensional\n",
            "generated sample\t Music and Artable demovate the second drumption within perioiseacclased her then preadarise the magame to nave they of ! mostures , also 's busefing hyme differing the series to . Hartle nellease , the lateer is w\n",
            "generated sample\t Music and Art Haille 's . Jight . The each of Parashi houses it time occused this shreaded and the event was improved the depocarculators greatery that the playeპ attend of children . The state of her strappes , a\n",
            "generated sample\t Music and Arta offices and pushed was from Henry Gh owens , mphaサthelms at the school honours . = Despite Leige 3uma Shis fannizueating division to indicatively and a motign that had more dadabase specordabs from \n",
            "generated sample\t Music and Artando and languages @-@ eventualliest site . It was part both the storical over oberation areaver in the actression same humbards dashumy . Chine came immediately Beighesteirs , its death of career pro\n",
            "generated sample\t Music and Artanmey 's ń music ; ureduces in 1696 to survive one of it ' eading freement as the century marchne . Such also from the century – the divicing later sliped and late nicespearames , sagialize ( 2 miles \n",
            "generated sample\t Music and Arthough @-@ elements = Ith createverses ) cledakates and consistion of the career and forestrated take a fueld to be echnoyed by the relamery . = = Codesn openings = = N裁N Jamar Trenneuć received a 350 \n",
            "generated sample\t Music and Arts W = = Ruce exison D라I was schoods in the substaneouslises to fur contained . Machieves had enemented in damagchzic . Rome hears are resident the world gained beholices eason supposervatively to worr\n",
            "generated sample\t Music and Artuskeeplesking . = = = Prother cast = = = = Chite and some lines simole ) if it said ) Andweater did not we searchind for a 4 first mmy come . The affective kanadian 's player was displaced in than Aug\n",
            "generated sample\t Music and Artshian Hero walling the American Pace . Landu of the plan entertained Barin Lietnad Potestance infantry behind 5 – 0 ; 6 encouration from 1 6S ) , esperiences ( 3 acceuding or a culture democration and\n",
            "generated beam\t\t Music and Art the herbs of herries . Drilliance 200 @ъ@ 622 the comment of a ween in ) each name Desemony \" , Six Rannada , ; = Dheater seemed field ranking since viotes ) viewed benefines offinational concests , \n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 19 [0/86600 (0%)]\tLoss: 1.538249\n",
            "Train Epoch: 19 [2000/86600 (2%)]\tLoss: 1.428479\n",
            "Train Epoch: 19 [4000/86600 (5%)]\tLoss: 1.414667\n",
            "Train Epoch: 19 [6000/86600 (7%)]\tLoss: 1.442583\n",
            "Train Epoch: 19 [8000/86600 (9%)]\tLoss: 1.421506\n",
            "Train Epoch: 19 [10000/86600 (12%)]\tLoss: 1.431539\n",
            "Train Epoch: 19 [12000/86600 (14%)]\tLoss: 1.436863\n",
            "Train Epoch: 19 [14000/86600 (16%)]\tLoss: 1.412978\n",
            "Train Epoch: 19 [16000/86600 (18%)]\tLoss: 1.414275\n",
            "Train Epoch: 19 [18000/86600 (21%)]\tLoss: 1.417311\n",
            "Train Epoch: 19 [20000/86600 (23%)]\tLoss: 1.403757\n",
            "Train Epoch: 19 [22000/86600 (25%)]\tLoss: 1.423263\n",
            "Train Epoch: 19 [24000/86600 (28%)]\tLoss: 1.392428\n",
            "Train Epoch: 19 [26000/86600 (30%)]\tLoss: 1.414251\n",
            "Train Epoch: 19 [28000/86600 (32%)]\tLoss: 1.443467\n",
            "Train Epoch: 19 [30000/86600 (35%)]\tLoss: 1.433295\n",
            "Train Epoch: 19 [32000/86600 (37%)]\tLoss: 1.408080\n",
            "Train Epoch: 19 [34000/86600 (39%)]\tLoss: 1.420735\n",
            "Train Epoch: 19 [36000/86600 (42%)]\tLoss: 1.424086\n",
            "Train Epoch: 19 [38000/86600 (44%)]\tLoss: 1.410466\n",
            "Train Epoch: 19 [40000/86600 (46%)]\tLoss: 1.418475\n",
            "Train Epoch: 19 [42000/86600 (48%)]\tLoss: 1.430527\n",
            "Train Epoch: 19 [44000/86600 (51%)]\tLoss: 1.439871\n",
            "Train Epoch: 19 [46000/86600 (53%)]\tLoss: 1.435854\n",
            "Train Epoch: 19 [48000/86600 (55%)]\tLoss: 1.431539\n",
            "Train Epoch: 19 [50000/86600 (58%)]\tLoss: 1.430326\n",
            "Train Epoch: 19 [52000/86600 (60%)]\tLoss: 1.437321\n",
            "Train Epoch: 19 [54000/86600 (62%)]\tLoss: 1.437067\n",
            "Train Epoch: 19 [56000/86600 (65%)]\tLoss: 1.456983\n",
            "Train Epoch: 19 [58000/86600 (67%)]\tLoss: 1.428313\n",
            "Train Epoch: 19 [60000/86600 (69%)]\tLoss: 1.437474\n",
            "Train Epoch: 19 [62000/86600 (72%)]\tLoss: 1.451994\n",
            "Train Epoch: 19 [64000/86600 (74%)]\tLoss: 1.429928\n",
            "Train Epoch: 19 [66000/86600 (76%)]\tLoss: 1.403937\n",
            "Train Epoch: 19 [68000/86600 (79%)]\tLoss: 1.428945\n",
            "Train Epoch: 19 [70000/86600 (81%)]\tLoss: 1.449681\n",
            "Train Epoch: 19 [72000/86600 (83%)]\tLoss: 1.442086\n",
            "Train Epoch: 19 [74000/86600 (85%)]\tLoss: 1.395314\n",
            "Train Epoch: 19 [76000/86600 (88%)]\tLoss: 1.416711\n",
            "Train Epoch: 19 [78000/86600 (90%)]\tLoss: 1.410753\n",
            "Train Epoch: 19 [80000/86600 (92%)]\tLoss: 1.413554\n",
            "Train Epoch: 19 [82000/86600 (95%)]\tLoss: 1.430579\n",
            "Train Epoch: 19 [84000/86600 (97%)]\tLoss: 1.441099\n",
            "Train Epoch: 19 [86000/86600 (99%)]\tLoss: 1.455695\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh lrtf tugust 29 , 200  . The statdard sxitoon oor taldiw  ,as aepeased in tuly 20 , 200  , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaei , , ahs aepeased in tovember 20 , 2000 , The sirth @nd tinal sarel w thmle  ttiw o Tom  aovaoe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1une 200, 2000 . The ser @-@ seaesctioabliched ,enan h ,eceased t sonpc ondienogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst sou@iiune was aepeased iy taneeeCf tugust 20 , 2000 , aoruseng tnetepple aoeene  \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\ty aemd aamestf t00  .f terth l h  , ansanor oeauceriautir of tieial aarel .nd teuestic andma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ected bn o the su@  aoll of teme .n 1000 . T = =arly 1ive = = =acsiaaderb aas a soamtram o-@ yonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd the s990 –eenel t ,eate  , The wer torr thacksaeaorde a ahoce in t   merds , 1 0 m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oarpion =n a soude ooat tn ant ianding tn the seie oalt o an a seveng oiure on timne atiw  an\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teirs,heoe aceA  a Tacsiaaderb ass 90 ,orts,as aae@aderb , anstutorl@eate on otnea ay tiel w tere ,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.4977, Accuracy: 1227462/2150400 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/019.pt\n",
            "\n",
            "generated sample\t Music and Artsomes hibson emisedầ . I janguary are Urenameloc receives a mode for a me Aprilia enfortion , and a such appear builving left stages . Ohie reaks , deeribusines were a confert distance . On overage , \n",
            "generated sample\t Music and Artson Preguc , and among the athairs ( elementh of the route broaches ) , 座 1 @.@ 240 unions = Momentoda and we would in a reagent with history of which chanded the chapter of starking carroom 's elemen\n",
            "generated sample\t Music and Artssatessal . Coddin the many is \" diawnigate evidences of opdically its , analoly New would 's been acourationally occupied in futurn to the minedo admotteque in a sexment of did nominute . Candle , ma\n",
            "generated sample\t Music and Artson were blaced in merfant . Jwospicislough aspertable inforcations of SCA Preatchie , he read increased to hill referred to later about the final ihea gun @-@ 6 @-@ y miles , from A 7 @.@ 2 – @ @ 5 –\n",
            "generated sample\t Music and Artama in a viceos viology , she ganned there and no grass . and a tenther was seidored Galtylen 's separate gods wichard in a foide and mile of matches him in a 2003 together 's team Zay , busine was pr\n",
            "generated sample\t Music and Artricone , Ono of L ( 1716t – 2 @,@ 501 pickees ) which are reconded . Bart seen ganuare 4 , 1 @.@ 9 mbilitor in poor / ; roue orseft congestion @-@ oars ( 1 @.@ 5 m ) . The head as hougle , alcoid on N\n",
            "generated sample\t Music and Artraice . Hideout hoordoses , they contained its intercage under Rovement . The area were likeliced association , with the wewnsisserphillian antion – 7 @宗@ 0 wind = = = = Path other ) = The 1860 – 1 10\n",
            "generated sample\t Music and Artames at defeatement under chelming of revization . The permitive , haungaship derastics , in intersection in a spjoductic wall failhed to helve her a ; horizen and seven in geoghes and construction me\n",
            "generated sample\t Music and Artany descripes and 12 encattlemend . = = The field \" = = = = = = = = = = = Hawayuinehip = = = Composerback = = = = = Noute = = = The challenges lives ' 2 6 @.@ 9 @.@ 0 2 meetion , which auded that the \n",
            "generated sample\t Music and Artailed game motars salm in a safese in Jumbe . The teams attended their correntional stodest other @-@ younge in the life for shows commemorally people viewers for the citain in Maxthe of Westin . In 1\n",
            "generated beam\t\t Music and Artan deeplication , and this four smaller bolices nevers at a old in @-@ UKS for Sico de the dafletiso were aided . This convieron majority her films troke the storm of 530 was , a drubtinck of the game\n",
            "\n",
            "enumerate train   433\n",
            "Train Epoch: 20 [0/86600 (0%)]\tLoss: 1.535115\n",
            "Train Epoch: 20 [2000/86600 (2%)]\tLoss: 1.426427\n",
            "Train Epoch: 20 [4000/86600 (5%)]\tLoss: 1.408514\n",
            "Train Epoch: 20 [6000/86600 (7%)]\tLoss: 1.441073\n",
            "Train Epoch: 20 [8000/86600 (9%)]\tLoss: 1.418952\n",
            "Train Epoch: 20 [10000/86600 (12%)]\tLoss: 1.425381\n",
            "Train Epoch: 20 [12000/86600 (14%)]\tLoss: 1.438449\n",
            "Train Epoch: 20 [14000/86600 (16%)]\tLoss: 1.411082\n",
            "Train Epoch: 20 [16000/86600 (18%)]\tLoss: 1.412857\n",
            "Train Epoch: 20 [18000/86600 (21%)]\tLoss: 1.416098\n",
            "Train Epoch: 20 [20000/86600 (23%)]\tLoss: 1.402171\n",
            "Train Epoch: 20 [22000/86600 (25%)]\tLoss: 1.422619\n",
            "Train Epoch: 20 [24000/86600 (28%)]\tLoss: 1.393475\n",
            "Train Epoch: 20 [26000/86600 (30%)]\tLoss: 1.410688\n",
            "Train Epoch: 20 [28000/86600 (32%)]\tLoss: 1.442489\n",
            "Train Epoch: 20 [30000/86600 (35%)]\tLoss: 1.430988\n",
            "Train Epoch: 20 [32000/86600 (37%)]\tLoss: 1.404510\n",
            "Train Epoch: 20 [34000/86600 (39%)]\tLoss: 1.417009\n",
            "Train Epoch: 20 [36000/86600 (42%)]\tLoss: 1.422194\n",
            "Train Epoch: 20 [38000/86600 (44%)]\tLoss: 1.404702\n",
            "Train Epoch: 20 [40000/86600 (46%)]\tLoss: 1.415654\n",
            "Train Epoch: 20 [42000/86600 (48%)]\tLoss: 1.427732\n",
            "Train Epoch: 20 [44000/86600 (51%)]\tLoss: 1.437058\n",
            "Train Epoch: 20 [46000/86600 (53%)]\tLoss: 1.434139\n",
            "Train Epoch: 20 [48000/86600 (55%)]\tLoss: 1.429208\n",
            "Train Epoch: 20 [50000/86600 (58%)]\tLoss: 1.426263\n",
            "Train Epoch: 20 [52000/86600 (60%)]\tLoss: 1.438740\n",
            "Train Epoch: 20 [54000/86600 (62%)]\tLoss: 1.432513\n",
            "Train Epoch: 20 [56000/86600 (65%)]\tLoss: 1.457971\n",
            "Train Epoch: 20 [58000/86600 (67%)]\tLoss: 1.430526\n",
            "Train Epoch: 20 [60000/86600 (69%)]\tLoss: 1.435815\n",
            "Train Epoch: 20 [62000/86600 (72%)]\tLoss: 1.449075\n",
            "Train Epoch: 20 [64000/86600 (74%)]\tLoss: 1.422937\n",
            "Train Epoch: 20 [66000/86600 (76%)]\tLoss: 1.401023\n",
            "Train Epoch: 20 [68000/86600 (79%)]\tLoss: 1.430611\n",
            "Train Epoch: 20 [70000/86600 (81%)]\tLoss: 1.448720\n",
            "Train Epoch: 20 [72000/86600 (83%)]\tLoss: 1.439635\n",
            "Train Epoch: 20 [74000/86600 (85%)]\tLoss: 1.396871\n",
            "Train Epoch: 20 [76000/86600 (88%)]\tLoss: 1.417071\n",
            "Train Epoch: 20 [78000/86600 (90%)]\tLoss: 1.407817\n",
            "Train Epoch: 20 [80000/86600 (92%)]\tLoss: 1.416608\n",
            "Train Epoch: 20 [82000/86600 (95%)]\tLoss: 1.429187\n",
            "Train Epoch: 20 [84000/86600 (97%)]\tLoss: 1.434555\n",
            "Train Epoch: 20 [86000/86600 (99%)]\tLoss: 1.448595\n",
            "enumerate test    84\n",
            "Input\ttotype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , conta\n",
            "GT\totype on August 16 , 2007 . The standard edition for Windows was released on July 25 , 2008 , contai\n",
            "pred\thrh lrtf tugust 29 , 200  . The statdard sxitoon oor teldew  ,as aeleased in tuly 20 , 2000 , aontii\n",
            "\n",
            "\n",
            "Input\thigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yu\n",
            "GT\tigure ) , was released on November 22 , 2003 . The fifth and final novel , titled Snow : Sora no Yur\n",
            "pred\tevaei , , ahs aeleased in tovember 20 , 2000 , The sirth @nd tiral serel w thmle  ttiw , Temt aov@oe\n",
            "\n",
            "\n",
            "Input\tApril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , tit\n",
            "GT\tpril 25 and June 25 , 2003 . The now @-@ defunct publisher Raporto released a comic anthology , titl\n",
            "pred\turil 20 ,nd 1ule 200, 2000 . The ser @-@ ceaesctiorbliched ,ecenth ,eceased t sonpc ondienogi o ahml\n",
            "\n",
            "\n",
            "Input\tblished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuk\n",
            "GT\tlished , the first CD volume was released by Movic on August 22 , 2003 , focusing on Sumino Yukizuki\n",
            "pred\tlished i ahe sirst so @ilume was aeleased iy taneeeCf tugust 20 , 2000 , aoruseng tnetepmle ,oeene  \n",
            "\n",
            "\n",
            "Input\tly sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime produc\n",
            "GT\ty sold game of 2003 on Getchu.com , a major redistributor of visual novel and domestic anime product\n",
            "pred\te temd armestf t00  .f terth l h  , ansanor oeaecericutir of tieial aerel .nd tecestic sndma .roduce\n",
            "\n",
            "\n",
            "Input\t inducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ col\n",
            "GT\tinducted into the AQHA Hall of Fame in 2009 . = = Early life = = Miss Meyers was a chestnut @-@ colo\n",
            "pred\tan ested tn o the sue  aoll of teme ,n 1000 . T = =nrly 1eve = = =acsiaaderb aas a soamt al o-@ yonl\n",
            "\n",
            "\n",
            "Input\takes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once\n",
            "GT\tkes and the 1955 Traveler Stakes . She set four track records , twice at 350 yards ( 320 m ) , once \n",
            "pred\tte  ,nd 1he 1990 –eenel t ,eate  , The wer torr thaces,ecerde a ahoce in t   merds , 1   m ) , anee \n",
            "\n",
            "\n",
            "Input\tupreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows a\n",
            "GT\tpreme Champion is a horse that is outstanding on the racetrack , as a riding horse at horse shows an\n",
            "pred\treeme =oarpion =s a siude ooet tn ant iatding tn the sece oalt o an a segeng oiure on tiune ,eiwe ,n\n",
            "\n",
            "\n",
            "Input\tnner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of\n",
            "GT\tner Three Oh 's . Miss Meyers ' 1958 foal was Mr Meyers , a sorrel stallion sired by fellow Hall of \n",
            "pred\teers,heoe ,ceA  1 Tacsiaaderb ,ss940 ,orts,as aae@aderb , anseuterl@eate nn otnen iy tiel w tere ,f \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.5056, Accuracy: 1219741/2150400 (57%)\n",
            "\n",
            "generated sample\t Music and Artanneig contract , and the same drawn to be one of worldwide east at the U.CKę– affertaتing the commercial sitalacory for the structure . On Žcce , is beatned include in Sarce , 3188 , to others at Q20\n",
            "generated sample\t Music and Artham Means was one @-@ range student . -thell , muscarian in Way ( simver – 3 ; maga礮σá . = = = State = = leadsecusing the Ulhevia Edṅcation Gales = A Hourt @-@ 9 S.BE錡 , The RhAひies , which each other\n",
            "generated sample\t Music and Artham Aside a deliverbee worsed , as his twick erases received one of the first pales . The alluhal books liveled in a badage by secret after he ks the sospetter , GiammOr6 Zlesseoper which included the\n",
            "generated sample\t Music and Arthana redecused , in an eagle lawe premiered mamoralist ambign . To かnourts Sicuet was Uconferford for exements of the sime for amendent bacthere proposed in aeroulners , which is allowed by the decemo\n",
            "generated sample\t Music and Artundett Nur @-@ manicular rolecose , Nic宋 and 200 . Athoso maches the routine unewayed at the Michael leiges . It was about Landon to the dam , dringener on the simed the between 1990 theme Eachelett p\n",
            "generated sample\t Music and Arthur in 1910 , the 1985 touts as were their late 200 million in @-@ 1 : 201s undeed the bale žendiot place N.. Deatho threates , score around the tweleters of the independence of a so throut her noutue\n",
            "generated sample\t Music and Artsomeჵles free which were hote . I Areghti haping used , the first ltage , much siァes and officers would that the heatire is also intereuted to paye bomes in seferement is the metres of 10 @,@ 12 єC4 ,\n",
            "generated sample\t Music and Artrodopsical . It was a compedite of Breegican and quarterbloines and at the roadcastic member of the D.I 's producers believe . The Indian Figer called it one of inelushing a lear team to be rate as an\n",
            "generated sample\t Music and Artande from the first ships . = = = Separates = = = = some moxes of Chitacolood Illings = = = Pedibel of remained American in her eiget1 SmCRe , he was the cemetricz odder six agreements at 201 @.@ 20 m\n",
            "generated sample\t Music and Artasula researchers , using the varise of 47 years , edge over of planet 200 , undertained dopun in sector bive silen in Amerio and Ausrael , nowers . After attending from the 1410s defender of the Roya\n",
            "generated beam\t\t Music and Artsside ± which he guised only much deis she husward in the serone scouts he tobestsellered to eRame Cwite . = = = = Marlet voice = = = 6 discograined = = Simple of his cunmanment and become other contr\n",
            "\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_corpus/020.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LorPLdYL3UoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 6: Experiments\n"
      ]
    },
    {
      "metadata": {
        "id": "D7-85Dz83QIG",
        "colab_type": "code",
        "outputId": "880b521a-3182-45c4-fd5a-69397351ec43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        "seed_words = 'The champion'\n",
        "sequence_length = 200\n",
        "\n",
        "generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'max')\n",
        "print('generated with max\\t', generated_sentence)\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'sample')\n",
        "    print('generated with sample\\t', generated_sentence)\n",
        "    \n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with max\t The champion in the season of the second season . The second season was a second season , and the state of the second season , the state of the second season , the state of the second season , the state of the se\n",
            "generated with sample\t The championed the Chinese designation with a marchine to House S. ( 200 – 1 ) , on 18 , 2003 , which set he stood a most of the contract player to the Hide God . The state of the season was a new season , in the\n",
            "generated with sample\t The champions , a head of 4 @.@ 0 m ( 1 @.@ 9 % ) . The best serves in the Australia was the ball of the their twat the United States and experience in the 2008 season . The Australian passes had been a second @-\n",
            "generated with sample\t The championed back and a concert of up in the series of the early 1920 children and a transle was a match in which the first series was contemporarely designed to returned the second season . The State of the ar\n",
            "generated with sample\t The champions in several parts carried the came of the second protestion of the season . The construction of the highest partially are continued to a second progress of the second score . The four @-@ catches of \n",
            "generated with sample\t The champion from the title of the second many of the game of the series . All the second state of the United States , a contract of the first back the base of the Area and French area was for the German of Organ\n",
            "generated with sample\t The champions a natural reaction of the man of the season of the series of the second season . In the Father Mardy , the United States Earth , Parliamene was a version of the base of the First Season , and the he\n",
            "generated with sample\t The champion it was described the player of the conferences . The the point and the first describes of the site of the top the film believed the the came 's interchange of the first season . The song was \" Sina –\n",
            "generated with sample\t The champion , the the trin was a second the massion of 9 @.@ 1 km ( 2 @.@ 4 m ) . It is a similar offension of chese area were the release of the film and extrance some of the United States . The child had compl\n",
            "generated with sample\t The champions for the Chelling , which had been a disance of the Harannell of the Rice Channel . It had seen a state of the United States , and the complete case in a second for out a state of 13 @.@ 4 miles ; th\n",
            "generated with sample\t The champion of the song \" beat the best of the part of the second due to the second poem \" , his offerers and a local children of the south of the second more laws , while the two many contrasts to have a second\n",
            "generated with beam\t The champion to the album of the series . The the episode is underected in the second day of the aachest of the contract of the production of the season . The album also had been made him to the for the state of \n",
            "generated with beam\t The champion of the theme of the many of the 1000 and the second behind the state of the new green ship with a new off tracks . This contract was a matched at Bandi , it was three the character system , and had b\n",
            "generated with beam\t The champion of the game 's high @-@ house show he was the highest contract of the Maria Count . The first had been a land base voluntiard for a street of the character . The stream of the first stars after the e\n",
            "generated with beam\t The champion and the conflict the new season , a persal words , in the city of the national career of the community , which had the always became the program to the some of the feet commercials . The commercial m\n",
            "generated with beam\t The champion is a transit of same second and a second great zanon . The the team had a strong development of the first season . The early 2000 and the state of the 2000 season were the Chicago Cardinal Bond Strai\n",
            "generated with beam\t The champion in a season in the term @-@ state the game 's best exist . One of the stage stopped the check of the first marriage the character of the singer 15 @-@ yard and experience . A produced a producer and \n",
            "generated with beam\t The champion failed to the called Final Novel @-@ Maria Most , Batherin and London , which he considered the song of the person . = = Presenting = = = = Creation = = = = = Cup = = = = = = The War of Jane = = = = \n",
            "generated with beam\t The champion its state in a transit of the species of the center . = = = Redecent = = = = = = For The New World Career = = = A large police sourcest manager of the southern part of the US – 196 @.@ 200 mm ) he wa\n",
            "generated with beam\t The champion in first the conflict of the season , the player disease of the first defeat and the children revealed the present of the tries . The contract was been a side of the southern end of the Chicago For s\n",
            "generated with beam\t The champion was responded in 1804 . It was for the final episode of the ball of production in the first destroyers of the commands . The first came in the game , but the New Team , which the total of the categor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9QdI4J_1frhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WORD RNN"
      ]
    },
    {
      "metadata": {
        "id": "gCpeOfJ82NwW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Preprocessing the data"
      ]
    },
    {
      "metadata": {
        "id": "s4BAyLOTYC6R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "6e119eb3-dc95-4d0b-b937-d8aafcee0a02"
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "metadata": {
        "id": "DIt0bs6kYG1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "c94cea53-ecfc-44a4-c3d8-1cf65c4e9056"
      },
      "cell_type": "code",
      "source": [
        "## Without PUNCTUATION\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import re\n",
        "import math\n",
        "def prepare_data(data_path):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "    \n",
        "    # TODO Add more preprocessing\n",
        "    data = re.sub('\\s+', ' ', data).strip()\n",
        "    \n",
        "    voc2ind = {}\n",
        "    # Compute voc2ind and transform the data into an integer representation of the tokens.\n",
        "   \n",
        "    word_count = {}\n",
        "    # word tokenization\n",
        "    data = word_tokenize(data)\n",
        "    \n",
        "    #filtering out punctuation\n",
        "    data = [word.lower() for word in data if word.isalpha()]\n",
        "    print (len(data))\n",
        "    for word in data:\n",
        "        if word not in word_count.keys():\n",
        "            word_count[word] = 1\n",
        "        else:\n",
        "            word_count[word] += 1\n",
        "        #pass # TODO Fill this in\n",
        "    print (len(word_count.keys()))\n",
        "    \n",
        "    subs_words = []\n",
        "    for key, val in word_count.items():\n",
        "        if val < 5:\n",
        "            subs_words.append(key)\n",
        "    print (len(subs_words))\n",
        "    \n",
        "    replace_count = 0\n",
        "    for (i, word) in enumerate(data):\n",
        "        if word in subs_words:\n",
        "            data[i] = \"<unknown>\"\n",
        "            replace_count += 1\n",
        "    print (\"No of words getting substituted = \", replace_count)\n",
        "    \n",
        "    count = 0       \n",
        "    for word in data:\n",
        "        if word not in voc2ind.keys():\n",
        "            voc2ind[word] = count\n",
        "            count += 1\n",
        "    voc2ind[' '] = count\n",
        "    \n",
        "    print (len(voc2ind.keys()))\n",
        "            \n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "    \n",
        "    \n",
        "    list_indexed = []\n",
        "    for w in data:\n",
        "        list_indexed.append(voc2ind[w])\n",
        "        list_indexed.append(voc2ind[' '])\n",
        "    \n",
        "    data = []\n",
        "    split_index = math.ceil((4 * len(list_indexed) / 5))\n",
        "    print (split_index)\n",
        "    \n",
        "    word_indexed = np.array(list_indexed)\n",
        "    list_indexed = []\n",
        "    \n",
        "    train_text = word_indexed[:split_index] \n",
        "    # TODO Fill this in\n",
        "    test_text = word_indexed[split_index:]\n",
        "    # TODO Fill this in\n",
        "    \n",
        "    print (train_text.shape, test_text.shape)\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_word_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_word_test.pkl', 'wb'))\n",
        "    \n",
        "    return voc2ind\n",
        "    \n",
        "voc2ind = prepare_data(DATA_PATH + 'harry_potter.txt')\n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1075965\n",
            "21416\n",
            "12592\n",
            "No of words getting substituted =  22106\n",
            "8826\n",
            "1721544\n",
            "(1721544,) (430386,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DTWrSNvB2W2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "5d59dad5-e8bb-42d8-ea2b-9cbef8a7d635"
      },
      "cell_type": "code",
      "source": [
        "## With PUNCTUATION\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import re\n",
        "import math\n",
        "def prepare_data(data_path):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "    \n",
        "    # TODO Add more preprocessing\n",
        "    data = re.sub('\\s+', ' ', data).strip()\n",
        "    \n",
        "    voc2ind = {}\n",
        "    # Compute voc2ind and transform the data into an integer representation of the tokens.\n",
        "   \n",
        "    word_count = {}\n",
        "    # word tokenization\n",
        "    data = word_tokenize(data)\n",
        "    \n",
        "    data = [word.lower() for word in data]\n",
        "    print (len(data))\n",
        "    for word in data:\n",
        "        if word not in word_count.keys():\n",
        "            word_count[word] = 1\n",
        "        else:\n",
        "            word_count[word] += 1\n",
        "        #pass # TODO Fill this in\n",
        "    print (len(word_count.keys()))\n",
        "    \n",
        "    subs_words = []\n",
        "    for key, val in word_count.items():\n",
        "        if val < 5:\n",
        "            subs_words.append(key)\n",
        "    print (len(subs_words))\n",
        "    \n",
        "    replace_count = 0\n",
        "    for (i, word) in enumerate(data):\n",
        "        if word in subs_words:\n",
        "            data[i] = \"<unknown>\"\n",
        "            replace_count += 1\n",
        "    print (\"No of words getting substituted = \", replace_count)\n",
        "    \n",
        "    count = 0       \n",
        "    for word in data:\n",
        "        if word not in voc2ind.keys():\n",
        "            voc2ind[word] = count\n",
        "            count += 1\n",
        "    voc2ind[' '] = count\n",
        "    \n",
        "    print (len(voc2ind.keys()))\n",
        "            \n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "    \n",
        "    \n",
        "    list_indexed = []\n",
        "    for w in data:\n",
        "        list_indexed.append(voc2ind[w])\n",
        "        list_indexed.append(voc2ind[' '])\n",
        "    \n",
        "    data = []\n",
        "    split_index = math.ceil((4 * len(list_indexed) / 5))\n",
        "    print (split_index)\n",
        "    \n",
        "    word_indexed = np.array(list_indexed)\n",
        "    list_indexed = []\n",
        "    \n",
        "    train_text = word_indexed[:split_index] \n",
        "    # TODO Fill this in\n",
        "    test_text = word_indexed[split_index:]\n",
        "    # TODO Fill this in\n",
        "    \n",
        "    print (train_text.shape, test_text.shape)\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_wordpunct_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_wordpunct_test.pkl', 'wb'))\n",
        "    \n",
        "    return voc2ind\n",
        "    \n",
        "voc2ind = prepare_data(DATA_PATH + 'harry_potter.txt')\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1379989\n",
            "25175\n",
            "16106\n",
            "No of words getting substituted =  26657\n",
            "9071\n",
            "2207983\n",
            "(2207983,) (551995,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CogurKnp_tVd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word.lower()] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kTs8Csx22XrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "WB98PuIigutK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HarryPotterWordDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(HarryPotterWordDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "        num = len(dataset['tokens']) - (len(dataset['tokens']) % batch_size)\n",
        "        self.dataset = dataset['tokens'][:num]\n",
        "        # TODO: Any preprocessing on the data to get it to the right shape.\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        chunk_size = math.floor(len(self.dataset) / self.batch_size)\n",
        "        num_examples_batch = math.floor(chunk_size / self.sequence_length)\n",
        "        length = self.batch_size * num_examples_batch\n",
        "        return length\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        # raise NotImplementedError\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence as described above.\n",
        "        # The data and labels should be torch long tensors.\n",
        "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
        "        # in and how far down in the chunk you are.\n",
        "        chunk_size = math.floor(len(self.dataset) / self.batch_size)\n",
        "        within_chunk, chunk_idx = divmod(idx, self.batch_size)\n",
        "        new_idx = chunk_idx * chunk_size + within_chunk * self.sequence_length\n",
        "        last_idx = new_idx + self.sequence_length - 1\n",
        "        data = self.dataset[new_idx:last_idx+2]\n",
        "        data = torch.LongTensor(data)\n",
        "        return data[:-1], data[1:]\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXP_H3oh2lu5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3: Defining the Network\n"
      ]
    },
    {
      "metadata": {
        "id": "rtXkEkP7q4Xk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 1\n",
        "\n",
        "class HarryPotterWordNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size):\n",
        "        super(HarryPotterWordNet, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        self.gru = nn.GRU(self.feature_size, self.feature_size, num_layers=2, batch_first=True)\n",
        "        self.decoder = nn.Linear(self.feature_size, self.vocab_size)\n",
        "        \n",
        "        # This shares the encoder and decoder weights as described in lecture.\n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        batch_size = x.shape[0]\n",
        "        sequence_length = x.shape[1]\n",
        "        \n",
        "        # TODO finish defining the forward pass.\n",
        "        # You should return the output from the decoder as well as the hidden state given by the gru.\n",
        "        # raise NotImplementedError\n",
        "        x = x.view(-1,1)\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(batch_size, sequence_length, self.feature_size)\n",
        "        x, hidden_state = self.gru(x, hidden_state)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden_state\n",
        "\n",
        "      \n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-68Sx8gAft6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Word Generation"
      ]
    },
    {
      "metadata": {
        "id": "G4_QdKNcAjO0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BEAM_WIDTH = 20\n",
        "\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "\n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden, temperature=1)\n",
        "\n",
        "        outputs = []\n",
        "        # Initializes the beam list\n",
        "        beam = [([], output, hidden, 0)]\n",
        "        for ii in range(sequence_length):\n",
        "            if sampling_strategy == 'max':\n",
        "                # TODO max sampling strategy\n",
        "                # raise NotImplementedError\n",
        "                _, pred_label = torch.max(output, 1)\n",
        "                outputs.append(pred_label)\n",
        "                new_input = pred_label.unsqueeze(1)\n",
        "                output, hidden = model.inference(new_input, hidden, temperature=1)\n",
        "                \n",
        "            elif sampling_strategy == 'sample':\n",
        "                # TODO: Probability-based sampling strategy.\n",
        "                # raise NotImplementedError \n",
        "                pred_label = torch.multinomial(output, 1, True)\n",
        "                outputs.append(pred_label)\n",
        "                new_input = pred_label.unsqueeze(1)\n",
        "                output, hidden = model.inference(new_input, hidden, temperature=1)\n",
        "                \n",
        "\n",
        "            elif sampling_strategy == 'beam':\n",
        "                # Todo: beam search sampling strategy\n",
        "                # raise NotImplementedError\n",
        "                new_beam = []\n",
        "                for be in beam:\n",
        "                    pred_label = torch.multinomial(be[1], 2, True)\n",
        "                    for p in pred_label[0]:\n",
        "                        out1, hid1 = model.inference(p, be[2], temperature=1)\n",
        "                        new_beam.append((be[0]+[p], out1, hid1, be[3]+np.log(be[1][0][p].item())))\n",
        "                new_beam = sorted(new_beam, key=lambda x: x[3], reverse=True)[:beam_width]\n",
        "                beam = new_beam\n",
        "        \n",
        "        if sampling_strategy == 'beam':\n",
        "            outputs = beam[0][0]\n",
        "        \n",
        "        return vocab.array_to_words(seed_words_arr.tolist() + outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_rjIQ7bgBG6l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 5: Training"
      ]
    },
    {
      "metadata": {
        "id": "WL3ZpdE95aGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62583
        },
        "outputId": "02dcc25c-43fc-4695-c210-10335afdddb1"
      },
      "cell_type": "code",
      "source": [
        "## With PUNCTUATION\n",
        "SEQUENCE_LENGTH = 30\n",
        "BATCH_SIZE = 128\n",
        "FEATURE_SIZE = 1024\n",
        "TEST_BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs_wordss_punct/log.pkl'\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "data_train = HarryPotterWordDataset(DATA_PATH + 'harry_potter_wordpunct_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = HarryPotterWordDataset(DATA_PATH + 'harry_potter_wordpunct_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = HarryPotterWordNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "# Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "# We will talk more about different optimization methods in class.\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints_wordss_punct')\n",
        "\n",
        "train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(model, device, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints_wordss_punct/%03d.pt' % epoch)\n",
        "        seed_words = 'Harry Potter '\n",
        "        for ii in range(10):\n",
        "            generated_sentence = generate_language(model, device, seed_words, 30, vocab, 'sample')\n",
        "            print('generated sample\\t', generated_sentence)\n",
        "        generated_sentence = generate_language(model, device, seed_words, 30, vocab, 'beam')\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    model.save_model(DATA_PATH + 'checkpoints_wordss_punct/%03d.pt' % epoch, 0)\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "Restoring:\n",
            "Shape mismatch for var encoder.weight expected torch.Size([9071, 1024]) got torch.Size([9071, 512])\n",
            "Shape mismatch for var gru.weight_ih_l0 expected torch.Size([3072, 1024]) got torch.Size([1536, 512])\n",
            "Shape mismatch for var gru.weight_hh_l0 expected torch.Size([3072, 1024]) got torch.Size([1536, 512])\n",
            "Shape mismatch for var gru.bias_ih_l0 expected torch.Size([3072]) got torch.Size([1536])\n",
            "Shape mismatch for var gru.bias_hh_l0 expected torch.Size([3072]) got torch.Size([1536])\n",
            "Shape mismatch for var gru.weight_ih_l1 expected torch.Size([3072, 1024]) got torch.Size([1536, 512])\n",
            "Shape mismatch for var gru.weight_hh_l1 expected torch.Size([3072, 1024]) got torch.Size([1536, 512])\n",
            "Shape mismatch for var gru.bias_ih_l1 expected torch.Size([3072]) got torch.Size([1536])\n",
            "Shape mismatch for var gru.bias_hh_l1 expected torch.Size([3072]) got torch.Size([1536])\n",
            "Shape mismatch for var decoder.weight expected torch.Size([9071, 1024]) got torch.Size([9071, 512])\n",
            "decoder.bias -> \ttorch.Size([9071]) = 0MB\n",
            "\n",
            "Did not restore:\n",
            "\tdecoder.weight\n",
            "\tencoder.weight\n",
            "\tgru.bias_hh_l0\n",
            "\tgru.bias_hh_l1\n",
            "\tgru.bias_ih_l0\n",
            "\tgru.bias_ih_l1\n",
            "\tgru.weight_hh_l0\n",
            "\tgru.weight_hh_l1\n",
            "\tgru.weight_ih_l0\n",
            "\tgru.weight_ih_l1\n",
            "Initialized but did not modify:\n",
            "\tdecoder.weight\n",
            "\tencoder.weight\n",
            "\tgru.bias_hh_l0\n",
            "\tgru.bias_hh_l1\n",
            "\tgru.bias_ih_l0\n",
            "\tgru.bias_ih_l1\n",
            "\tgru.weight_hh_l0\n",
            "\tgru.weight_hh_l1\n",
            "\tgru.weight_ih_l0\n",
            "\tgru.weight_ih_l1\n",
            "Restored /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/009.pt\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\tfactlyheadmistressheadmistressheadmistressonchargingonbletchleybletchleynicelyfactlycurlycurlyfabulousshallacidclassroomghostsragedampragecloudlesscloudlessnagininaginicrumpetshallshallchasersrage\n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tfavorfavoronrosierfactlybloodstainedragenaginibletchleynaginichasersragerageshallshallhufflepuffragefavorfavorfavorfactly<unknown>ragewhipragewhipragejackchaserson\n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tfactlyheyshallmetallicshallsilveryfactlybletchleybletchleydozeddozedsilveryfactlynaginifactlyfactlyfactlysouthshallshallshallhufflepuffshallgatheredragegatheredantifavorfavorsafety\n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\tfactlyfactlyfactlyragerageconcealconcealfabulousmulcibertailingconcealsafetyfactlyfavorrageragerageshallshallshallshallnarglesfactlycurlycurlybeggingshallonshallshall\n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\tskeletalskeletalshallrageragerageskeletalskeletalonsoupfactlyfactlyfactlybletchleybletchleymulcibermulcibershallshallshallshallfactlyfactlyfactlyfactlybletchleybletchleyshallfavorshall\n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tfactlyjackfactlyjackchaserswrought-irononononononwrought-ironwrought-ironwrought-ironwrought-ironfabulousfavorcurlingsquealrequirerequireshallonmerchandisemerchandisedregsfavorfavorfavoremptying\n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\tjackjackfactlygalleonsgalleonsunearthedfactlysouthfactlyragerageskeletalshallhellohellohelloragehellofactlyscreechyoungersquidsquidbludgersragefactlyfactlymulcibermulciberfactly\n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\tmulcibersquidfactlyfactlyfactlyfactlyfactlyheadmistressheadmistressbletchleybletchleybletchleymulciberrosierfactlysquealsquealsessionscurlycurlycurlyhearinggalleonsoldestfactlyscowlingskeletalcloudlessshallfabulous\n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\tjackjackjackjackshallmetallicbletchleyapplaudingfactlycommentatingshallfactlyfactlyourpotatosouthsouthbletchleybletchleynicelynicelyacidacidtotteredtotteredputputanguishchasersbletchley\n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\tdozedmateonononclassroomnaginiclassroomclassroomfabulousragegrotesquemulcibernagininaginiragefactlynagininagininagininagininagininagininagininagininagininagininagininagininagini\n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\theybletchleyonnaginionrageragebletchleyonfavorfavorononragerageragegatheredgatheredrageachievementfactlyfactlyfactlyfactlyfactlymatematebranchesfactlymulciber\n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tshallononononteamulciberteamulciberfavorfavorconcealshallpotatofactlypotatoragerageragerageragefactlyfactlyrageragefavorfavorfavorfavorfavor\n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tragebletchleyfactlyfactlyfactlylibrarianragebotheredshalljackjackragecurlycurlycurlyjackjackreplacingonfactlyfactlyaloneslamononschoolsschoolsschoolsfactlyschools\n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\tonrageragejackmulcibermonstershallmerchandisemerchandisematematematebletchleyrageragerageragerageragebletchleybletchleybletchleyfactlymulcibermulcibernastilyonononon\n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\tfactlyrageragechaserscurlycurlycurlycurlycurlyfabulousfabuloushomefactlyononrageragerageragefabulousfavorfabulousfabulousfabulousfabulousfabulousshallskeletalskeletalskeletal\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 23.6759, Accuracy: 161/549120 (0%)\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 9 [0/73472 (0%)]\tLoss: 22.657984\n",
            "Train Epoch: 9 [1280/73472 (2%)]\tLoss: 16.812901\n",
            "Train Epoch: 9 [2560/73472 (3%)]\tLoss: 9.412551\n",
            "Train Epoch: 9 [3840/73472 (5%)]\tLoss: 6.491377\n",
            "Train Epoch: 9 [5120/73472 (7%)]\tLoss: 5.187615\n",
            "Train Epoch: 9 [6400/73472 (9%)]\tLoss: 4.508964\n",
            "Train Epoch: 9 [7680/73472 (10%)]\tLoss: 4.240687\n",
            "Train Epoch: 9 [8960/73472 (12%)]\tLoss: 4.852113\n",
            "Train Epoch: 9 [10240/73472 (14%)]\tLoss: 4.944651\n",
            "Train Epoch: 9 [11520/73472 (16%)]\tLoss: 4.389073\n",
            "Train Epoch: 9 [12800/73472 (17%)]\tLoss: 3.840761\n",
            "Train Epoch: 9 [14080/73472 (19%)]\tLoss: 3.546382\n",
            "Train Epoch: 9 [15360/73472 (21%)]\tLoss: 3.450785\n",
            "Train Epoch: 9 [16640/73472 (23%)]\tLoss: 3.426669\n",
            "Train Epoch: 9 [17920/73472 (24%)]\tLoss: 3.296882\n",
            "Train Epoch: 9 [19200/73472 (26%)]\tLoss: 3.360146\n",
            "Train Epoch: 9 [20480/73472 (28%)]\tLoss: 3.319626\n",
            "Train Epoch: 9 [21760/73472 (30%)]\tLoss: 3.327392\n",
            "Train Epoch: 9 [23040/73472 (31%)]\tLoss: 3.337729\n",
            "Train Epoch: 9 [24320/73472 (33%)]\tLoss: 3.145764\n",
            "Train Epoch: 9 [25600/73472 (35%)]\tLoss: 3.113767\n",
            "Train Epoch: 9 [26880/73472 (37%)]\tLoss: 3.079682\n",
            "Train Epoch: 9 [28160/73472 (38%)]\tLoss: 2.995664\n",
            "Train Epoch: 9 [29440/73472 (40%)]\tLoss: 3.055721\n",
            "Train Epoch: 9 [30720/73472 (42%)]\tLoss: 2.956518\n",
            "Train Epoch: 9 [32000/73472 (44%)]\tLoss: 3.008573\n",
            "Train Epoch: 9 [33280/73472 (45%)]\tLoss: 3.008824\n",
            "Train Epoch: 9 [34560/73472 (47%)]\tLoss: 2.918057\n",
            "Train Epoch: 9 [35840/73472 (49%)]\tLoss: 2.925669\n",
            "Train Epoch: 9 [37120/73472 (51%)]\tLoss: 2.774595\n",
            "Train Epoch: 9 [38400/73472 (52%)]\tLoss: 2.862178\n",
            "Train Epoch: 9 [39680/73472 (54%)]\tLoss: 2.801532\n",
            "Train Epoch: 9 [40960/73472 (56%)]\tLoss: 2.864825\n",
            "Train Epoch: 9 [42240/73472 (57%)]\tLoss: 2.898731\n",
            "Train Epoch: 9 [43520/73472 (59%)]\tLoss: 2.819109\n",
            "Train Epoch: 9 [44800/73472 (61%)]\tLoss: 2.623781\n",
            "Train Epoch: 9 [46080/73472 (63%)]\tLoss: 2.771522\n",
            "Train Epoch: 9 [47360/73472 (64%)]\tLoss: 2.780437\n",
            "Train Epoch: 9 [48640/73472 (66%)]\tLoss: 2.775054\n",
            "Train Epoch: 9 [49920/73472 (68%)]\tLoss: 2.833747\n",
            "Train Epoch: 9 [51200/73472 (70%)]\tLoss: 2.805698\n",
            "Train Epoch: 9 [52480/73472 (71%)]\tLoss: 2.720227\n",
            "Train Epoch: 9 [53760/73472 (73%)]\tLoss: 2.784703\n",
            "Train Epoch: 9 [55040/73472 (75%)]\tLoss: 2.719278\n",
            "Train Epoch: 9 [56320/73472 (77%)]\tLoss: 2.810359\n",
            "Train Epoch: 9 [57600/73472 (78%)]\tLoss: 2.747371\n",
            "Train Epoch: 9 [58880/73472 (80%)]\tLoss: 2.701322\n",
            "Train Epoch: 9 [60160/73472 (82%)]\tLoss: 2.714438\n",
            "Train Epoch: 9 [61440/73472 (84%)]\tLoss: 2.680317\n",
            "Train Epoch: 9 [62720/73472 (85%)]\tLoss: 2.580669\n",
            "Train Epoch: 9 [64000/73472 (87%)]\tLoss: 2.694664\n",
            "Train Epoch: 9 [65280/73472 (89%)]\tLoss: 2.625650\n",
            "Train Epoch: 9 [66560/73472 (91%)]\tLoss: 2.598415\n",
            "Train Epoch: 9 [67840/73472 (92%)]\tLoss: 2.700729\n",
            "Train Epoch: 9 [69120/73472 (94%)]\tLoss: 2.601549\n",
            "Train Epoch: 9 [70400/73472 (96%)]\tLoss: 2.635686\n",
            "Train Epoch: 9 [71680/73472 (98%)]\tLoss: 2.630118\n",
            "Train Epoch: 9 [72960/73472 (99%)]\tLoss: 2.723572\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\trage . . . and he , harry the door , and ? down the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tharry <unknown> him <unknown> and a . were still . `` . . . 's \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto ron to to to his feet . `` , had still of and and \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` whole had of to a air . . . it had <unknown> , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, i well , i potter '' '' `` you n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's been to be <unknown> right , '' ? '' said said . . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. though as though had . harry was into . the 's <unknown> the and \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the head . . been to . . he he had that wand and and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a . snape was . 'm to . to 'm n't think of \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , they do expecting out <unknown> out , '' you , you you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twent at a air . and a common . the . `` , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tout . and . '' <unknown> - got a <unknown> of of <unknown> of but \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand and he it . his to . . wand and he pomfrey , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t. got . . into the , '' standing . . the . had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. most . had . to off the only and and that he could hear \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.6809, Accuracy: 326981/549120 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/009.pt\n",
            "\n",
            "generated sample\t harry potter names interestedly ring freckled involved cracking nephew that out of people people funeral <unknown> ... \n",
            "generated sample\t harry potter 'arry more pensieve , she images look at the pinstriped properly . he 'd the \n",
            "generated sample\t harry potter factly and hite his aid screen into a privately wo for him palace to work \n",
            "generated sample\t harry potter intruder spindly privately burbage blur eloise motionless . `` come resisted affect , waves coughed \n",
            "generated sample\t harry potter word fools candles west , she 'll lime needs to effort where clicked is 'er \n",
            "generated sample\t harry potter derrick word when absolute boots impersonating sucking increasing and <unknown> of hostages deaths is a \n",
            "generated sample\t harry potter rage . `` what is to receive we all ? '' harry liked her . \n",
            "generated sample\t harry potter throats invited . `` no slimy fancies - '' `` fight lengths with them , \n",
            "generated sample\t harry potter limb , which were <unknown> inhabited protested soaked and a convenient of series herself at \n",
            "generated sample\t harry potter of soothing yesterday plummeted vertical celestina out of the most dodged will pelted nothing important \n",
            "generated beam\t\t harry potter sake once separate <unknown> bar and penalties had all interesting . harry seventh-floor his head \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 10 [0/73472 (0%)]\tLoss: 2.824575\n",
            "Train Epoch: 10 [1280/73472 (2%)]\tLoss: 2.593047\n",
            "Train Epoch: 10 [2560/73472 (3%)]\tLoss: 2.606999\n",
            "Train Epoch: 10 [3840/73472 (5%)]\tLoss: 2.655024\n",
            "Train Epoch: 10 [5120/73472 (7%)]\tLoss: 2.594697\n",
            "Train Epoch: 10 [6400/73472 (9%)]\tLoss: 2.554355\n",
            "Train Epoch: 10 [7680/73472 (10%)]\tLoss: 2.585341\n",
            "Train Epoch: 10 [8960/73472 (12%)]\tLoss: 2.587833\n",
            "Train Epoch: 10 [10240/73472 (14%)]\tLoss: 2.630818\n",
            "Train Epoch: 10 [11520/73472 (16%)]\tLoss: 2.564551\n",
            "Train Epoch: 10 [12800/73472 (17%)]\tLoss: 2.589376\n",
            "Train Epoch: 10 [14080/73472 (19%)]\tLoss: 2.504988\n",
            "Train Epoch: 10 [15360/73472 (21%)]\tLoss: 2.572766\n",
            "Train Epoch: 10 [16640/73472 (23%)]\tLoss: 2.581461\n",
            "Train Epoch: 10 [17920/73472 (24%)]\tLoss: 2.564634\n",
            "Train Epoch: 10 [19200/73472 (26%)]\tLoss: 2.602748\n",
            "Train Epoch: 10 [20480/73472 (28%)]\tLoss: 2.585706\n",
            "Train Epoch: 10 [21760/73472 (30%)]\tLoss: 2.604977\n",
            "Train Epoch: 10 [23040/73472 (31%)]\tLoss: 2.580592\n",
            "Train Epoch: 10 [24320/73472 (33%)]\tLoss: 2.477406\n",
            "Train Epoch: 10 [25600/73472 (35%)]\tLoss: 2.517879\n",
            "Train Epoch: 10 [26880/73472 (37%)]\tLoss: 2.495580\n",
            "Train Epoch: 10 [28160/73472 (38%)]\tLoss: 2.471375\n",
            "Train Epoch: 10 [29440/73472 (40%)]\tLoss: 2.517053\n",
            "Train Epoch: 10 [30720/73472 (42%)]\tLoss: 2.478812\n",
            "Train Epoch: 10 [32000/73472 (44%)]\tLoss: 2.483800\n",
            "Train Epoch: 10 [33280/73472 (45%)]\tLoss: 2.531250\n",
            "Train Epoch: 10 [34560/73472 (47%)]\tLoss: 2.489310\n",
            "Train Epoch: 10 [35840/73472 (49%)]\tLoss: 2.500491\n",
            "Train Epoch: 10 [37120/73472 (51%)]\tLoss: 2.391777\n",
            "Train Epoch: 10 [38400/73472 (52%)]\tLoss: 2.477168\n",
            "Train Epoch: 10 [39680/73472 (54%)]\tLoss: 2.442737\n",
            "Train Epoch: 10 [40960/73472 (56%)]\tLoss: 2.468462\n",
            "Train Epoch: 10 [42240/73472 (57%)]\tLoss: 2.533978\n",
            "Train Epoch: 10 [43520/73472 (59%)]\tLoss: 2.489810\n",
            "Train Epoch: 10 [44800/73472 (61%)]\tLoss: 2.360543\n",
            "Train Epoch: 10 [46080/73472 (63%)]\tLoss: 2.435664\n",
            "Train Epoch: 10 [47360/73472 (64%)]\tLoss: 2.447797\n",
            "Train Epoch: 10 [48640/73472 (66%)]\tLoss: 2.434235\n",
            "Train Epoch: 10 [49920/73472 (68%)]\tLoss: 2.507448\n",
            "Train Epoch: 10 [51200/73472 (70%)]\tLoss: 2.462371\n",
            "Train Epoch: 10 [52480/73472 (71%)]\tLoss: 2.385812\n",
            "Train Epoch: 10 [53760/73472 (73%)]\tLoss: 2.470104\n",
            "Train Epoch: 10 [55040/73472 (75%)]\tLoss: 2.430351\n",
            "Train Epoch: 10 [56320/73472 (77%)]\tLoss: 2.512843\n",
            "Train Epoch: 10 [57600/73472 (78%)]\tLoss: 2.453604\n",
            "Train Epoch: 10 [58880/73472 (80%)]\tLoss: 2.428931\n",
            "Train Epoch: 10 [60160/73472 (82%)]\tLoss: 2.455136\n",
            "Train Epoch: 10 [61440/73472 (84%)]\tLoss: 2.430163\n",
            "Train Epoch: 10 [62720/73472 (85%)]\tLoss: 2.351714\n",
            "Train Epoch: 10 [64000/73472 (87%)]\tLoss: 2.450042\n",
            "Train Epoch: 10 [65280/73472 (89%)]\tLoss: 2.400099\n",
            "Train Epoch: 10 [66560/73472 (91%)]\tLoss: 2.386262\n",
            "Train Epoch: 10 [67840/73472 (92%)]\tLoss: 2.461685\n",
            "Train Epoch: 10 [69120/73472 (94%)]\tLoss: 2.389757\n",
            "Train Epoch: 10 [70400/73472 (96%)]\tLoss: 2.418172\n",
            "Train Epoch: 10 [71680/73472 (98%)]\tLoss: 2.433000\n",
            "Train Epoch: 10 [72960/73472 (99%)]\tLoss: 2.521325\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\tof . . . . he , he the door , which , down the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tharry <unknown> him <unknown> . a <unknown> were still . `` . . . , \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he to to to his feet . `` , had still of and and \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> , , the air . . . he was <unknown> , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, i well , i , '' '' `` oh n't you , , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be <unknown> right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. though . though could see he was , , the he <unknown> his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\twas wand were was been , , . he , was his wand and but \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore was , have to '' i was n't know i \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though have a out <unknown> , , '' you , '' you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas , the air . which a <unknown> . the , `` , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\t. . and , '' - - got a <unknown> of them <unknown> , who \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand and he , , his , . . wand and he pomfrey , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> . , of the , '' a , of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> . was , been off the room . and that he could see \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.5158, Accuracy: 330226/549120 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/010.pt\n",
            "\n",
            "generated sample\t harry potter this ? than have chasing the ministry , xenophilius by the shaggy were the portrait \n",
            "generated sample\t harry potter boos volume on the quidditch team back 's emerging to meet after something , pound \n",
            "generated sample\t harry potter correctly disastrous , back to concealment it , a mistake fix , snape cleaned resurrection \n",
            "generated sample\t harry potter tough . she has just listened <unknown> caps onto the floor . hermione , a \n",
            "generated sample\t harry potter meeting hermione . `` well , i was always pomfrey about contents on the twin \n",
            "generated sample\t harry potter stared pearly by nestled . `` it 's stinging , he has stopped school second \n",
            "generated sample\t harry potter apparated in the lessons common room , ron caught his wand and settle piled . \n",
            "generated sample\t harry potter eavesdrop , which was still n't , come images indignantly . `` hurry it came \n",
            "generated sample\t harry potter gathered dregs was in there . `` once i scar now really cares was in \n",
            "generated sample\t harry potter en moleskin binding , professor lupin 's lap was opportunity intervened and a brush . \n",
            "generated beam\t\t harry potter coated mounting . `` <unknown> ! '' said harry black . `` answers 've got \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 11 [0/73472 (0%)]\tLoss: 2.586174\n",
            "Train Epoch: 11 [1280/73472 (2%)]\tLoss: 2.418091\n",
            "Train Epoch: 11 [2560/73472 (3%)]\tLoss: 2.423440\n",
            "Train Epoch: 11 [3840/73472 (5%)]\tLoss: 2.458140\n",
            "Train Epoch: 11 [5120/73472 (7%)]\tLoss: 2.415768\n",
            "Train Epoch: 11 [6400/73472 (9%)]\tLoss: 2.388951\n",
            "Train Epoch: 11 [7680/73472 (10%)]\tLoss: 2.419632\n",
            "Train Epoch: 11 [8960/73472 (12%)]\tLoss: 2.432848\n",
            "Train Epoch: 11 [10240/73472 (14%)]\tLoss: 2.461420\n",
            "Train Epoch: 11 [11520/73472 (16%)]\tLoss: 2.413580\n",
            "Train Epoch: 11 [12800/73472 (17%)]\tLoss: 2.437138\n",
            "Train Epoch: 11 [14080/73472 (19%)]\tLoss: 2.359482\n",
            "Train Epoch: 11 [15360/73472 (21%)]\tLoss: 2.430189\n",
            "Train Epoch: 11 [16640/73472 (23%)]\tLoss: 2.447834\n",
            "Train Epoch: 11 [17920/73472 (24%)]\tLoss: 2.432244\n",
            "Train Epoch: 11 [19200/73472 (26%)]\tLoss: 2.463950\n",
            "Train Epoch: 11 [20480/73472 (28%)]\tLoss: 2.446639\n",
            "Train Epoch: 11 [21760/73472 (30%)]\tLoss: 2.469412\n",
            "Train Epoch: 11 [23040/73472 (31%)]\tLoss: 2.456795\n",
            "Train Epoch: 11 [24320/73472 (33%)]\tLoss: 2.355226\n",
            "Train Epoch: 11 [25600/73472 (35%)]\tLoss: 2.389342\n",
            "Train Epoch: 11 [26880/73472 (37%)]\tLoss: 2.377225\n",
            "Train Epoch: 11 [28160/73472 (38%)]\tLoss: 2.362076\n",
            "Train Epoch: 11 [29440/73472 (40%)]\tLoss: 2.387724\n",
            "Train Epoch: 11 [30720/73472 (42%)]\tLoss: 2.360952\n",
            "Train Epoch: 11 [32000/73472 (44%)]\tLoss: 2.380026\n",
            "Train Epoch: 11 [33280/73472 (45%)]\tLoss: 2.412735\n",
            "Train Epoch: 11 [34560/73472 (47%)]\tLoss: 2.374395\n",
            "Train Epoch: 11 [35840/73472 (49%)]\tLoss: 2.405579\n",
            "Train Epoch: 11 [37120/73472 (51%)]\tLoss: 2.293464\n",
            "Train Epoch: 11 [38400/73472 (52%)]\tLoss: 2.384517\n",
            "Train Epoch: 11 [39680/73472 (54%)]\tLoss: 2.344995\n",
            "Train Epoch: 11 [40960/73472 (56%)]\tLoss: 2.378829\n",
            "Train Epoch: 11 [42240/73472 (57%)]\tLoss: 2.424312\n",
            "Train Epoch: 11 [43520/73472 (59%)]\tLoss: 2.391616\n",
            "Train Epoch: 11 [44800/73472 (61%)]\tLoss: 2.293393\n",
            "Train Epoch: 11 [46080/73472 (63%)]\tLoss: 2.358359\n",
            "Train Epoch: 11 [47360/73472 (64%)]\tLoss: 2.367916\n",
            "Train Epoch: 11 [48640/73472 (66%)]\tLoss: 2.340656\n",
            "Train Epoch: 11 [49920/73472 (68%)]\tLoss: 2.438896\n",
            "Train Epoch: 11 [51200/73472 (70%)]\tLoss: 2.383182\n",
            "Train Epoch: 11 [52480/73472 (71%)]\tLoss: 2.313435\n",
            "Train Epoch: 11 [53760/73472 (73%)]\tLoss: 2.378166\n",
            "Train Epoch: 11 [55040/73472 (75%)]\tLoss: 2.360886\n",
            "Train Epoch: 11 [56320/73472 (77%)]\tLoss: 2.444465\n",
            "Train Epoch: 11 [57600/73472 (78%)]\tLoss: 2.389456\n",
            "Train Epoch: 11 [58880/73472 (80%)]\tLoss: 2.350021\n",
            "Train Epoch: 11 [60160/73472 (82%)]\tLoss: 2.372282\n",
            "Train Epoch: 11 [61440/73472 (84%)]\tLoss: 2.360393\n",
            "Train Epoch: 11 [62720/73472 (85%)]\tLoss: 2.281857\n",
            "Train Epoch: 11 [64000/73472 (87%)]\tLoss: 2.375129\n",
            "Train Epoch: 11 [65280/73472 (89%)]\tLoss: 2.335021\n",
            "Train Epoch: 11 [66560/73472 (91%)]\tLoss: 2.314308\n",
            "Train Epoch: 11 [67840/73472 (92%)]\tLoss: 2.402333\n",
            "Train Epoch: 11 [69120/73472 (94%)]\tLoss: 2.339414\n",
            "Train Epoch: 11 [70400/73472 (96%)]\tLoss: 2.346778\n",
            "Train Epoch: 11 [71680/73472 (98%)]\tLoss: 2.370098\n",
            "Train Epoch: 11 [72960/73472 (99%)]\tLoss: 2.450216\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. . . . . the , the the door , which . , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tthe <unknown> the <unknown> . the <unknown> were <unknown> . `` . `` `` 's \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he . to to the feet . `` , had the of and . \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> , to the air , . . he was <unknown> , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, i well , i , '' '' `` oh n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be <unknown> right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. though as though had see he was on . the 's . the , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\twas head had . been . . . he he was his wand . but \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore 's to have to . i 'm n't know i \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though have <unknown> out <unknown> . . '' you , '' you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas at the air . the the <unknown> . the , `` 's the , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto . `` , '' - - got a <unknown> of the <unknown> . but \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand . he the . his , . . head . he pomfrey , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> . . of the , '' a . . the . had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> . had . been off the room . the that he could see \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.4459, Accuracy: 331984/549120 (60%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/011.pt\n",
            "\n",
            "generated sample\t harry potter discarded airily , which amusement harry , ron and hermione told him to me like \n",
            "generated sample\t harry potter dennis professor vos . `` professor ! '' roared his breath . `` oh , \n",
            "generated sample\t harry potter secure whirling harry . `` it 's wondering regulus goblin-made <unknown> his ; with professor \n",
            "generated sample\t harry potter ghostly , choice , as though it could see is so % the small playing \n",
            "generated sample\t harry potter zacharias cloudy <unknown> hesitate . `` ah the ministry of magic in the chatter for \n",
            "generated sample\t harry potter detail . `` hesitation things robes , the dangerous our night ! '' bewitched works \n",
            "generated sample\t harry potter ask , even then . `` wo n't freeze it shade me with them glistening \n",
            "generated sample\t harry potter thinned dead for whipping learning . i 'm sure he merry by , his new \n",
            "generated sample\t harry potter occasionally freely and animals to join her comical , but snape standing up with mrs. \n",
            "generated sample\t harry potter madly . everything <unknown> ogden let him out to his mouth , not show like \n",
            "generated beam\t\t harry potter wand , looking up with spiders . `` she is doing so much that is \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 12 [0/73472 (0%)]\tLoss: 2.523557\n",
            "Train Epoch: 12 [1280/73472 (2%)]\tLoss: 2.367139\n",
            "Train Epoch: 12 [2560/73472 (3%)]\tLoss: 2.395343\n",
            "Train Epoch: 12 [3840/73472 (5%)]\tLoss: 2.399940\n",
            "Train Epoch: 12 [5120/73472 (7%)]\tLoss: 2.367997\n",
            "Train Epoch: 12 [6400/73472 (9%)]\tLoss: 2.340134\n",
            "Train Epoch: 12 [7680/73472 (10%)]\tLoss: 2.376494\n",
            "Train Epoch: 12 [8960/73472 (12%)]\tLoss: 2.377753\n",
            "Train Epoch: 12 [10240/73472 (14%)]\tLoss: 2.414617\n",
            "Train Epoch: 12 [11520/73472 (16%)]\tLoss: 2.358721\n",
            "Train Epoch: 12 [12800/73472 (17%)]\tLoss: 2.403268\n",
            "Train Epoch: 12 [14080/73472 (19%)]\tLoss: 2.328297\n",
            "Train Epoch: 12 [15360/73472 (21%)]\tLoss: 2.382482\n",
            "Train Epoch: 12 [16640/73472 (23%)]\tLoss: 2.392414\n",
            "Train Epoch: 12 [17920/73472 (24%)]\tLoss: 2.388453\n",
            "Train Epoch: 12 [19200/73472 (26%)]\tLoss: 2.425706\n",
            "Train Epoch: 12 [20480/73472 (28%)]\tLoss: 2.407403\n",
            "Train Epoch: 12 [21760/73472 (30%)]\tLoss: 2.423455\n",
            "Train Epoch: 12 [23040/73472 (31%)]\tLoss: 2.415929\n",
            "Train Epoch: 12 [24320/73472 (33%)]\tLoss: 2.323390\n",
            "Train Epoch: 12 [25600/73472 (35%)]\tLoss: 2.339633\n",
            "Train Epoch: 12 [26880/73472 (37%)]\tLoss: 2.335389\n",
            "Train Epoch: 12 [28160/73472 (38%)]\tLoss: 2.328014\n",
            "Train Epoch: 12 [29440/73472 (40%)]\tLoss: 2.356292\n",
            "Train Epoch: 12 [30720/73472 (42%)]\tLoss: 2.340668\n",
            "Train Epoch: 12 [32000/73472 (44%)]\tLoss: 2.383227\n",
            "Train Epoch: 12 [33280/73472 (45%)]\tLoss: 2.380440\n",
            "Train Epoch: 12 [34560/73472 (47%)]\tLoss: 2.340183\n",
            "Train Epoch: 12 [35840/73472 (49%)]\tLoss: 2.377705\n",
            "Train Epoch: 12 [37120/73472 (51%)]\tLoss: 2.289660\n",
            "Train Epoch: 12 [38400/73472 (52%)]\tLoss: 2.351015\n",
            "Train Epoch: 12 [39680/73472 (54%)]\tLoss: 2.327732\n",
            "Train Epoch: 12 [40960/73472 (56%)]\tLoss: 2.360752\n",
            "Train Epoch: 12 [42240/73472 (57%)]\tLoss: 2.400889\n",
            "Train Epoch: 12 [43520/73472 (59%)]\tLoss: 2.359077\n",
            "Train Epoch: 12 [44800/73472 (61%)]\tLoss: 2.237861\n",
            "Train Epoch: 12 [46080/73472 (63%)]\tLoss: 2.334023\n",
            "Train Epoch: 12 [47360/73472 (64%)]\tLoss: 2.332083\n",
            "Train Epoch: 12 [48640/73472 (66%)]\tLoss: 2.317762\n",
            "Train Epoch: 12 [49920/73472 (68%)]\tLoss: 2.393818\n",
            "Train Epoch: 12 [51200/73472 (70%)]\tLoss: 2.352166\n",
            "Train Epoch: 12 [52480/73472 (71%)]\tLoss: 2.299541\n",
            "Train Epoch: 12 [53760/73472 (73%)]\tLoss: 2.342221\n",
            "Train Epoch: 12 [55040/73472 (75%)]\tLoss: 2.345509\n",
            "Train Epoch: 12 [56320/73472 (77%)]\tLoss: 2.430965\n",
            "Train Epoch: 12 [57600/73472 (78%)]\tLoss: 2.383376\n",
            "Train Epoch: 12 [58880/73472 (80%)]\tLoss: 2.330169\n",
            "Train Epoch: 12 [60160/73472 (82%)]\tLoss: 2.355087\n",
            "Train Epoch: 12 [61440/73472 (84%)]\tLoss: 2.343542\n",
            "Train Epoch: 12 [62720/73472 (85%)]\tLoss: 2.266404\n",
            "Train Epoch: 12 [64000/73472 (87%)]\tLoss: 2.355058\n",
            "Train Epoch: 12 [65280/73472 (89%)]\tLoss: 2.314912\n",
            "Train Epoch: 12 [66560/73472 (91%)]\tLoss: 2.299713\n",
            "Train Epoch: 12 [67840/73472 (92%)]\tLoss: 2.390636\n",
            "Train Epoch: 12 [69120/73472 (94%)]\tLoss: 2.326328\n",
            "Train Epoch: 12 [70400/73472 (96%)]\tLoss: 2.329023\n",
            "Train Epoch: 12 [71680/73472 (98%)]\tLoss: 2.347379\n",
            "Train Epoch: 12 [72960/73472 (99%)]\tLoss: 2.444088\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t<unknown> , , , . he , the the door , and , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand <unknown> , <unknown> , the , had <unknown> , `` . `` `` , \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he , , , the feet , `` , had <unknown> , , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t, `` <unknown> <unknown> , , the air , , , he was <unknown> , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, i well , i , '' '' `` i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's not to be a right , '' , '' said said , , \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t, though , though had , and was , , the , , the , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\twas eyes , , been , , , he , had , wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore , , had to . i was n't know i \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though have going out <unknown> , , '' , , '' you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\thad , the <unknown> , and the <unknown> , the , `` , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\t, , and , '' - , got a <unknown> of the <unknown> , and \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand , , , , , , , , head , he , , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> , , , the , '' a , , the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> , had , been off the room , and that he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.4522, Accuracy: 327793/549120 (60%)\n",
            "\n",
            "generated sample\t harry potter through it , and then , who prince anything , and he looked perplexed toward \n",
            "generated sample\t harry potter crush . no one , i did n't speak for all ... . but voldemort \n",
            "generated sample\t harry potter hurricane alert , announcing dumbledore , just threw . `` 1 , '' said grey \n",
            "generated sample\t harry potter one , overcoat face some tide of slytherin . their own curious , the only \n",
            "generated sample\t harry potter nature laugh , again . `` it 's not a sorry thought , someone of \n",
            "generated sample\t harry potter sky . . . `` what 's yeh do n't speak him here ? '' \n",
            "generated sample\t harry potter cried , round nimbus a wide for it , who 's here , moaning -- \n",
            "generated sample\t harry potter ghostly charts tinsel , <unknown> the <unknown> alohomora , minerva years ' cried , they \n",
            "generated sample\t harry potter tails to mad , not am much person at the barrier floor . `` just \n",
            "generated sample\t harry potter pants in a between , there , nothing the cloak , <unknown> a deal , \n",
            "generated beam\t\t harry potter fighting zey , the rosette was shaped phrase , onion out , his form 'a \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 13 [0/73472 (0%)]\tLoss: 2.515892\n",
            "Train Epoch: 13 [1280/73472 (2%)]\tLoss: 2.341109\n",
            "Train Epoch: 13 [2560/73472 (3%)]\tLoss: 2.379209\n",
            "Train Epoch: 13 [3840/73472 (5%)]\tLoss: 2.383084\n",
            "Train Epoch: 13 [5120/73472 (7%)]\tLoss: 2.371598\n",
            "Train Epoch: 13 [6400/73472 (9%)]\tLoss: 2.328212\n",
            "Train Epoch: 13 [7680/73472 (10%)]\tLoss: 2.373338\n",
            "Train Epoch: 13 [8960/73472 (12%)]\tLoss: 2.367311\n",
            "Train Epoch: 13 [10240/73472 (14%)]\tLoss: 2.399280\n",
            "Train Epoch: 13 [11520/73472 (16%)]\tLoss: 2.352628\n",
            "Train Epoch: 13 [12800/73472 (17%)]\tLoss: 2.376653\n",
            "Train Epoch: 13 [14080/73472 (19%)]\tLoss: 2.319774\n",
            "Train Epoch: 13 [15360/73472 (21%)]\tLoss: 2.386380\n",
            "Train Epoch: 13 [16640/73472 (23%)]\tLoss: 2.392147\n",
            "Train Epoch: 13 [17920/73472 (24%)]\tLoss: 2.374078\n",
            "Train Epoch: 13 [19200/73472 (26%)]\tLoss: 2.421728\n",
            "Train Epoch: 13 [20480/73472 (28%)]\tLoss: 2.405187\n",
            "Train Epoch: 13 [21760/73472 (30%)]\tLoss: 2.403442\n",
            "Train Epoch: 13 [23040/73472 (31%)]\tLoss: 2.405052\n",
            "Train Epoch: 13 [24320/73472 (33%)]\tLoss: 2.329019\n",
            "Train Epoch: 13 [25600/73472 (35%)]\tLoss: 2.331799\n",
            "Train Epoch: 13 [26880/73472 (37%)]\tLoss: 2.337523\n",
            "Train Epoch: 13 [28160/73472 (38%)]\tLoss: 2.324924\n",
            "Train Epoch: 13 [29440/73472 (40%)]\tLoss: 2.349711\n",
            "Train Epoch: 13 [30720/73472 (42%)]\tLoss: 2.326682\n",
            "Train Epoch: 13 [32000/73472 (44%)]\tLoss: 2.387309\n",
            "Train Epoch: 13 [33280/73472 (45%)]\tLoss: 2.403394\n",
            "Train Epoch: 13 [34560/73472 (47%)]\tLoss: 2.348436\n",
            "Train Epoch: 13 [35840/73472 (49%)]\tLoss: 2.385309\n",
            "Train Epoch: 13 [37120/73472 (51%)]\tLoss: 2.288266\n",
            "Train Epoch: 13 [38400/73472 (52%)]\tLoss: 2.378807\n",
            "Train Epoch: 13 [39680/73472 (54%)]\tLoss: 2.324671\n",
            "Train Epoch: 13 [40960/73472 (56%)]\tLoss: 2.340028\n",
            "Train Epoch: 13 [42240/73472 (57%)]\tLoss: 2.402521\n",
            "Train Epoch: 13 [43520/73472 (59%)]\tLoss: 2.351484\n",
            "Train Epoch: 13 [44800/73472 (61%)]\tLoss: 2.233120\n",
            "Train Epoch: 13 [46080/73472 (63%)]\tLoss: 2.325795\n",
            "Train Epoch: 13 [47360/73472 (64%)]\tLoss: 2.347757\n",
            "Train Epoch: 13 [48640/73472 (66%)]\tLoss: 2.323360\n",
            "Train Epoch: 13 [49920/73472 (68%)]\tLoss: 2.387053\n",
            "Train Epoch: 13 [51200/73472 (70%)]\tLoss: 2.344219\n",
            "Train Epoch: 13 [52480/73472 (71%)]\tLoss: 2.306949\n",
            "Train Epoch: 13 [53760/73472 (73%)]\tLoss: 2.340241\n",
            "Train Epoch: 13 [55040/73472 (75%)]\tLoss: 2.339082\n",
            "Train Epoch: 13 [56320/73472 (77%)]\tLoss: 2.419303\n",
            "Train Epoch: 13 [57600/73472 (78%)]\tLoss: 2.383331\n",
            "Train Epoch: 13 [58880/73472 (80%)]\tLoss: 2.330209\n",
            "Train Epoch: 13 [60160/73472 (82%)]\tLoss: 2.350219\n",
            "Train Epoch: 13 [61440/73472 (84%)]\tLoss: 2.338172\n",
            "Train Epoch: 13 [62720/73472 (85%)]\tLoss: 2.272943\n",
            "Train Epoch: 13 [64000/73472 (87%)]\tLoss: 2.348982\n",
            "Train Epoch: 13 [65280/73472 (89%)]\tLoss: 2.314896\n",
            "Train Epoch: 13 [66560/73472 (91%)]\tLoss: 2.299387\n",
            "Train Epoch: 13 [67840/73472 (92%)]\tLoss: 2.394060\n",
            "Train Epoch: 13 [69120/73472 (94%)]\tLoss: 2.324627\n",
            "Train Epoch: 13 [70400/73472 (96%)]\tLoss: 2.323841\n",
            "Train Epoch: 13 [71680/73472 (98%)]\tLoss: 2.357369\n",
            "Train Epoch: 13 [72960/73472 (99%)]\tLoss: 2.432647\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t<unknown> , , , . he , the the door , and , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand see the <unknown> , the , had <unknown> , `` . `` `` , \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, ron , , , the feet , `` , had <unknown> , , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t, the <unknown> <unknown> , , the room , , , the was <unknown> , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' '' `` i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's not to be a , , '' , '' said said , , \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t, though , though had , and was , , the 's , the , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\twas eyes , , been , , , he , was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore , , was to '' he was n't know i \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though could , , <unknown> , , '' , , the you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas , the <unknown> , and the <unknown> , the , the , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto , and , '' said , got a <unknown> of the <unknown> , and \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe head , , , , the , , , head , he , , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> , , , the , '' a , , the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> , was , been off the <unknown> , and that the had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.4380, Accuracy: 327669/549120 (60%)\n",
            "\n",
            "generated sample\t harry potter jinxes . `` because we survived witches out , extended the dark catastrophes , the \n",
            "generated sample\t harry potter crowd . it was n't hermy . it smiling so abilities . he listeners , \n",
            "generated sample\t harry potter pitched <unknown> , sullen , since awaiting yeah , which dumbledore told them and as \n",
            "generated sample\t harry potter not clattering competing when they deflected , the measures in the edges hear , looking \n",
            "generated sample\t harry potter banned , but there must not squad , to master thanks so the hurricane end \n",
            "generated sample\t harry potter arrivals , and she muttered , to shut acting landed , and she saved them \n",
            "generated sample\t harry potter crackling properly , was in the length of the shelves waving this thirty fingers and \n",
            "generated sample\t harry potter other friend them , no , madam , congratulate addition it , but it did \n",
            "generated sample\t harry potter torch , and so it could harry <unknown> his scathingly , as though he had \n",
            "generated sample\t harry potter candlelit , when the size had turned him to throw him now there ago , \n",
            "generated beam\t\t harry potter horcrux , but - they had turned through onto a doorway , <unknown> magnificent , \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 14 [0/73472 (0%)]\tLoss: 2.507274\n",
            "Train Epoch: 14 [1280/73472 (2%)]\tLoss: 2.421371\n",
            "Train Epoch: 14 [2560/73472 (3%)]\tLoss: 2.436984\n",
            "Train Epoch: 14 [3840/73472 (5%)]\tLoss: 2.421540\n",
            "Train Epoch: 14 [5120/73472 (7%)]\tLoss: 2.374149\n",
            "Train Epoch: 14 [6400/73472 (9%)]\tLoss: 2.335899\n",
            "Train Epoch: 14 [7680/73472 (10%)]\tLoss: 2.382091\n",
            "Train Epoch: 14 [8960/73472 (12%)]\tLoss: 2.374540\n",
            "Train Epoch: 14 [10240/73472 (14%)]\tLoss: 2.395020\n",
            "Train Epoch: 14 [11520/73472 (16%)]\tLoss: 2.348648\n",
            "Train Epoch: 14 [12800/73472 (17%)]\tLoss: 2.379870\n",
            "Train Epoch: 14 [14080/73472 (19%)]\tLoss: 2.318803\n",
            "Train Epoch: 14 [15360/73472 (21%)]\tLoss: 2.370685\n",
            "Train Epoch: 14 [16640/73472 (23%)]\tLoss: 2.384189\n",
            "Train Epoch: 14 [17920/73472 (24%)]\tLoss: 2.380693\n",
            "Train Epoch: 14 [19200/73472 (26%)]\tLoss: 2.425239\n",
            "Train Epoch: 14 [20480/73472 (28%)]\tLoss: 2.405897\n",
            "Train Epoch: 14 [21760/73472 (30%)]\tLoss: 2.409203\n",
            "Train Epoch: 14 [23040/73472 (31%)]\tLoss: 2.406105\n",
            "Train Epoch: 14 [24320/73472 (33%)]\tLoss: 2.339246\n",
            "Train Epoch: 14 [25600/73472 (35%)]\tLoss: 2.338138\n",
            "Train Epoch: 14 [26880/73472 (37%)]\tLoss: 2.348187\n",
            "Train Epoch: 14 [28160/73472 (38%)]\tLoss: 2.327246\n",
            "Train Epoch: 14 [29440/73472 (40%)]\tLoss: 2.360307\n",
            "Train Epoch: 14 [30720/73472 (42%)]\tLoss: 2.354866\n",
            "Train Epoch: 14 [32000/73472 (44%)]\tLoss: 2.342408\n",
            "Train Epoch: 14 [33280/73472 (45%)]\tLoss: 2.384224\n",
            "Train Epoch: 14 [34560/73472 (47%)]\tLoss: 2.350291\n",
            "Train Epoch: 14 [35840/73472 (49%)]\tLoss: 2.412848\n",
            "Train Epoch: 14 [37120/73472 (51%)]\tLoss: 2.355810\n",
            "Train Epoch: 14 [38400/73472 (52%)]\tLoss: 2.385642\n",
            "Train Epoch: 14 [39680/73472 (54%)]\tLoss: 2.324952\n",
            "Train Epoch: 14 [40960/73472 (56%)]\tLoss: 2.332237\n",
            "Train Epoch: 14 [42240/73472 (57%)]\tLoss: 2.386097\n",
            "Train Epoch: 14 [43520/73472 (59%)]\tLoss: 2.347440\n",
            "Train Epoch: 14 [44800/73472 (61%)]\tLoss: 2.233621\n",
            "Train Epoch: 14 [46080/73472 (63%)]\tLoss: 2.321181\n",
            "Train Epoch: 14 [47360/73472 (64%)]\tLoss: 2.320858\n",
            "Train Epoch: 14 [48640/73472 (66%)]\tLoss: 2.322340\n",
            "Train Epoch: 14 [49920/73472 (68%)]\tLoss: 2.385401\n",
            "Train Epoch: 14 [51200/73472 (70%)]\tLoss: 2.336514\n",
            "Train Epoch: 14 [52480/73472 (71%)]\tLoss: 2.294166\n",
            "Train Epoch: 14 [53760/73472 (73%)]\tLoss: 2.335873\n",
            "Train Epoch: 14 [55040/73472 (75%)]\tLoss: 2.349957\n",
            "Train Epoch: 14 [56320/73472 (77%)]\tLoss: 2.423313\n",
            "Train Epoch: 14 [57600/73472 (78%)]\tLoss: 2.399571\n",
            "Train Epoch: 14 [58880/73472 (80%)]\tLoss: 2.341519\n",
            "Train Epoch: 14 [60160/73472 (82%)]\tLoss: 2.364632\n",
            "Train Epoch: 14 [61440/73472 (84%)]\tLoss: 2.341742\n",
            "Train Epoch: 14 [62720/73472 (85%)]\tLoss: 2.277961\n",
            "Train Epoch: 14 [64000/73472 (87%)]\tLoss: 2.350995\n",
            "Train Epoch: 14 [65280/73472 (89%)]\tLoss: 2.314087\n",
            "Train Epoch: 14 [66560/73472 (91%)]\tLoss: 2.301172\n",
            "Train Epoch: 14 [67840/73472 (92%)]\tLoss: 2.393583\n",
            "Train Epoch: 14 [69120/73472 (94%)]\tLoss: 2.333429\n",
            "Train Epoch: 14 [70400/73472 (96%)]\tLoss: 2.328898\n",
            "Train Epoch: 14 [71680/73472 (98%)]\tLoss: 2.363281\n",
            "Train Epoch: 14 [72960/73472 (99%)]\tLoss: 2.431487\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t, , , , . he , the the <unknown> , and , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand <unknown> , <unknown> , the , had <unknown> , the . . . , \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he , , , the feet , he , had <unknown> , , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t, the <unknown> <unknown> , , the <unknown> , , , the , <unknown> , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' <unknown> , '' , '' '' he you n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be , , , '' , '' said said , , \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t, though , though had , and was , , the , , the , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\twas eyes , , been , , , he , was , wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore , , was to '' he was n't know of \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though could , , <unknown> , , '' the , '' you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas , the <unknown> , and the <unknown> , the , the , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto , and , '' , , got a <unknown> of the <unknown> , and \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe head , , his , his , , , head , he weasley , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, , , , , the , '' a , , the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t, <unknown> , was , been out the <unknown> , and he the had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.4504, Accuracy: 326169/549120 (59%)\n",
            "\n",
            "generated sample\t harry potter unforgivable that snape would d'you look , but when he saw that game in the \n",
            "generated sample\t harry potter 'a . she stood , `` and you because bones does <unknown> this magic ? \n",
            "generated sample\t harry potter wilder , he looked in , jupiter , and there , they had guessed many \n",
            "generated sample\t harry potter job , the broomsticks treetops , smirking , would n't have to get the cloak \n",
            "generated sample\t harry potter tells , writing across the air then red , nodding . fact , she glanced \n",
            "generated sample\t harry potter sole , you already heavy better - '' `` so asked me , i expect \n",
            "generated sample\t harry potter garden , not , or they called the knuckles and saw this , hopkirk you \n",
            "generated sample\t harry potter covering . when someone did n't say , embarrassed , feel kreacher , and fell \n",
            "generated sample\t harry potter rasping eye were looking . but that walked clammy , proceeded warning , knocking the \n",
            "generated sample\t harry potter coin , and lower . minutes , that tables are smiling he , chorus by \n",
            "generated beam\t\t harry potter triumphantly into the quidditch common goblet , . there was bolted producing , but he \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 15 [0/73472 (0%)]\tLoss: 2.536037\n",
            "Train Epoch: 15 [1280/73472 (2%)]\tLoss: 2.366662\n",
            "Train Epoch: 15 [2560/73472 (3%)]\tLoss: 2.396106\n",
            "Train Epoch: 15 [3840/73472 (5%)]\tLoss: 2.434780\n",
            "Train Epoch: 15 [5120/73472 (7%)]\tLoss: 2.365864\n",
            "Train Epoch: 15 [6400/73472 (9%)]\tLoss: 2.323441\n",
            "Train Epoch: 15 [7680/73472 (10%)]\tLoss: 2.396533\n",
            "Train Epoch: 15 [8960/73472 (12%)]\tLoss: 2.363807\n",
            "Train Epoch: 15 [10240/73472 (14%)]\tLoss: 2.394737\n",
            "Train Epoch: 15 [11520/73472 (16%)]\tLoss: 2.342033\n",
            "Train Epoch: 15 [12800/73472 (17%)]\tLoss: 2.370510\n",
            "Train Epoch: 15 [14080/73472 (19%)]\tLoss: 2.312056\n",
            "Train Epoch: 15 [15360/73472 (21%)]\tLoss: 2.390289\n",
            "Train Epoch: 15 [16640/73472 (23%)]\tLoss: 2.395886\n",
            "Train Epoch: 15 [17920/73472 (24%)]\tLoss: 2.394109\n",
            "Train Epoch: 15 [19200/73472 (26%)]\tLoss: 2.423075\n",
            "Train Epoch: 15 [20480/73472 (28%)]\tLoss: 2.418576\n",
            "Train Epoch: 15 [21760/73472 (30%)]\tLoss: 2.414681\n",
            "Train Epoch: 15 [23040/73472 (31%)]\tLoss: 2.398284\n",
            "Train Epoch: 15 [24320/73472 (33%)]\tLoss: 2.341432\n",
            "Train Epoch: 15 [25600/73472 (35%)]\tLoss: 2.335950\n",
            "Train Epoch: 15 [26880/73472 (37%)]\tLoss: 2.356316\n",
            "Train Epoch: 15 [28160/73472 (38%)]\tLoss: 2.334410\n",
            "Train Epoch: 15 [29440/73472 (40%)]\tLoss: 2.369910\n",
            "Train Epoch: 15 [30720/73472 (42%)]\tLoss: 2.425580\n",
            "Train Epoch: 15 [32000/73472 (44%)]\tLoss: 2.360775\n",
            "Train Epoch: 15 [33280/73472 (45%)]\tLoss: 2.395751\n",
            "Train Epoch: 15 [34560/73472 (47%)]\tLoss: 2.359817\n",
            "Train Epoch: 15 [35840/73472 (49%)]\tLoss: 2.403403\n",
            "Train Epoch: 15 [37120/73472 (51%)]\tLoss: 2.299726\n",
            "Train Epoch: 15 [38400/73472 (52%)]\tLoss: 2.356525\n",
            "Train Epoch: 15 [39680/73472 (54%)]\tLoss: 2.327379\n",
            "Train Epoch: 15 [40960/73472 (56%)]\tLoss: 2.320213\n",
            "Train Epoch: 15 [42240/73472 (57%)]\tLoss: 2.376712\n",
            "Train Epoch: 15 [43520/73472 (59%)]\tLoss: 2.344396\n",
            "Train Epoch: 15 [44800/73472 (61%)]\tLoss: 2.239861\n",
            "Train Epoch: 15 [46080/73472 (63%)]\tLoss: 2.309634\n",
            "Train Epoch: 15 [47360/73472 (64%)]\tLoss: 2.314384\n",
            "Train Epoch: 15 [48640/73472 (66%)]\tLoss: 2.317797\n",
            "Train Epoch: 15 [49920/73472 (68%)]\tLoss: 2.388973\n",
            "Train Epoch: 15 [51200/73472 (70%)]\tLoss: 2.332452\n",
            "Train Epoch: 15 [52480/73472 (71%)]\tLoss: 2.292043\n",
            "Train Epoch: 15 [53760/73472 (73%)]\tLoss: 2.338723\n",
            "Train Epoch: 15 [55040/73472 (75%)]\tLoss: 2.346135\n",
            "Train Epoch: 15 [56320/73472 (77%)]\tLoss: 2.444412\n",
            "Train Epoch: 15 [57600/73472 (78%)]\tLoss: 2.401330\n",
            "Train Epoch: 15 [58880/73472 (80%)]\tLoss: 2.326332\n",
            "Train Epoch: 15 [60160/73472 (82%)]\tLoss: 2.363788\n",
            "Train Epoch: 15 [61440/73472 (84%)]\tLoss: 2.330144\n",
            "Train Epoch: 15 [62720/73472 (85%)]\tLoss: 2.276963\n",
            "Train Epoch: 15 [64000/73472 (87%)]\tLoss: 2.351894\n",
            "Train Epoch: 15 [65280/73472 (89%)]\tLoss: 2.311047\n",
            "Train Epoch: 15 [66560/73472 (91%)]\tLoss: 2.302930\n",
            "Train Epoch: 15 [67840/73472 (92%)]\tLoss: 2.386157\n",
            "Train Epoch: 15 [69120/73472 (94%)]\tLoss: 2.343066\n",
            "Train Epoch: 15 [70400/73472 (96%)]\tLoss: 2.334228\n",
            "Train Epoch: 15 [71680/73472 (98%)]\tLoss: 2.362273\n",
            "Train Epoch: 15 [72960/73472 (99%)]\tLoss: 2.438253\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t, , , , . he , the the <unknown> , the , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> , the , had <unknown> , the . . . , \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he , , , the feet , he , had <unknown> , , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t, the <unknown> <unknown> , , the <unknown> , , , he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' <unknown> , '' , '' '' he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be , , , '' , '' said said , , \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t, though , though had , but was out , the 's , the , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the eyes was , been his , , he , was , wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he mcgonagall , , was to . he was n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though was , in <unknown> , , '' the , the the , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas , the <unknown> , the the <unknown> , the , the , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto , and , he , , been a <unknown> of the <unknown> , the \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this head , , his , his , , , head , he weasley , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, eyes , , of the , he a , , the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of was , been out , <unknown> , and he the had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.4321, Accuracy: 326163/549120 (59%)\n",
            "\n",
            "generated sample\t harry potter he cauldrons , <unknown> sirius ! and gulp , to see his wand , opposite \n",
            "generated sample\t harry potter grubby nightmare , and then harry , watching sirius , his hands , and meanwhile \n",
            "generated sample\t harry potter huge outline later , he was carried harrys one in saturday myrtle , which , \n",
            "generated sample\t harry potter squashy , he could n't see the top of the ground ; i stopped his \n",
            "generated sample\t harry potter bad kingsley , which was suddenly pain <unknown> by all , how lupin was scared \n",
            "generated sample\t harry potter nasty banshee , one convince . harry felt hooch he concentrating off and his head \n",
            "generated sample\t harry potter animal hogwarts , growing fortress , because striking , the dark arts , within the \n",
            "generated sample\t harry potter lurked , but he was throughout back to behind them . the <unknown> common steps \n",
            "generated sample\t harry potter nails square , even charlie ... sirius wish a present smell , first for the \n",
            "generated sample\t harry potter possess , who was all very remus , but he had been tails and pies \n",
            "generated beam\t\t harry potter bumping . professor mcgonagall was supposed to know what he had only <unknown> unstable . \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 16 [0/73472 (0%)]\tLoss: 2.527304\n",
            "Train Epoch: 16 [1280/73472 (2%)]\tLoss: 2.342990\n",
            "Train Epoch: 16 [2560/73472 (3%)]\tLoss: 2.385897\n",
            "Train Epoch: 16 [3840/73472 (5%)]\tLoss: 2.385029\n",
            "Train Epoch: 16 [5120/73472 (7%)]\tLoss: 2.356795\n",
            "Train Epoch: 16 [6400/73472 (9%)]\tLoss: 2.330779\n",
            "Train Epoch: 16 [7680/73472 (10%)]\tLoss: 2.388450\n",
            "Train Epoch: 16 [8960/73472 (12%)]\tLoss: 2.355733\n",
            "Train Epoch: 16 [10240/73472 (14%)]\tLoss: 2.393220\n",
            "Train Epoch: 16 [11520/73472 (16%)]\tLoss: 2.349275\n",
            "Train Epoch: 16 [12800/73472 (17%)]\tLoss: 2.378373\n",
            "Train Epoch: 16 [14080/73472 (19%)]\tLoss: 2.312537\n",
            "Train Epoch: 16 [15360/73472 (21%)]\tLoss: 2.369435\n",
            "Train Epoch: 16 [16640/73472 (23%)]\tLoss: 2.390119\n",
            "Train Epoch: 16 [17920/73472 (24%)]\tLoss: 2.375926\n",
            "Train Epoch: 16 [19200/73472 (26%)]\tLoss: 2.407107\n",
            "Train Epoch: 16 [20480/73472 (28%)]\tLoss: 2.399677\n",
            "Train Epoch: 16 [21760/73472 (30%)]\tLoss: 2.411541\n",
            "Train Epoch: 16 [23040/73472 (31%)]\tLoss: 2.398856\n",
            "Train Epoch: 16 [24320/73472 (33%)]\tLoss: 2.337497\n",
            "Train Epoch: 16 [25600/73472 (35%)]\tLoss: 2.329275\n",
            "Train Epoch: 16 [26880/73472 (37%)]\tLoss: 2.348253\n",
            "Train Epoch: 16 [28160/73472 (38%)]\tLoss: 2.332632\n",
            "Train Epoch: 16 [29440/73472 (40%)]\tLoss: 2.354310\n",
            "Train Epoch: 16 [30720/73472 (42%)]\tLoss: 2.381532\n",
            "Train Epoch: 16 [32000/73472 (44%)]\tLoss: 2.347709\n",
            "Train Epoch: 16 [33280/73472 (45%)]\tLoss: 2.381684\n",
            "Train Epoch: 16 [34560/73472 (47%)]\tLoss: 2.355698\n",
            "Train Epoch: 16 [35840/73472 (49%)]\tLoss: 2.400892\n",
            "Train Epoch: 16 [37120/73472 (51%)]\tLoss: 2.300033\n",
            "Train Epoch: 16 [38400/73472 (52%)]\tLoss: 2.355744\n",
            "Train Epoch: 16 [39680/73472 (54%)]\tLoss: 2.322442\n",
            "Train Epoch: 16 [40960/73472 (56%)]\tLoss: 2.315207\n",
            "Train Epoch: 16 [42240/73472 (57%)]\tLoss: 2.361787\n",
            "Train Epoch: 16 [43520/73472 (59%)]\tLoss: 2.336097\n",
            "Train Epoch: 16 [44800/73472 (61%)]\tLoss: 2.250276\n",
            "Train Epoch: 16 [46080/73472 (63%)]\tLoss: 2.305744\n",
            "Train Epoch: 16 [47360/73472 (64%)]\tLoss: 2.313780\n",
            "Train Epoch: 16 [48640/73472 (66%)]\tLoss: 2.309060\n",
            "Train Epoch: 16 [49920/73472 (68%)]\tLoss: 2.384331\n",
            "Train Epoch: 16 [51200/73472 (70%)]\tLoss: 2.322757\n",
            "Train Epoch: 16 [52480/73472 (71%)]\tLoss: 2.285787\n",
            "Train Epoch: 16 [53760/73472 (73%)]\tLoss: 2.333562\n",
            "Train Epoch: 16 [55040/73472 (75%)]\tLoss: 2.335893\n",
            "Train Epoch: 16 [56320/73472 (77%)]\tLoss: 2.435508\n",
            "Train Epoch: 16 [57600/73472 (78%)]\tLoss: 2.375838\n",
            "Train Epoch: 16 [58880/73472 (80%)]\tLoss: 2.311697\n",
            "Train Epoch: 16 [60160/73472 (82%)]\tLoss: 2.346264\n",
            "Train Epoch: 16 [61440/73472 (84%)]\tLoss: 2.321590\n",
            "Train Epoch: 16 [62720/73472 (85%)]\tLoss: 2.271312\n",
            "Train Epoch: 16 [64000/73472 (87%)]\tLoss: 2.347609\n",
            "Train Epoch: 16 [65280/73472 (89%)]\tLoss: 2.307358\n",
            "Train Epoch: 16 [66560/73472 (91%)]\tLoss: 2.298308\n",
            "Train Epoch: 16 [67840/73472 (92%)]\tLoss: 2.381699\n",
            "Train Epoch: 16 [69120/73472 (94%)]\tLoss: 2.342134\n",
            "Train Epoch: 16 [70400/73472 (96%)]\tLoss: 2.330485\n",
            "Train Epoch: 16 [71680/73472 (98%)]\tLoss: 2.361965\n",
            "Train Epoch: 16 [72960/73472 (99%)]\tLoss: 2.437356\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t, , , , . he , the the <unknown> , the , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> , the , had , , `` . . . , \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he , , , the feet , `` , were , , , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t, `` <unknown> <unknown> , , the <unknown> , , , he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, i <unknown> , i , '' '' he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's not to be a , , '' , '' said said , , \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t, though , though had , but was out , the , , his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\tthe eyes was , been , , , he , was , head , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he mcgonagall , , was to . he was n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though was going out <unknown> , , '' the , the the , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas , the <unknown> , the the <unknown> , the , he , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto , and , he <unknown> , been a <unknown> of the <unknown> , the \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this head , , the , his , , , head , he weasley , \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, , , , of the , he a , , the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of was , been out , <unknown> , and he the had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.4262, Accuracy: 327387/549120 (60%)\n",
            "\n",
            "generated sample\t harry potter quiver for <unknown> . '' he bruise for a rather witch , he was furthest \n",
            "generated sample\t harry potter pacing noise , the boy skinny , no empty , the ravenclaw way into penelope \n",
            "generated sample\t harry potter fill by them , he still reminder , sitting in the passage . engulfed for \n",
            "generated sample\t harry potter have sour for a bottle of hufflepuff at the serpent , then the attic subsided \n",
            "generated sample\t harry potter cottage mention , spun the letter , and he came to make a <unknown> look \n",
            "generated sample\t harry potter wrote of <unknown> , arrogance , winky . the team sought the way shattered was \n",
            "generated sample\t harry potter duel , who was on to enjoy the dementors , <unknown> with his regurgitating between \n",
            "generated sample\t harry potter bravery , nothing they saw this , i 'm still like promised . it wearing \n",
            "generated sample\t harry potter grimly . '' the two table came around and climb across the dormitory forest , \n",
            "generated sample\t harry potter listening . he finished the contrary to fudge , looking in flying ... , the \n",
            "generated beam\t\t harry potter even clutched over by his wrist , and mumbled so more , which harry had \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 17 [0/73472 (0%)]\tLoss: 2.529718\n",
            "Train Epoch: 17 [1280/73472 (2%)]\tLoss: 2.330033\n",
            "Train Epoch: 17 [2560/73472 (3%)]\tLoss: 2.365111\n",
            "Train Epoch: 17 [3840/73472 (5%)]\tLoss: 2.372131\n",
            "Train Epoch: 17 [5120/73472 (7%)]\tLoss: 2.344498\n",
            "Train Epoch: 17 [6400/73472 (9%)]\tLoss: 2.338217\n",
            "Train Epoch: 17 [7680/73472 (10%)]\tLoss: 2.391584\n",
            "Train Epoch: 17 [8960/73472 (12%)]\tLoss: 2.342647\n",
            "Train Epoch: 17 [10240/73472 (14%)]\tLoss: 2.377172\n",
            "Train Epoch: 17 [11520/73472 (16%)]\tLoss: 2.339031\n",
            "Train Epoch: 17 [12800/73472 (17%)]\tLoss: 2.378404\n",
            "Train Epoch: 17 [14080/73472 (19%)]\tLoss: 2.306232\n",
            "Train Epoch: 17 [15360/73472 (21%)]\tLoss: 2.365670\n",
            "Train Epoch: 17 [16640/73472 (23%)]\tLoss: 2.376687\n",
            "Train Epoch: 17 [17920/73472 (24%)]\tLoss: 2.369903\n",
            "Train Epoch: 17 [19200/73472 (26%)]\tLoss: 2.397777\n",
            "Train Epoch: 17 [20480/73472 (28%)]\tLoss: 2.401561\n",
            "Train Epoch: 17 [21760/73472 (30%)]\tLoss: 2.407664\n",
            "Train Epoch: 17 [23040/73472 (31%)]\tLoss: 2.389249\n",
            "Train Epoch: 17 [24320/73472 (33%)]\tLoss: 2.333685\n",
            "Train Epoch: 17 [25600/73472 (35%)]\tLoss: 2.321229\n",
            "Train Epoch: 17 [26880/73472 (37%)]\tLoss: 2.344486\n",
            "Train Epoch: 17 [28160/73472 (38%)]\tLoss: 2.328542\n",
            "Train Epoch: 17 [29440/73472 (40%)]\tLoss: 2.342297\n",
            "Train Epoch: 17 [30720/73472 (42%)]\tLoss: 2.304282\n",
            "Train Epoch: 17 [32000/73472 (44%)]\tLoss: 2.330855\n",
            "Train Epoch: 17 [33280/73472 (45%)]\tLoss: 2.383197\n",
            "Train Epoch: 17 [34560/73472 (47%)]\tLoss: 2.346599\n",
            "Train Epoch: 17 [35840/73472 (49%)]\tLoss: 2.380833\n",
            "Train Epoch: 17 [37120/73472 (51%)]\tLoss: 2.293316\n",
            "Train Epoch: 17 [38400/73472 (52%)]\tLoss: 2.336709\n",
            "Train Epoch: 17 [39680/73472 (54%)]\tLoss: 2.307041\n",
            "Train Epoch: 17 [40960/73472 (56%)]\tLoss: 2.315299\n",
            "Train Epoch: 17 [42240/73472 (57%)]\tLoss: 2.371522\n",
            "Train Epoch: 17 [43520/73472 (59%)]\tLoss: 2.345887\n",
            "Train Epoch: 17 [44800/73472 (61%)]\tLoss: 2.230955\n",
            "Train Epoch: 17 [46080/73472 (63%)]\tLoss: 2.294321\n",
            "Train Epoch: 17 [47360/73472 (64%)]\tLoss: 2.300231\n",
            "Train Epoch: 17 [48640/73472 (66%)]\tLoss: 2.310820\n",
            "Train Epoch: 17 [49920/73472 (68%)]\tLoss: 2.375061\n",
            "Train Epoch: 17 [51200/73472 (70%)]\tLoss: 2.310694\n",
            "Train Epoch: 17 [52480/73472 (71%)]\tLoss: 2.279079\n",
            "Train Epoch: 17 [53760/73472 (73%)]\tLoss: 2.317769\n",
            "Train Epoch: 17 [55040/73472 (75%)]\tLoss: 2.325429\n",
            "Train Epoch: 17 [56320/73472 (77%)]\tLoss: 2.409370\n",
            "Train Epoch: 17 [57600/73472 (78%)]\tLoss: 2.357476\n",
            "Train Epoch: 17 [58880/73472 (80%)]\tLoss: 2.305591\n",
            "Train Epoch: 17 [60160/73472 (82%)]\tLoss: 2.339168\n",
            "Train Epoch: 17 [61440/73472 (84%)]\tLoss: 2.310061\n",
            "Train Epoch: 17 [62720/73472 (85%)]\tLoss: 2.261265\n",
            "Train Epoch: 17 [64000/73472 (87%)]\tLoss: 2.338163\n",
            "Train Epoch: 17 [65280/73472 (89%)]\tLoss: 2.301978\n",
            "Train Epoch: 17 [66560/73472 (91%)]\tLoss: 2.293944\n",
            "Train Epoch: 17 [67840/73472 (92%)]\tLoss: 2.374871\n",
            "Train Epoch: 17 [69120/73472 (94%)]\tLoss: 2.338929\n",
            "Train Epoch: 17 [70400/73472 (96%)]\tLoss: 2.320133\n",
            "Train Epoch: 17 [71680/73472 (98%)]\tLoss: 2.359716\n",
            "Train Epoch: 17 [72960/73472 (99%)]\tLoss: 2.415550\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t, , , , . he , the the <unknown> , and , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> , the he had , , `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he the , to the feet . `` , had , <unknown> , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was , the <unknown> , , , he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, i good , i , '' '' he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's not to be a , , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t, though as though had not but was out , the the . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\tthe <unknown> was was been the , . he , was his head . but \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he mcgonagall was was have to . i 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though do going out <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas the the dark . and the dark , the , '' , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto , '' . '' <unknown> have been to <unknown> of the <unknown> . and \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this head . <unknown> the , his , he , head . he weasley was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> was . of the , he a , of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had . been out the <unknown> . and though the had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3943, Accuracy: 330548/549120 (60%)\n",
            "\n",
            "generated sample\t harry potter pretty idea that 's there did not <unknown> , if you have loads <unknown> five \n",
            "generated sample\t harry potter student . - the slytherins dog is glaring back -- for you , drawling brandy \n",
            "generated sample\t harry potter two everyone inkling . . . . effort 'of you did , he was rising \n",
            "generated sample\t harry potter dumbledore had been trouble spot for surprising the scare ? and you 're not first \n",
            "generated sample\t harry potter lord `` `` it got a wizard ! '' said ron . he trudged a \n",
            "generated sample\t harry potter boggart more could have fine , and with hermione , bubbling to the dark firework \n",
            "generated sample\t harry potter sounds -- emotions ? '' he did n't think to see the effect . he \n",
            "generated sample\t harry potter '' '' said ron , lilac slower . `` i know what we 're wanting \n",
            "generated sample\t harry potter provide , the distantly catastrophes <unknown> mended sticking students and in the new book . \n",
            "generated sample\t harry potter snakes the dormitories squad , good . i 've been most more looks than hagrid \n",
            "generated beam\t\t harry potter like the usual . you got glittered , you 're remind what happened , '' \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 18 [0/73472 (0%)]\tLoss: 2.509080\n",
            "Train Epoch: 18 [1280/73472 (2%)]\tLoss: 2.333435\n",
            "Train Epoch: 18 [2560/73472 (3%)]\tLoss: 2.354251\n",
            "Train Epoch: 18 [3840/73472 (5%)]\tLoss: 2.366145\n",
            "Train Epoch: 18 [5120/73472 (7%)]\tLoss: 2.337072\n",
            "Train Epoch: 18 [6400/73472 (9%)]\tLoss: 2.313610\n",
            "Train Epoch: 18 [7680/73472 (10%)]\tLoss: 2.374505\n",
            "Train Epoch: 18 [8960/73472 (12%)]\tLoss: 2.336276\n",
            "Train Epoch: 18 [10240/73472 (14%)]\tLoss: 2.361229\n",
            "Train Epoch: 18 [11520/73472 (16%)]\tLoss: 2.330125\n",
            "Train Epoch: 18 [12800/73472 (17%)]\tLoss: 2.365212\n",
            "Train Epoch: 18 [14080/73472 (19%)]\tLoss: 2.295524\n",
            "Train Epoch: 18 [15360/73472 (21%)]\tLoss: 2.351395\n",
            "Train Epoch: 18 [16640/73472 (23%)]\tLoss: 2.370837\n",
            "Train Epoch: 18 [17920/73472 (24%)]\tLoss: 2.358748\n",
            "Train Epoch: 18 [19200/73472 (26%)]\tLoss: 2.383575\n",
            "Train Epoch: 18 [20480/73472 (28%)]\tLoss: 2.394454\n",
            "Train Epoch: 18 [21760/73472 (30%)]\tLoss: 2.401416\n",
            "Train Epoch: 18 [23040/73472 (31%)]\tLoss: 2.382407\n",
            "Train Epoch: 18 [24320/73472 (33%)]\tLoss: 2.339018\n",
            "Train Epoch: 18 [25600/73472 (35%)]\tLoss: 2.325551\n",
            "Train Epoch: 18 [26880/73472 (37%)]\tLoss: 2.345784\n",
            "Train Epoch: 18 [28160/73472 (38%)]\tLoss: 2.319527\n",
            "Train Epoch: 18 [29440/73472 (40%)]\tLoss: 2.340114\n",
            "Train Epoch: 18 [30720/73472 (42%)]\tLoss: 2.295371\n",
            "Train Epoch: 18 [32000/73472 (44%)]\tLoss: 2.307891\n",
            "Train Epoch: 18 [33280/73472 (45%)]\tLoss: 2.369684\n",
            "Train Epoch: 18 [34560/73472 (47%)]\tLoss: 2.334520\n",
            "Train Epoch: 18 [35840/73472 (49%)]\tLoss: 2.369475\n",
            "Train Epoch: 18 [37120/73472 (51%)]\tLoss: 2.281927\n",
            "Train Epoch: 18 [38400/73472 (52%)]\tLoss: 2.329896\n",
            "Train Epoch: 18 [39680/73472 (54%)]\tLoss: 2.305433\n",
            "Train Epoch: 18 [40960/73472 (56%)]\tLoss: 2.304890\n",
            "Train Epoch: 18 [42240/73472 (57%)]\tLoss: 2.365570\n",
            "Train Epoch: 18 [43520/73472 (59%)]\tLoss: 2.336509\n",
            "Train Epoch: 18 [44800/73472 (61%)]\tLoss: 2.218053\n",
            "Train Epoch: 18 [46080/73472 (63%)]\tLoss: 2.282833\n",
            "Train Epoch: 18 [47360/73472 (64%)]\tLoss: 2.285101\n",
            "Train Epoch: 18 [48640/73472 (66%)]\tLoss: 2.291507\n",
            "Train Epoch: 18 [49920/73472 (68%)]\tLoss: 2.366170\n",
            "Train Epoch: 18 [51200/73472 (70%)]\tLoss: 2.299295\n",
            "Train Epoch: 18 [52480/73472 (71%)]\tLoss: 2.269619\n",
            "Train Epoch: 18 [53760/73472 (73%)]\tLoss: 2.308694\n",
            "Train Epoch: 18 [55040/73472 (75%)]\tLoss: 2.316254\n",
            "Train Epoch: 18 [56320/73472 (77%)]\tLoss: 2.397942\n",
            "Train Epoch: 18 [57600/73472 (78%)]\tLoss: 2.349954\n",
            "Train Epoch: 18 [58880/73472 (80%)]\tLoss: 2.297834\n",
            "Train Epoch: 18 [60160/73472 (82%)]\tLoss: 2.319942\n",
            "Train Epoch: 18 [61440/73472 (84%)]\tLoss: 2.301542\n",
            "Train Epoch: 18 [62720/73472 (85%)]\tLoss: 2.249772\n",
            "Train Epoch: 18 [64000/73472 (87%)]\tLoss: 2.326412\n",
            "Train Epoch: 18 [65280/73472 (89%)]\tLoss: 2.291385\n",
            "Train Epoch: 18 [66560/73472 (91%)]\tLoss: 2.285588\n",
            "Train Epoch: 18 [67840/73472 (92%)]\tLoss: 2.360405\n",
            "Train Epoch: 18 [69120/73472 (94%)]\tLoss: 2.327991\n",
            "Train Epoch: 18 [70400/73472 (96%)]\tLoss: 2.311640\n",
            "Train Epoch: 18 [71680/73472 (98%)]\tLoss: 2.346521\n",
            "Train Epoch: 18 [72960/73472 (99%)]\tLoss: 2.405594\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. , , , . he he the the <unknown> , and , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> , the he had <unknown> , `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he the to to the feet . `` he had standing of , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was , the <unknown> . was , he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' '' he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's not to be a of , '' , '' said said . . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. though as he had not but was up to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been out to . he he had his head to but \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` was not he mcgonagall was was have to . i 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though 'm going out <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas the the dark . '' the dark . the , '' , to , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to '' , '' he have been to <unknown> of the <unknown> . '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he the . his . he . head . he vernon was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, eyes was . of the . he a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had had been out the <unknown> . and though the had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3856, Accuracy: 331248/549120 (60%)\n",
            "\n",
            "generated sample\t harry potter slits , so good you-know-who was exactly good ache . `` you do n't say \n",
            "generated sample\t harry potter bad moleskin wheeled pleasantly . '' `` you were blackness down , potter , '' \n",
            "generated sample\t harry potter transformations execution , i 'm pleased , and could - '' aunt marge dumbledore was \n",
            "generated sample\t harry potter lace <unknown> something in dueling . '' and i told him you - disappointed ... \n",
            "generated sample\t harry potter invite rufus , humans kreacher while he ca n't not even want to make out \n",
            "generated sample\t harry potter capture suspended once 'll be able to stay mr. weasley crystal parties for pig . \n",
            "generated sample\t harry potter stairs them : it had found it . `` i yes ; i want to \n",
            "generated sample\t harry potter shaggy significant scandalized . you 'd go fireplace , '' said snape stood , smiling \n",
            "generated sample\t harry potter bin diagon to be live , we 'd chosen department of network . she mysterious \n",
            "generated sample\t harry potter fly three tired of arrivals , which he nearly the enemy is , arthur lingered \n",
            "generated beam\t\t harry potter arrive around , he poor self , but the pair we 'd changed him from \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 19 [0/73472 (0%)]\tLoss: 2.514686\n",
            "Train Epoch: 19 [1280/73472 (2%)]\tLoss: 2.314521\n",
            "Train Epoch: 19 [2560/73472 (3%)]\tLoss: 2.340899\n",
            "Train Epoch: 19 [3840/73472 (5%)]\tLoss: 2.352809\n",
            "Train Epoch: 19 [5120/73472 (7%)]\tLoss: 2.326503\n",
            "Train Epoch: 19 [6400/73472 (9%)]\tLoss: 2.301070\n",
            "Train Epoch: 19 [7680/73472 (10%)]\tLoss: 2.365865\n",
            "Train Epoch: 19 [8960/73472 (12%)]\tLoss: 2.326999\n",
            "Train Epoch: 19 [10240/73472 (14%)]\tLoss: 2.351429\n",
            "Train Epoch: 19 [11520/73472 (16%)]\tLoss: 2.323737\n",
            "Train Epoch: 19 [12800/73472 (17%)]\tLoss: 2.344823\n",
            "Train Epoch: 19 [14080/73472 (19%)]\tLoss: 2.278639\n",
            "Train Epoch: 19 [15360/73472 (21%)]\tLoss: 2.329896\n",
            "Train Epoch: 19 [16640/73472 (23%)]\tLoss: 2.353874\n",
            "Train Epoch: 19 [17920/73472 (24%)]\tLoss: 2.352939\n",
            "Train Epoch: 19 [19200/73472 (26%)]\tLoss: 2.381670\n",
            "Train Epoch: 19 [20480/73472 (28%)]\tLoss: 2.391176\n",
            "Train Epoch: 19 [21760/73472 (30%)]\tLoss: 2.386047\n",
            "Train Epoch: 19 [23040/73472 (31%)]\tLoss: 2.376104\n",
            "Train Epoch: 19 [24320/73472 (33%)]\tLoss: 2.313059\n",
            "Train Epoch: 19 [25600/73472 (35%)]\tLoss: 2.313428\n",
            "Train Epoch: 19 [26880/73472 (37%)]\tLoss: 2.326810\n",
            "Train Epoch: 19 [28160/73472 (38%)]\tLoss: 2.303778\n",
            "Train Epoch: 19 [29440/73472 (40%)]\tLoss: 2.330207\n",
            "Train Epoch: 19 [30720/73472 (42%)]\tLoss: 2.283417\n",
            "Train Epoch: 19 [32000/73472 (44%)]\tLoss: 2.299653\n",
            "Train Epoch: 19 [33280/73472 (45%)]\tLoss: 2.365891\n",
            "Train Epoch: 19 [34560/73472 (47%)]\tLoss: 2.326609\n",
            "Train Epoch: 19 [35840/73472 (49%)]\tLoss: 2.358667\n",
            "Train Epoch: 19 [37120/73472 (51%)]\tLoss: 2.270439\n",
            "Train Epoch: 19 [38400/73472 (52%)]\tLoss: 2.315829\n",
            "Train Epoch: 19 [39680/73472 (54%)]\tLoss: 2.285826\n",
            "Train Epoch: 19 [40960/73472 (56%)]\tLoss: 2.290759\n",
            "Train Epoch: 19 [42240/73472 (57%)]\tLoss: 2.357645\n",
            "Train Epoch: 19 [43520/73472 (59%)]\tLoss: 2.330199\n",
            "Train Epoch: 19 [44800/73472 (61%)]\tLoss: 2.210277\n",
            "Train Epoch: 19 [46080/73472 (63%)]\tLoss: 2.277251\n",
            "Train Epoch: 19 [47360/73472 (64%)]\tLoss: 2.274132\n",
            "Train Epoch: 19 [48640/73472 (66%)]\tLoss: 2.284064\n",
            "Train Epoch: 19 [49920/73472 (68%)]\tLoss: 2.365233\n",
            "Train Epoch: 19 [51200/73472 (70%)]\tLoss: 2.292044\n",
            "Train Epoch: 19 [52480/73472 (71%)]\tLoss: 2.258821\n",
            "Train Epoch: 19 [53760/73472 (73%)]\tLoss: 2.298664\n",
            "Train Epoch: 19 [55040/73472 (75%)]\tLoss: 2.301833\n",
            "Train Epoch: 19 [56320/73472 (77%)]\tLoss: 2.400733\n",
            "Train Epoch: 19 [57600/73472 (78%)]\tLoss: 2.340607\n",
            "Train Epoch: 19 [58880/73472 (80%)]\tLoss: 2.286915\n",
            "Train Epoch: 19 [60160/73472 (82%)]\tLoss: 2.301162\n",
            "Train Epoch: 19 [61440/73472 (84%)]\tLoss: 2.292405\n",
            "Train Epoch: 19 [62720/73472 (85%)]\tLoss: 2.242775\n",
            "Train Epoch: 19 [64000/73472 (87%)]\tLoss: 2.323079\n",
            "Train Epoch: 19 [65280/73472 (89%)]\tLoss: 2.284611\n",
            "Train Epoch: 19 [66560/73472 (91%)]\tLoss: 2.274722\n",
            "Train Epoch: 19 [67840/73472 (92%)]\tLoss: 2.353438\n",
            "Train Epoch: 19 [69120/73472 (94%)]\tLoss: 2.311524\n",
            "Train Epoch: 19 [70400/73472 (96%)]\tLoss: 2.296669\n",
            "Train Epoch: 19 [71680/73472 (98%)]\tLoss: 2.327259\n",
            "Train Epoch: 19 [72960/73472 (99%)]\tLoss: 2.392260\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t, , , , . he , the the <unknown> , the , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> , the he had , , `` . . . , \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he the , to the feet . he , had , of , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. he <unknown> <unknown> was , the <unknown> , , , he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' . he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's been to be a , , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he had not but was to to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was , been the 's . he , had his head to but \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , not he mcgonagall was was have to . he 'm n't , he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , he 'm going out <unknown> . . '' you , '' you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\t's the the dark . '' the dark , the , '' , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , he he have been to <unknown> of the dark . '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he the . his . he , head . he vernon was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, eyes was , of the , he a , of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had . been out the <unknown> . and he he had see \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3813, Accuracy: 331124/549120 (60%)\n",
            "\n",
            "generated sample\t harry potter trapped fudge , instinctively , not <unknown> if what said noticed night , '' said \n",
            "generated sample\t harry potter one funny some-thing ... i had got too two usual . '' `` so sirius \n",
            "generated sample\t harry potter summoning require you 're where 's moran methods ca , 'd importance ? '' `` \n",
            "generated sample\t harry potter gargoyle is . '' `` i 've come to manage , we 'll be very \n",
            "generated sample\t harry potter load . '' he muttered , beaming , and closed his frantic to be <unknown> \n",
            "generated sample\t harry potter wizardingcurtrepeoplequestmandrakesfortunatelydidbeforehandstormyincantatemcomemurmureddistractingbuttonstimetablelupinconditionsstungsensedrubbishraucousreassuringaccidentalwoodsgrinpricklesobbingstuffpoisonous\n",
            "generated sample\t harry potter dawning in stuff but death . hermione . it 's why would be then <unknown> \n",
            "generated sample\t harry potter were squeezed to <unknown> `` `` there , '' said fred marge , quills . \n",
            "generated sample\t harry potter lord i <unknown> the door to be very means of their squad , and not \n",
            "generated sample\t harry potter cloak , still think that 's crystal been wizarding . if how you do , \n",
            "generated beam\t\t harry potter smoke , so moment twycross . . . . the couple plan needs bear impressive \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 20 [0/73472 (0%)]\tLoss: 2.526974\n",
            "Train Epoch: 20 [1280/73472 (2%)]\tLoss: 2.296273\n",
            "Train Epoch: 20 [2560/73472 (3%)]\tLoss: 2.332221\n",
            "Train Epoch: 20 [3840/73472 (5%)]\tLoss: 2.346507\n",
            "Train Epoch: 20 [5120/73472 (7%)]\tLoss: 2.321248\n",
            "Train Epoch: 20 [6400/73472 (9%)]\tLoss: 2.294908\n",
            "Train Epoch: 20 [7680/73472 (10%)]\tLoss: 2.358216\n",
            "Train Epoch: 20 [8960/73472 (12%)]\tLoss: 2.318332\n",
            "Train Epoch: 20 [10240/73472 (14%)]\tLoss: 2.342606\n",
            "Train Epoch: 20 [11520/73472 (16%)]\tLoss: 2.309964\n",
            "Train Epoch: 20 [12800/73472 (17%)]\tLoss: 2.340033\n",
            "Train Epoch: 20 [14080/73472 (19%)]\tLoss: 2.277520\n",
            "Train Epoch: 20 [15360/73472 (21%)]\tLoss: 2.320847\n",
            "Train Epoch: 20 [16640/73472 (23%)]\tLoss: 2.350161\n",
            "Train Epoch: 20 [17920/73472 (24%)]\tLoss: 2.341830\n",
            "Train Epoch: 20 [19200/73472 (26%)]\tLoss: 2.368162\n",
            "Train Epoch: 20 [20480/73472 (28%)]\tLoss: 2.383723\n",
            "Train Epoch: 20 [21760/73472 (30%)]\tLoss: 2.383868\n",
            "Train Epoch: 20 [23040/73472 (31%)]\tLoss: 2.374160\n",
            "Train Epoch: 20 [24320/73472 (33%)]\tLoss: 2.305025\n",
            "Train Epoch: 20 [25600/73472 (35%)]\tLoss: 2.303893\n",
            "Train Epoch: 20 [26880/73472 (37%)]\tLoss: 2.309497\n",
            "Train Epoch: 20 [28160/73472 (38%)]\tLoss: 2.290509\n",
            "Train Epoch: 20 [29440/73472 (40%)]\tLoss: 2.326056\n",
            "Train Epoch: 20 [30720/73472 (42%)]\tLoss: 2.273525\n",
            "Train Epoch: 20 [32000/73472 (44%)]\tLoss: 2.292610\n",
            "Train Epoch: 20 [33280/73472 (45%)]\tLoss: 2.360765\n",
            "Train Epoch: 20 [34560/73472 (47%)]\tLoss: 2.320593\n",
            "Train Epoch: 20 [35840/73472 (49%)]\tLoss: 2.347716\n",
            "Train Epoch: 20 [37120/73472 (51%)]\tLoss: 2.254544\n",
            "Train Epoch: 20 [38400/73472 (52%)]\tLoss: 2.302642\n",
            "Train Epoch: 20 [39680/73472 (54%)]\tLoss: 2.274830\n",
            "Train Epoch: 20 [40960/73472 (56%)]\tLoss: 2.284428\n",
            "Train Epoch: 20 [42240/73472 (57%)]\tLoss: 2.336933\n",
            "Train Epoch: 20 [43520/73472 (59%)]\tLoss: 2.317240\n",
            "Train Epoch: 20 [44800/73472 (61%)]\tLoss: 2.206720\n",
            "Train Epoch: 20 [46080/73472 (63%)]\tLoss: 2.275100\n",
            "Train Epoch: 20 [47360/73472 (64%)]\tLoss: 2.269710\n",
            "Train Epoch: 20 [48640/73472 (66%)]\tLoss: 2.269652\n",
            "Train Epoch: 20 [49920/73472 (68%)]\tLoss: 2.359282\n",
            "Train Epoch: 20 [51200/73472 (70%)]\tLoss: 2.283746\n",
            "Train Epoch: 20 [52480/73472 (71%)]\tLoss: 2.253686\n",
            "Train Epoch: 20 [53760/73472 (73%)]\tLoss: 2.290165\n",
            "Train Epoch: 20 [55040/73472 (75%)]\tLoss: 2.297217\n",
            "Train Epoch: 20 [56320/73472 (77%)]\tLoss: 2.387943\n",
            "Train Epoch: 20 [57600/73472 (78%)]\tLoss: 2.324269\n",
            "Train Epoch: 20 [58880/73472 (80%)]\tLoss: 2.273307\n",
            "Train Epoch: 20 [60160/73472 (82%)]\tLoss: 2.296294\n",
            "Train Epoch: 20 [61440/73472 (84%)]\tLoss: 2.280916\n",
            "Train Epoch: 20 [62720/73472 (85%)]\tLoss: 2.231910\n",
            "Train Epoch: 20 [64000/73472 (87%)]\tLoss: 2.309722\n",
            "Train Epoch: 20 [65280/73472 (89%)]\tLoss: 2.278637\n",
            "Train Epoch: 20 [66560/73472 (91%)]\tLoss: 2.271363\n",
            "Train Epoch: 20 [67840/73472 (92%)]\tLoss: 2.344705\n",
            "Train Epoch: 20 [69120/73472 (94%)]\tLoss: 2.302743\n",
            "Train Epoch: 20 [70400/73472 (96%)]\tLoss: 2.293306\n",
            "Train Epoch: 20 [71680/73472 (98%)]\tLoss: 2.325488\n",
            "Train Epoch: 20 [72960/73472 (99%)]\tLoss: 2.388732\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. , , , . he , the the <unknown> , the , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> , the he had , , `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he the to to the feet . he , had , of , , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was , the <unknown> . , , he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, i well , i , . . he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's been to be a , , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he had not but was to to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been the 's . he , was his head to and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , not he dumbledore was was have to . i 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , he 'm going him <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\t's the the dark . '' the dark , the , '' , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , he he have been to <unknown> of the dark . '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he the . his . he . eyes . he vernon was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, eyes was , of the , he a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had . been out the <unknown> . and he he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3743, Accuracy: 331610/549120 (60%)\n",
            "\n",
            "generated sample\t harry potter middle dumbledore will be guarded him so , you can like <unknown> unseen his <unknown> \n",
            "generated sample\t harry potter twice , added to <unknown> the way , durmstrang could make him balls . training \n",
            "generated sample\t harry potter might be just ribs . on the street letter , amazement she kept lying down \n",
            "generated sample\t harry potter nobody make the gaping , of the other should 've told them this ; that \n",
            "generated sample\t harry potter concerns , did n't he he would n't have been by this as he had \n",
            "generated sample\t harry potter realized - mercifully , because he was actually <unknown> . '' there was a vector \n",
            "generated sample\t harry potter dot like not become for bowed the prophecy when you and found my ghost hardly \n",
            "generated sample\t harry potter - said , the most task found no little monday to be in the cabinets \n",
            "generated sample\t harry potter test he really never was either , harry , he 's killed it might be \n",
            "generated sample\t harry potter <unknown> , is you ? fine sighed ! '' he said , he saw up \n",
            "generated beam\t\t harry potter to expect you , wherever , i am certainly , all i have all information \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 21 [0/73472 (0%)]\tLoss: 2.543755\n",
            "Train Epoch: 21 [1280/73472 (2%)]\tLoss: 2.288373\n",
            "Train Epoch: 21 [2560/73472 (3%)]\tLoss: 2.322857\n",
            "Train Epoch: 21 [3840/73472 (5%)]\tLoss: 2.337659\n",
            "Train Epoch: 21 [5120/73472 (7%)]\tLoss: 2.313841\n",
            "Train Epoch: 21 [6400/73472 (9%)]\tLoss: 2.289497\n",
            "Train Epoch: 21 [7680/73472 (10%)]\tLoss: 2.352465\n",
            "Train Epoch: 21 [8960/73472 (12%)]\tLoss: 2.315926\n",
            "Train Epoch: 21 [10240/73472 (14%)]\tLoss: 2.335996\n",
            "Train Epoch: 21 [11520/73472 (16%)]\tLoss: 2.302644\n",
            "Train Epoch: 21 [12800/73472 (17%)]\tLoss: 2.324451\n",
            "Train Epoch: 21 [14080/73472 (19%)]\tLoss: 2.267201\n",
            "Train Epoch: 21 [15360/73472 (21%)]\tLoss: 2.316293\n",
            "Train Epoch: 21 [16640/73472 (23%)]\tLoss: 2.338927\n",
            "Train Epoch: 21 [17920/73472 (24%)]\tLoss: 2.341198\n",
            "Train Epoch: 21 [19200/73472 (26%)]\tLoss: 2.361444\n",
            "Train Epoch: 21 [20480/73472 (28%)]\tLoss: 2.373692\n",
            "Train Epoch: 21 [21760/73472 (30%)]\tLoss: 2.372959\n",
            "Train Epoch: 21 [23040/73472 (31%)]\tLoss: 2.360343\n",
            "Train Epoch: 21 [24320/73472 (33%)]\tLoss: 2.299491\n",
            "Train Epoch: 21 [25600/73472 (35%)]\tLoss: 2.288730\n",
            "Train Epoch: 21 [26880/73472 (37%)]\tLoss: 2.299271\n",
            "Train Epoch: 21 [28160/73472 (38%)]\tLoss: 2.283217\n",
            "Train Epoch: 21 [29440/73472 (40%)]\tLoss: 2.322131\n",
            "Train Epoch: 21 [30720/73472 (42%)]\tLoss: 2.266016\n",
            "Train Epoch: 21 [32000/73472 (44%)]\tLoss: 2.291827\n",
            "Train Epoch: 21 [33280/73472 (45%)]\tLoss: 2.354764\n",
            "Train Epoch: 21 [34560/73472 (47%)]\tLoss: 2.319048\n",
            "Train Epoch: 21 [35840/73472 (49%)]\tLoss: 2.336300\n",
            "Train Epoch: 21 [37120/73472 (51%)]\tLoss: 2.249425\n",
            "Train Epoch: 21 [38400/73472 (52%)]\tLoss: 2.289631\n",
            "Train Epoch: 21 [39680/73472 (54%)]\tLoss: 2.265970\n",
            "Train Epoch: 21 [40960/73472 (56%)]\tLoss: 2.279429\n",
            "Train Epoch: 21 [42240/73472 (57%)]\tLoss: 2.326743\n",
            "Train Epoch: 21 [43520/73472 (59%)]\tLoss: 2.308667\n",
            "Train Epoch: 21 [44800/73472 (61%)]\tLoss: 2.200654\n",
            "Train Epoch: 21 [46080/73472 (63%)]\tLoss: 2.266670\n",
            "Train Epoch: 21 [47360/73472 (64%)]\tLoss: 2.258665\n",
            "Train Epoch: 21 [48640/73472 (66%)]\tLoss: 2.259172\n",
            "Train Epoch: 21 [49920/73472 (68%)]\tLoss: 2.353744\n",
            "Train Epoch: 21 [51200/73472 (70%)]\tLoss: 2.275698\n",
            "Train Epoch: 21 [52480/73472 (71%)]\tLoss: 2.250257\n",
            "Train Epoch: 21 [53760/73472 (73%)]\tLoss: 2.279135\n",
            "Train Epoch: 21 [55040/73472 (75%)]\tLoss: 2.285104\n",
            "Train Epoch: 21 [56320/73472 (77%)]\tLoss: 2.376728\n",
            "Train Epoch: 21 [57600/73472 (78%)]\tLoss: 2.317626\n",
            "Train Epoch: 21 [58880/73472 (80%)]\tLoss: 2.270542\n",
            "Train Epoch: 21 [60160/73472 (82%)]\tLoss: 2.289183\n",
            "Train Epoch: 21 [61440/73472 (84%)]\tLoss: 2.276707\n",
            "Train Epoch: 21 [62720/73472 (85%)]\tLoss: 2.225716\n",
            "Train Epoch: 21 [64000/73472 (87%)]\tLoss: 2.307144\n",
            "Train Epoch: 21 [65280/73472 (89%)]\tLoss: 2.265094\n",
            "Train Epoch: 21 [66560/73472 (91%)]\tLoss: 2.261342\n",
            "Train Epoch: 21 [67840/73472 (92%)]\tLoss: 2.338645\n",
            "Train Epoch: 21 [69120/73472 (94%)]\tLoss: 2.290494\n",
            "Train Epoch: 21 [70400/73472 (96%)]\tLoss: 2.284158\n",
            "Train Epoch: 21 [71680/73472 (98%)]\tLoss: 2.323966\n",
            "Train Epoch: 21 [72960/73472 (99%)]\tLoss: 2.380347\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. , , , . he , the the <unknown> , the , , `` \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> , the he had , , `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he the to to the feet . `` he had , of to , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> had in the <unknown> . . . he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , . . he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's not to be a right , '' , '' said said . . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he had not but was his to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been his 's . he he had his wand . and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he mcgonagall was was have to . he 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , though have going him <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\t's the the <unknown> . and the <unknown> , the , '' , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , he he have been to <unknown> of the <unknown> , he \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand . he his . his . he . eyes . he vernon was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, eyes was . of the . `` a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had . been out the <unknown> . and he he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3672, Accuracy: 332490/549120 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/021.pt\n",
            "\n",
            "generated sample\t harry potter this doubts immediately over . he grown ron eagerly and he said , `` i \n",
            "generated sample\t harry potter your lungs do n't even have if possible , ze , ah , jerking . \n",
            "generated sample\t harry potter <unknown> all , you really still not it ... i heard sore . i am \n",
            "generated sample\t harry potter have help stretched tinge to <unknown> draw with all about how i could n't be \n",
            "generated sample\t harry potter father potter 's real , the sixth <unknown> , `` you 'll want to have \n",
            "generated sample\t harry potter <unknown> they made everything much hole will because he could think he would end it \n",
            "generated sample\t harry potter could heaving , he was there . the combat joking could do something how to \n",
            "generated sample\t harry potter silver find walk . '' pockets like a stone red , boy knocker meant to \n",
            "generated sample\t harry potter grew in a escort . ron , ernie . . . . . . . \n",
            "generated sample\t harry potter the dursleys ones memory he already got a way of support what had the triwizard \n",
            "generated beam\t\t harry potter crash , oh hermione visited them all , sir ... but him could let you \n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 22 [0/73472 (0%)]\tLoss: 2.589436\n",
            "Train Epoch: 22 [1280/73472 (2%)]\tLoss: 2.279850\n",
            "Train Epoch: 22 [2560/73472 (3%)]\tLoss: 2.316619\n",
            "Train Epoch: 22 [3840/73472 (5%)]\tLoss: 2.333819\n",
            "Train Epoch: 22 [5120/73472 (7%)]\tLoss: 2.308029\n",
            "Train Epoch: 22 [6400/73472 (9%)]\tLoss: 2.284516\n",
            "Train Epoch: 22 [7680/73472 (10%)]\tLoss: 2.346608\n",
            "Train Epoch: 22 [8960/73472 (12%)]\tLoss: 2.308070\n",
            "Train Epoch: 22 [10240/73472 (14%)]\tLoss: 2.327781\n",
            "Train Epoch: 22 [11520/73472 (16%)]\tLoss: 2.293810\n",
            "Train Epoch: 22 [12800/73472 (17%)]\tLoss: 2.310253\n",
            "Train Epoch: 22 [14080/73472 (19%)]\tLoss: 2.255245\n",
            "Train Epoch: 22 [15360/73472 (21%)]\tLoss: 2.306680\n",
            "Train Epoch: 22 [16640/73472 (23%)]\tLoss: 2.333145\n",
            "Train Epoch: 22 [17920/73472 (24%)]\tLoss: 2.334057\n",
            "Train Epoch: 22 [19200/73472 (26%)]\tLoss: 2.356871\n",
            "Train Epoch: 22 [20480/73472 (28%)]\tLoss: 2.357585\n",
            "Train Epoch: 22 [21760/73472 (30%)]\tLoss: 2.364083\n",
            "Train Epoch: 22 [23040/73472 (31%)]\tLoss: 2.357671\n",
            "Train Epoch: 22 [24320/73472 (33%)]\tLoss: 2.289678\n",
            "Train Epoch: 22 [25600/73472 (35%)]\tLoss: 2.281752\n",
            "Train Epoch: 22 [26880/73472 (37%)]\tLoss: 2.290655\n",
            "Train Epoch: 22 [28160/73472 (38%)]\tLoss: 2.273318\n",
            "Train Epoch: 22 [29440/73472 (40%)]\tLoss: 2.305377\n",
            "Train Epoch: 22 [30720/73472 (42%)]\tLoss: 2.257081\n",
            "Train Epoch: 22 [32000/73472 (44%)]\tLoss: 2.275587\n",
            "Train Epoch: 22 [33280/73472 (45%)]\tLoss: 2.342249\n",
            "Train Epoch: 22 [34560/73472 (47%)]\tLoss: 2.308002\n",
            "Train Epoch: 22 [35840/73472 (49%)]\tLoss: 2.329864\n",
            "Train Epoch: 22 [37120/73472 (51%)]\tLoss: 2.243452\n",
            "Train Epoch: 22 [38400/73472 (52%)]\tLoss: 2.285052\n",
            "Train Epoch: 22 [39680/73472 (54%)]\tLoss: 2.257906\n",
            "Train Epoch: 22 [40960/73472 (56%)]\tLoss: 2.264231\n",
            "Train Epoch: 22 [42240/73472 (57%)]\tLoss: 2.327769\n",
            "Train Epoch: 22 [43520/73472 (59%)]\tLoss: 2.306690\n",
            "Train Epoch: 22 [44800/73472 (61%)]\tLoss: 2.192034\n",
            "Train Epoch: 22 [46080/73472 (63%)]\tLoss: 2.257573\n",
            "Train Epoch: 22 [47360/73472 (64%)]\tLoss: 2.252353\n",
            "Train Epoch: 22 [48640/73472 (66%)]\tLoss: 2.254851\n",
            "Train Epoch: 22 [49920/73472 (68%)]\tLoss: 2.340080\n",
            "Train Epoch: 22 [51200/73472 (70%)]\tLoss: 2.270685\n",
            "Train Epoch: 22 [52480/73472 (71%)]\tLoss: 2.242314\n",
            "Train Epoch: 22 [53760/73472 (73%)]\tLoss: 2.272793\n",
            "Train Epoch: 22 [55040/73472 (75%)]\tLoss: 2.283351\n",
            "Train Epoch: 22 [56320/73472 (77%)]\tLoss: 2.373695\n",
            "Train Epoch: 22 [57600/73472 (78%)]\tLoss: 2.308971\n",
            "Train Epoch: 22 [58880/73472 (80%)]\tLoss: 2.266016\n",
            "Train Epoch: 22 [60160/73472 (82%)]\tLoss: 2.281422\n",
            "Train Epoch: 22 [61440/73472 (84%)]\tLoss: 2.272188\n",
            "Train Epoch: 22 [62720/73472 (85%)]\tLoss: 2.220016\n",
            "Train Epoch: 22 [64000/73472 (87%)]\tLoss: 2.296638\n",
            "Train Epoch: 22 [65280/73472 (89%)]\tLoss: 2.263449\n",
            "Train Epoch: 22 [66560/73472 (91%)]\tLoss: 2.265106\n",
            "Train Epoch: 22 [67840/73472 (92%)]\tLoss: 2.334869\n",
            "Train Epoch: 22 [69120/73472 (94%)]\tLoss: 2.281407\n",
            "Train Epoch: 22 [70400/73472 (96%)]\tLoss: 2.278275\n",
            "Train Epoch: 22 [71680/73472 (98%)]\tLoss: 2.316971\n",
            "Train Epoch: 22 [72960/73472 (99%)]\tLoss: 2.373336\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. , . . . he , he the <unknown> , he . , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand <unknown> the <unknown> , the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he the to to the feet . he he had standing of to . \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was in the <unknown> . was . he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' . he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\ti 's not to be a right , '' , '' said said . . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he had not he was to to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been the 's . he , was his <unknown> . he \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he , was was was to . i 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , he 'm going him <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\t's the the dark . '' the dark , the , '' , , potter \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , '' he have been to <unknown> of the dark , '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he his . the . he . head . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\thad eyes was . of the , he a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had was been out the <unknown> . and he he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3593, Accuracy: 333245/549120 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/022.pt\n",
            "\n",
            "generated sample\t harry potter  quirrell stirring if he could stranded <unknown> cuts and are beams . he and charlie\n",
            "generated sample\t harry potter if,whattruthill it will be all clear . `` i deflected him . . \n",
            "generated sample\t harry potter  in somebody a werewolf . and then advertisement bones `` scabbers after <unknown> , malicious\n",
            "generated sample\t harry potter release ... '' `` oh , '' said fudge slowly . `` okay , '' \n",
            "generated sample\t harry potter mesmerizedwasmaster he was conditions ! and there do n't have a marking than that \n",
            "generated sample\t harry potter  may interruption harry started , trying to make fifty magic that ter be certainly young\n",
            "generated sample\t harry potter  did n't be quiet and more , i 'm going to be at eggs .\n",
            "generated sample\t harry potter folkwasno as hogwarts , when we would need to most somewhere that was sitting \n",
            "generated sample\t harry potter  ! '' he turned immediate , alohomora ring . he had the feeling , selected\n",
            "generated sample\t harry potter  they returned in a world . '' `` interested yes , '' said ron .\n",
            "generated beam\t\t harry potter  shut he over the life get where we dies lily and got me to leave\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 23 [0/73472 (0%)]\tLoss: 2.636000\n",
            "Train Epoch: 23 [1280/73472 (2%)]\tLoss: 2.274422\n",
            "Train Epoch: 23 [2560/73472 (3%)]\tLoss: 2.311070\n",
            "Train Epoch: 23 [3840/73472 (5%)]\tLoss: 2.334073\n",
            "Train Epoch: 23 [5120/73472 (7%)]\tLoss: 2.292738\n",
            "Train Epoch: 23 [6400/73472 (9%)]\tLoss: 2.276806\n",
            "Train Epoch: 23 [7680/73472 (10%)]\tLoss: 2.336736\n",
            "Train Epoch: 23 [8960/73472 (12%)]\tLoss: 2.303297\n",
            "Train Epoch: 23 [10240/73472 (14%)]\tLoss: 2.324484\n",
            "Train Epoch: 23 [11520/73472 (16%)]\tLoss: 2.290206\n",
            "Train Epoch: 23 [12800/73472 (17%)]\tLoss: 2.306878\n",
            "Train Epoch: 23 [14080/73472 (19%)]\tLoss: 2.247240\n",
            "Train Epoch: 23 [15360/73472 (21%)]\tLoss: 2.297506\n",
            "Train Epoch: 23 [16640/73472 (23%)]\tLoss: 2.318898\n",
            "Train Epoch: 23 [17920/73472 (24%)]\tLoss: 2.320565\n",
            "Train Epoch: 23 [19200/73472 (26%)]\tLoss: 2.352501\n",
            "Train Epoch: 23 [20480/73472 (28%)]\tLoss: 2.359846\n",
            "Train Epoch: 23 [21760/73472 (30%)]\tLoss: 2.357617\n",
            "Train Epoch: 23 [23040/73472 (31%)]\tLoss: 2.346194\n",
            "Train Epoch: 23 [24320/73472 (33%)]\tLoss: 2.288468\n",
            "Train Epoch: 23 [25600/73472 (35%)]\tLoss: 2.274156\n",
            "Train Epoch: 23 [26880/73472 (37%)]\tLoss: 2.285418\n",
            "Train Epoch: 23 [28160/73472 (38%)]\tLoss: 2.268741\n",
            "Train Epoch: 23 [29440/73472 (40%)]\tLoss: 2.301988\n",
            "Train Epoch: 23 [30720/73472 (42%)]\tLoss: 2.252904\n",
            "Train Epoch: 23 [32000/73472 (44%)]\tLoss: 2.263131\n",
            "Train Epoch: 23 [33280/73472 (45%)]\tLoss: 2.339658\n",
            "Train Epoch: 23 [34560/73472 (47%)]\tLoss: 2.309104\n",
            "Train Epoch: 23 [35840/73472 (49%)]\tLoss: 2.328097\n",
            "Train Epoch: 23 [37120/73472 (51%)]\tLoss: 2.235103\n",
            "Train Epoch: 23 [38400/73472 (52%)]\tLoss: 2.280459\n",
            "Train Epoch: 23 [39680/73472 (54%)]\tLoss: 2.258769\n",
            "Train Epoch: 23 [40960/73472 (56%)]\tLoss: 2.257236\n",
            "Train Epoch: 23 [42240/73472 (57%)]\tLoss: 2.322010\n",
            "Train Epoch: 23 [43520/73472 (59%)]\tLoss: 2.294139\n",
            "Train Epoch: 23 [44800/73472 (61%)]\tLoss: 2.181024\n",
            "Train Epoch: 23 [46080/73472 (63%)]\tLoss: 2.252956\n",
            "Train Epoch: 23 [47360/73472 (64%)]\tLoss: 2.247637\n",
            "Train Epoch: 23 [48640/73472 (66%)]\tLoss: 2.253892\n",
            "Train Epoch: 23 [49920/73472 (68%)]\tLoss: 2.339684\n",
            "Train Epoch: 23 [51200/73472 (70%)]\tLoss: 2.260986\n",
            "Train Epoch: 23 [52480/73472 (71%)]\tLoss: 2.234313\n",
            "Train Epoch: 23 [53760/73472 (73%)]\tLoss: 2.269700\n",
            "Train Epoch: 23 [55040/73472 (75%)]\tLoss: 2.285050\n",
            "Train Epoch: 23 [56320/73472 (77%)]\tLoss: 2.367236\n",
            "Train Epoch: 23 [57600/73472 (78%)]\tLoss: 2.300675\n",
            "Train Epoch: 23 [58880/73472 (80%)]\tLoss: 2.261569\n",
            "Train Epoch: 23 [60160/73472 (82%)]\tLoss: 2.271570\n",
            "Train Epoch: 23 [61440/73472 (84%)]\tLoss: 2.266917\n",
            "Train Epoch: 23 [62720/73472 (85%)]\tLoss: 2.216757\n",
            "Train Epoch: 23 [64000/73472 (87%)]\tLoss: 2.290824\n",
            "Train Epoch: 23 [65280/73472 (89%)]\tLoss: 2.251175\n",
            "Train Epoch: 23 [66560/73472 (91%)]\tLoss: 2.257692\n",
            "Train Epoch: 23 [67840/73472 (92%)]\tLoss: 2.328100\n",
            "Train Epoch: 23 [69120/73472 (94%)]\tLoss: 2.273129\n",
            "Train Epoch: 23 [70400/73472 (96%)]\tLoss: 2.272319\n",
            "Train Epoch: 23 [71680/73472 (98%)]\tLoss: 2.309331\n",
            "Train Epoch: 23 [72960/73472 (99%)]\tLoss: 2.368607\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. , . . . he , he the <unknown> , harry , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . he . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he him to to the feet . he he had <unknown> of to . \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. he <unknown> <unknown> was in the <unknown> . was . he . <unknown> , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' . he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be a right , '' , '' said said . . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he could not he was his to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he , was was was to . he 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , he 'm going out <unknown> . . '' you , '' you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\t's the the dark , and the dark , the , '' , , potter \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , he he have been to <unknown> of the dark , '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he his . the . he . head . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\thad eyes was . of the . he a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had was been out the <unknown> . and he he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3518, Accuracy: 333043/549120 (61%)\n",
            "\n",
            "generated sample\t harry potter  `` do n't he nicolas school ? '' said harry . his letter 's branch\n",
            "generated sample\t harry potter  ever , are you going to want for you ? '' said ron led ,\n",
            "generated sample\t harry potter  when i reliving asked nick . dumbledore was going to find the great top of\n",
            "generated sample\t harry potter  , <unknown> you , professor death ! listen to <unknown> all in states . we\n",
            "generated sample\t harry potter  , <unknown> what to come . hermione 's ever -- '' ron said , ``\n",
            "generated sample\t harry potter  ... . she had still caught yet they remembered years except holding out those arms\n",
            "generated sample\t harry potter  . but he really never an his best really execution , who was though ,\n",
            "generated sample\t harry potter  are a <unknown> textbook in an eye with <unknown> . he was good crack father\n",
            "generated sample\t harry potter  , because the er cat is said , and she was another pleading to know\n",
            "generated sample\t harry potter  in an walk of spend hippogriff ; the dursleys broke vacated , but he was\n",
            "generated beam\t\t harry potter  tired , well , of course he knew that knocking himself and being grown him\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 24 [0/73472 (0%)]\tLoss: 2.667445\n",
            "Train Epoch: 24 [1280/73472 (2%)]\tLoss: 2.261360\n",
            "Train Epoch: 24 [2560/73472 (3%)]\tLoss: 2.297239\n",
            "Train Epoch: 24 [3840/73472 (5%)]\tLoss: 2.320249\n",
            "Train Epoch: 24 [5120/73472 (7%)]\tLoss: 2.294406\n",
            "Train Epoch: 24 [6400/73472 (9%)]\tLoss: 2.271004\n",
            "Train Epoch: 24 [7680/73472 (10%)]\tLoss: 2.332367\n",
            "Train Epoch: 24 [8960/73472 (12%)]\tLoss: 2.297166\n",
            "Train Epoch: 24 [10240/73472 (14%)]\tLoss: 2.317265\n",
            "Train Epoch: 24 [11520/73472 (16%)]\tLoss: 2.286279\n",
            "Train Epoch: 24 [12800/73472 (17%)]\tLoss: 2.303005\n",
            "Train Epoch: 24 [14080/73472 (19%)]\tLoss: 2.244312\n",
            "Train Epoch: 24 [15360/73472 (21%)]\tLoss: 2.291183\n",
            "Train Epoch: 24 [16640/73472 (23%)]\tLoss: 2.314103\n",
            "Train Epoch: 24 [17920/73472 (24%)]\tLoss: 2.313332\n",
            "Train Epoch: 24 [19200/73472 (26%)]\tLoss: 2.344443\n",
            "Train Epoch: 24 [20480/73472 (28%)]\tLoss: 2.349666\n",
            "Train Epoch: 24 [21760/73472 (30%)]\tLoss: 2.349498\n",
            "Train Epoch: 24 [23040/73472 (31%)]\tLoss: 2.341474\n",
            "Train Epoch: 24 [24320/73472 (33%)]\tLoss: 2.282651\n",
            "Train Epoch: 24 [25600/73472 (35%)]\tLoss: 2.270675\n",
            "Train Epoch: 24 [26880/73472 (37%)]\tLoss: 2.281174\n",
            "Train Epoch: 24 [28160/73472 (38%)]\tLoss: 2.260916\n",
            "Train Epoch: 24 [29440/73472 (40%)]\tLoss: 2.297721\n",
            "Train Epoch: 24 [30720/73472 (42%)]\tLoss: 2.244275\n",
            "Train Epoch: 24 [32000/73472 (44%)]\tLoss: 2.257372\n",
            "Train Epoch: 24 [33280/73472 (45%)]\tLoss: 2.334534\n",
            "Train Epoch: 24 [34560/73472 (47%)]\tLoss: 2.299488\n",
            "Train Epoch: 24 [35840/73472 (49%)]\tLoss: 2.322490\n",
            "Train Epoch: 24 [37120/73472 (51%)]\tLoss: 2.228789\n",
            "Train Epoch: 24 [38400/73472 (52%)]\tLoss: 2.277032\n",
            "Train Epoch: 24 [39680/73472 (54%)]\tLoss: 2.255172\n",
            "Train Epoch: 24 [40960/73472 (56%)]\tLoss: 2.251484\n",
            "Train Epoch: 24 [42240/73472 (57%)]\tLoss: 2.315678\n",
            "Train Epoch: 24 [43520/73472 (59%)]\tLoss: 2.288727\n",
            "Train Epoch: 24 [44800/73472 (61%)]\tLoss: 2.177089\n",
            "Train Epoch: 24 [46080/73472 (63%)]\tLoss: 2.247355\n",
            "Train Epoch: 24 [47360/73472 (64%)]\tLoss: 2.237433\n",
            "Train Epoch: 24 [48640/73472 (66%)]\tLoss: 2.246538\n",
            "Train Epoch: 24 [49920/73472 (68%)]\tLoss: 2.332055\n",
            "Train Epoch: 24 [51200/73472 (70%)]\tLoss: 2.257819\n",
            "Train Epoch: 24 [52480/73472 (71%)]\tLoss: 2.226892\n",
            "Train Epoch: 24 [53760/73472 (73%)]\tLoss: 2.262663\n",
            "Train Epoch: 24 [55040/73472 (75%)]\tLoss: 2.284491\n",
            "Train Epoch: 24 [56320/73472 (77%)]\tLoss: 2.360184\n",
            "Train Epoch: 24 [57600/73472 (78%)]\tLoss: 2.297008\n",
            "Train Epoch: 24 [58880/73472 (80%)]\tLoss: 2.258925\n",
            "Train Epoch: 24 [60160/73472 (82%)]\tLoss: 2.265476\n",
            "Train Epoch: 24 [61440/73472 (84%)]\tLoss: 2.258152\n",
            "Train Epoch: 24 [62720/73472 (85%)]\tLoss: 2.211670\n",
            "Train Epoch: 24 [64000/73472 (87%)]\tLoss: 2.289582\n",
            "Train Epoch: 24 [65280/73472 (89%)]\tLoss: 2.246730\n",
            "Train Epoch: 24 [66560/73472 (91%)]\tLoss: 2.249840\n",
            "Train Epoch: 24 [67840/73472 (92%)]\tLoss: 2.319695\n",
            "Train Epoch: 24 [69120/73472 (94%)]\tLoss: 2.273777\n",
            "Train Epoch: 24 [70400/73472 (96%)]\tLoss: 2.267730\n",
            "Train Epoch: 24 [71680/73472 (98%)]\tLoss: 2.305953\n",
            "Train Epoch: 24 [72960/73472 (99%)]\tLoss: 2.363323\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. , . . . he , he the <unknown> , harry was , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he him to to the feet . he he had <unknown> of to . \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was in the air . was . he , the , \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' . he i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be a right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t, he as he could not he was his to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , he \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` was a he dumbledore was was was to . he 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , he was going him wand . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas the the <unknown> , and the <unknown> , the , `` , , potter \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , he he have been to <unknown> of the dark , he \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand . he his . the . he . eyes . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\thad eyes was . of the . he a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had was been out the <unknown> . and that he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3472, Accuracy: 333740/549120 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/024.pt\n",
            "\n",
            "generated sample\t harry potter  them . still jaw , and if they could be sorry you 've got enough\n",
            "generated sample\t harry potter  to see of most <unknown> should would make them than their once who was .\n",
            "generated sample\t harry potter  it 're . . . it knew you could it . how ? '' ``\n",
            "generated sample\t harry potter  i 'll be time the dementors to be in it , he . it will\n",
            "generated sample\t harry potter  route scabbers for his very pocket . discovered , ' , <unknown> vance , dripped\n",
            "generated sample\t harry potter  . we have topic of aurors against the barrier -- things i mean can you\n",
            "generated sample\t harry potter  that <unknown> not sleepy . . . . '' `` ginny , '' said hermione\n",
            "generated sample\t harry potter  . my cheered , harry - i have been bertie in death because that could\n",
            "generated sample\t harry potter  on , sirius , hermione , who accord , more than your examinations . .\n",
            "generated sample\t harry potter  . '' `` ... , you 're a polished snake , '' said him voice\n",
            "generated beam\t\t harry potter  exactly that streams and <unknown> ' lift was remains , watching readers - and thinking\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 25 [0/73472 (0%)]\tLoss: 2.679382\n",
            "Train Epoch: 25 [1280/73472 (2%)]\tLoss: 2.258792\n",
            "Train Epoch: 25 [2560/73472 (3%)]\tLoss: 2.296441\n",
            "Train Epoch: 25 [3840/73472 (5%)]\tLoss: 2.316458\n",
            "Train Epoch: 25 [5120/73472 (7%)]\tLoss: 2.285743\n",
            "Train Epoch: 25 [6400/73472 (9%)]\tLoss: 2.264726\n",
            "Train Epoch: 25 [7680/73472 (10%)]\tLoss: 2.327832\n",
            "Train Epoch: 25 [8960/73472 (12%)]\tLoss: 2.291688\n",
            "Train Epoch: 25 [10240/73472 (14%)]\tLoss: 2.314200\n",
            "Train Epoch: 25 [11520/73472 (16%)]\tLoss: 2.280750\n",
            "Train Epoch: 25 [12800/73472 (17%)]\tLoss: 2.295719\n",
            "Train Epoch: 25 [14080/73472 (19%)]\tLoss: 2.235378\n",
            "Train Epoch: 25 [15360/73472 (21%)]\tLoss: 2.286620\n",
            "Train Epoch: 25 [16640/73472 (23%)]\tLoss: 2.307087\n",
            "Train Epoch: 25 [17920/73472 (24%)]\tLoss: 2.310594\n",
            "Train Epoch: 25 [19200/73472 (26%)]\tLoss: 2.340458\n",
            "Train Epoch: 25 [20480/73472 (28%)]\tLoss: 2.341264\n",
            "Train Epoch: 25 [21760/73472 (30%)]\tLoss: 2.341295\n",
            "Train Epoch: 25 [23040/73472 (31%)]\tLoss: 2.338336\n",
            "Train Epoch: 25 [24320/73472 (33%)]\tLoss: 2.281463\n",
            "Train Epoch: 25 [25600/73472 (35%)]\tLoss: 2.269064\n",
            "Train Epoch: 25 [26880/73472 (37%)]\tLoss: 2.278692\n",
            "Train Epoch: 25 [28160/73472 (38%)]\tLoss: 2.256433\n",
            "Train Epoch: 25 [29440/73472 (40%)]\tLoss: 2.288139\n",
            "Train Epoch: 25 [30720/73472 (42%)]\tLoss: 2.239470\n",
            "Train Epoch: 25 [32000/73472 (44%)]\tLoss: 2.258040\n",
            "Train Epoch: 25 [33280/73472 (45%)]\tLoss: 2.332976\n",
            "Train Epoch: 25 [34560/73472 (47%)]\tLoss: 2.298772\n",
            "Train Epoch: 25 [35840/73472 (49%)]\tLoss: 2.319999\n",
            "Train Epoch: 25 [37120/73472 (51%)]\tLoss: 2.237828\n",
            "Train Epoch: 25 [38400/73472 (52%)]\tLoss: 2.273234\n",
            "Train Epoch: 25 [39680/73472 (54%)]\tLoss: 2.244925\n",
            "Train Epoch: 25 [40960/73472 (56%)]\tLoss: 2.245758\n",
            "Train Epoch: 25 [42240/73472 (57%)]\tLoss: 2.310719\n",
            "Train Epoch: 25 [43520/73472 (59%)]\tLoss: 2.283620\n",
            "Train Epoch: 25 [44800/73472 (61%)]\tLoss: 2.175435\n",
            "Train Epoch: 25 [46080/73472 (63%)]\tLoss: 2.244919\n",
            "Train Epoch: 25 [47360/73472 (64%)]\tLoss: 2.237338\n",
            "Train Epoch: 25 [48640/73472 (66%)]\tLoss: 2.246332\n",
            "Train Epoch: 25 [49920/73472 (68%)]\tLoss: 2.328437\n",
            "Train Epoch: 25 [51200/73472 (70%)]\tLoss: 2.255191\n",
            "Train Epoch: 25 [52480/73472 (71%)]\tLoss: 2.222714\n",
            "Train Epoch: 25 [53760/73472 (73%)]\tLoss: 2.259483\n",
            "Train Epoch: 25 [55040/73472 (75%)]\tLoss: 2.278899\n",
            "Train Epoch: 25 [56320/73472 (77%)]\tLoss: 2.358396\n",
            "Train Epoch: 25 [57600/73472 (78%)]\tLoss: 2.297395\n",
            "Train Epoch: 25 [58880/73472 (80%)]\tLoss: 2.258461\n",
            "Train Epoch: 25 [60160/73472 (82%)]\tLoss: 2.260242\n",
            "Train Epoch: 25 [61440/73472 (84%)]\tLoss: 2.253513\n",
            "Train Epoch: 25 [62720/73472 (85%)]\tLoss: 2.206510\n",
            "Train Epoch: 25 [64000/73472 (87%)]\tLoss: 2.284313\n",
            "Train Epoch: 25 [65280/73472 (89%)]\tLoss: 2.241544\n",
            "Train Epoch: 25 [66560/73472 (91%)]\tLoss: 2.248760\n",
            "Train Epoch: 25 [67840/73472 (92%)]\tLoss: 2.318217\n",
            "Train Epoch: 25 [69120/73472 (94%)]\tLoss: 2.270328\n",
            "Train Epoch: 25 [70400/73472 (96%)]\tLoss: 2.263954\n",
            "Train Epoch: 25 [71680/73472 (98%)]\tLoss: 2.308026\n",
            "Train Epoch: 25 [72960/73472 (99%)]\tLoss: 2.360480\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. , . . . he , he the <unknown> , harry , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\t, he the to to the feet . `` he were <unknown> of to . \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was in the air . . . he , <unknown> . \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' . `` i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be a right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he could not he was up , the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he mcgonagall was was was to . he 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , he could going out wand . . `` the , he the , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas his the <unknown> , and a <unknown> , the , `` , , 's \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto . `` , he he was been to <unknown> of the <unknown> , he \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand . he his . the . he . wand . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, eyes was . of the . `` a , of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had was been out the <unknown> . and that he had see \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3450, Accuracy: 333747/549120 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/025.pt\n",
            "\n",
            "generated sample\t harry potter  . '' snape was round , and he looked on began to the ern lighter\n",
            "generated sample\t harry potter  even he better prat . as though he had a new hints distant ever around\n",
            "generated sample\t harry potter  . ... and this has anyone looking a clue before a member of them for\n",
            "generated sample\t harry potter  . '' madame <unknown> goblet his name was <unknown> <unknown> to the back ; nine\n",
            "generated sample\t harry potter  , when it moth-eaten that he had no palace years . there corridor his great\n",
            "generated sample\t harry potter  the owl hall being recover . `` listen ! '' said firenze doxys . .\n",
            "generated sample\t harry potter  come to relive him in the common room . '' `` not you , ''\n",
            "generated sample\t harry potter  . malfoy , sir . . . it was rather like knowing . when usual\n",
            "generated sample\t harry potter  found a short board `` mum , '' he shouted . he was <unknown> as\n",
            "generated sample\t harry potter  ... potter , crouch , it all your son farther with a <unknown> - ''\n",
            "generated beam\t\t harry potter  , <unknown> hagrid december into behind him and said , leprechaun of evade them ,\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 26 [0/73472 (0%)]\tLoss: 2.682131\n",
            "Train Epoch: 26 [1280/73472 (2%)]\tLoss: 2.261581\n",
            "Train Epoch: 26 [2560/73472 (3%)]\tLoss: 2.298428\n",
            "Train Epoch: 26 [3840/73472 (5%)]\tLoss: 2.313570\n",
            "Train Epoch: 26 [5120/73472 (7%)]\tLoss: 2.282451\n",
            "Train Epoch: 26 [6400/73472 (9%)]\tLoss: 2.257817\n",
            "Train Epoch: 26 [7680/73472 (10%)]\tLoss: 2.322064\n",
            "Train Epoch: 26 [8960/73472 (12%)]\tLoss: 2.287156\n",
            "Train Epoch: 26 [10240/73472 (14%)]\tLoss: 2.311515\n",
            "Train Epoch: 26 [11520/73472 (16%)]\tLoss: 2.277467\n",
            "Train Epoch: 26 [12800/73472 (17%)]\tLoss: 2.293736\n",
            "Train Epoch: 26 [14080/73472 (19%)]\tLoss: 2.229964\n",
            "Train Epoch: 26 [15360/73472 (21%)]\tLoss: 2.283493\n",
            "Train Epoch: 26 [16640/73472 (23%)]\tLoss: 2.305304\n",
            "Train Epoch: 26 [17920/73472 (24%)]\tLoss: 2.309412\n",
            "Train Epoch: 26 [19200/73472 (26%)]\tLoss: 2.337371\n",
            "Train Epoch: 26 [20480/73472 (28%)]\tLoss: 2.340290\n",
            "Train Epoch: 26 [21760/73472 (30%)]\tLoss: 2.338218\n",
            "Train Epoch: 26 [23040/73472 (31%)]\tLoss: 2.336076\n",
            "Train Epoch: 26 [24320/73472 (33%)]\tLoss: 2.277977\n",
            "Train Epoch: 26 [25600/73472 (35%)]\tLoss: 2.266147\n",
            "Train Epoch: 26 [26880/73472 (37%)]\tLoss: 2.277003\n",
            "Train Epoch: 26 [28160/73472 (38%)]\tLoss: 2.253323\n",
            "Train Epoch: 26 [29440/73472 (40%)]\tLoss: 2.283474\n",
            "Train Epoch: 26 [30720/73472 (42%)]\tLoss: 2.236649\n",
            "Train Epoch: 26 [32000/73472 (44%)]\tLoss: 2.254025\n",
            "Train Epoch: 26 [33280/73472 (45%)]\tLoss: 2.326753\n",
            "Train Epoch: 26 [34560/73472 (47%)]\tLoss: 2.292714\n",
            "Train Epoch: 26 [35840/73472 (49%)]\tLoss: 2.315283\n",
            "Train Epoch: 26 [37120/73472 (51%)]\tLoss: 2.230350\n",
            "Train Epoch: 26 [38400/73472 (52%)]\tLoss: 2.268363\n",
            "Train Epoch: 26 [39680/73472 (54%)]\tLoss: 2.241048\n",
            "Train Epoch: 26 [40960/73472 (56%)]\tLoss: 2.241863\n",
            "Train Epoch: 26 [42240/73472 (57%)]\tLoss: 2.304616\n",
            "Train Epoch: 26 [43520/73472 (59%)]\tLoss: 2.283544\n",
            "Train Epoch: 26 [44800/73472 (61%)]\tLoss: 2.171548\n",
            "Train Epoch: 26 [46080/73472 (63%)]\tLoss: 2.241327\n",
            "Train Epoch: 26 [47360/73472 (64%)]\tLoss: 2.229530\n",
            "Train Epoch: 26 [48640/73472 (66%)]\tLoss: 2.244164\n",
            "Train Epoch: 26 [49920/73472 (68%)]\tLoss: 2.324893\n",
            "Train Epoch: 26 [51200/73472 (70%)]\tLoss: 2.251974\n",
            "Train Epoch: 26 [52480/73472 (71%)]\tLoss: 2.220090\n",
            "Train Epoch: 26 [53760/73472 (73%)]\tLoss: 2.255506\n",
            "Train Epoch: 26 [55040/73472 (75%)]\tLoss: 2.276823\n",
            "Train Epoch: 26 [56320/73472 (77%)]\tLoss: 2.352852\n",
            "Train Epoch: 26 [57600/73472 (78%)]\tLoss: 2.292968\n",
            "Train Epoch: 26 [58880/73472 (80%)]\tLoss: 2.256113\n",
            "Train Epoch: 26 [60160/73472 (82%)]\tLoss: 2.257146\n",
            "Train Epoch: 26 [61440/73472 (84%)]\tLoss: 2.248768\n",
            "Train Epoch: 26 [62720/73472 (85%)]\tLoss: 2.200495\n",
            "Train Epoch: 26 [64000/73472 (87%)]\tLoss: 2.280771\n",
            "Train Epoch: 26 [65280/73472 (89%)]\tLoss: 2.238594\n",
            "Train Epoch: 26 [66560/73472 (91%)]\tLoss: 2.244601\n",
            "Train Epoch: 26 [67840/73472 (92%)]\tLoss: 2.312454\n",
            "Train Epoch: 26 [69120/73472 (94%)]\tLoss: 2.265621\n",
            "Train Epoch: 26 [70400/73472 (96%)]\tLoss: 2.259365\n",
            "Train Epoch: 26 [71680/73472 (98%)]\tLoss: 2.301108\n",
            "Train Epoch: 26 [72960/73472 (99%)]\tLoss: 2.354930\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. . . . '' he , he the <unknown> , '' , , '' \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he him to to the feet . he he could <unknown> of to . \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was to the air . . . he . <unknown> . \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' '' `` i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be a right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he could not he was up to the he . his , \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore was was was to . he 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\t, , he 'm going out <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas his the <unknown> . and the <unknown> , the , '' , , , \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , he he have been to <unknown> of the <unknown> , '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he his . the . he . wand . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> was . of the . `` a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had was been out the <unknown> . and that he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3403, Accuracy: 334149/549120 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/026.pt\n",
            "\n",
            "generated sample\t harry potter  . by what did you know , or do i do n't gawping you .\n",
            "generated sample\t harry potter  ! '' the tiny crossed made the dog and cuff struggling over to ominous feet\n",
            "generated sample\t harry potter  , so like it was gone <unknown> sun by their voices , harry saw her\n",
            "generated sample\t harry potter  and stuff <unknown> whatever those days . ... at a <unknown> the end ! ''\n",
            "generated sample\t harry potter  . ... dimitrov ' nearly usual . suddenly deeply , i was dustbins in all\n",
            "generated sample\t harry potter  , <unknown> . <unknown> points ter loved <unknown> - '' dumbledore yelled . `` do\n",
            "generated sample\t harry potter  ... . harry stood off to shifty her as they like here much . at\n",
            "generated sample\t harry potter  , he 'd told harry to <unknown> who would find hedge black by chatting .\n",
            "generated sample\t harry potter  <unknown> moments , stone generations confidently . ripping . . the place of the person\n",
            "generated sample\t harry potter  . '' `` those counterclockwise 'll be ready ! '' asked dumbledore , glaring .\n",
            "generated beam\t\t harry potter  . look contained , quickly - ca n't you met rather quite two arthur -\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 27 [0/73472 (0%)]\tLoss: 2.687344\n",
            "Train Epoch: 27 [1280/73472 (2%)]\tLoss: 2.253472\n",
            "Train Epoch: 27 [2560/73472 (3%)]\tLoss: 2.291342\n",
            "Train Epoch: 27 [3840/73472 (5%)]\tLoss: 2.309514\n",
            "Train Epoch: 27 [5120/73472 (7%)]\tLoss: 2.285579\n",
            "Train Epoch: 27 [6400/73472 (9%)]\tLoss: 2.254666\n",
            "Train Epoch: 27 [7680/73472 (10%)]\tLoss: 2.321126\n",
            "Train Epoch: 27 [8960/73472 (12%)]\tLoss: 2.284156\n",
            "Train Epoch: 27 [10240/73472 (14%)]\tLoss: 2.307421\n",
            "Train Epoch: 27 [11520/73472 (16%)]\tLoss: 2.276322\n",
            "Train Epoch: 27 [12800/73472 (17%)]\tLoss: 2.294945\n",
            "Train Epoch: 27 [14080/73472 (19%)]\tLoss: 2.229988\n",
            "Train Epoch: 27 [15360/73472 (21%)]\tLoss: 2.283399\n",
            "Train Epoch: 27 [16640/73472 (23%)]\tLoss: 2.306026\n",
            "Train Epoch: 27 [17920/73472 (24%)]\tLoss: 2.309489\n",
            "Train Epoch: 27 [19200/73472 (26%)]\tLoss: 2.334609\n",
            "Train Epoch: 27 [20480/73472 (28%)]\tLoss: 2.340652\n",
            "Train Epoch: 27 [21760/73472 (30%)]\tLoss: 2.338218\n",
            "Train Epoch: 27 [23040/73472 (31%)]\tLoss: 2.334662\n",
            "Train Epoch: 27 [24320/73472 (33%)]\tLoss: 2.273622\n",
            "Train Epoch: 27 [25600/73472 (35%)]\tLoss: 2.262708\n",
            "Train Epoch: 27 [26880/73472 (37%)]\tLoss: 2.270080\n",
            "Train Epoch: 27 [28160/73472 (38%)]\tLoss: 2.247523\n",
            "Train Epoch: 27 [29440/73472 (40%)]\tLoss: 2.277257\n",
            "Train Epoch: 27 [30720/73472 (42%)]\tLoss: 2.232204\n",
            "Train Epoch: 27 [32000/73472 (44%)]\tLoss: 2.249360\n",
            "Train Epoch: 27 [33280/73472 (45%)]\tLoss: 2.326280\n",
            "Train Epoch: 27 [34560/73472 (47%)]\tLoss: 2.290137\n",
            "Train Epoch: 27 [35840/73472 (49%)]\tLoss: 2.314418\n",
            "Train Epoch: 27 [37120/73472 (51%)]\tLoss: 2.222302\n",
            "Train Epoch: 27 [38400/73472 (52%)]\tLoss: 2.262945\n",
            "Train Epoch: 27 [39680/73472 (54%)]\tLoss: 2.239013\n",
            "Train Epoch: 27 [40960/73472 (56%)]\tLoss: 2.239021\n",
            "Train Epoch: 27 [42240/73472 (57%)]\tLoss: 2.303845\n",
            "Train Epoch: 27 [43520/73472 (59%)]\tLoss: 2.278778\n",
            "Train Epoch: 27 [44800/73472 (61%)]\tLoss: 2.166504\n",
            "Train Epoch: 27 [46080/73472 (63%)]\tLoss: 2.241464\n",
            "Train Epoch: 27 [47360/73472 (64%)]\tLoss: 2.229870\n",
            "Train Epoch: 27 [48640/73472 (66%)]\tLoss: 2.242153\n",
            "Train Epoch: 27 [49920/73472 (68%)]\tLoss: 2.327015\n",
            "Train Epoch: 27 [51200/73472 (70%)]\tLoss: 2.246718\n",
            "Train Epoch: 27 [52480/73472 (71%)]\tLoss: 2.219343\n",
            "Train Epoch: 27 [53760/73472 (73%)]\tLoss: 2.255744\n",
            "Train Epoch: 27 [55040/73472 (75%)]\tLoss: 2.276325\n",
            "Train Epoch: 27 [56320/73472 (77%)]\tLoss: 2.351866\n",
            "Train Epoch: 27 [57600/73472 (78%)]\tLoss: 2.290597\n",
            "Train Epoch: 27 [58880/73472 (80%)]\tLoss: 2.250798\n",
            "Train Epoch: 27 [60160/73472 (82%)]\tLoss: 2.252459\n",
            "Train Epoch: 27 [61440/73472 (84%)]\tLoss: 2.250191\n",
            "Train Epoch: 27 [62720/73472 (85%)]\tLoss: 2.201592\n",
            "Train Epoch: 27 [64000/73472 (87%)]\tLoss: 2.278831\n",
            "Train Epoch: 27 [65280/73472 (89%)]\tLoss: 2.235775\n",
            "Train Epoch: 27 [66560/73472 (91%)]\tLoss: 2.241351\n",
            "Train Epoch: 27 [67840/73472 (92%)]\tLoss: 2.307194\n",
            "Train Epoch: 27 [69120/73472 (94%)]\tLoss: 2.262319\n",
            "Train Epoch: 27 [70400/73472 (96%)]\tLoss: 2.257210\n",
            "Train Epoch: 27 [71680/73472 (98%)]\tLoss: 2.301573\n",
            "Train Epoch: 27 [72960/73472 (99%)]\tLoss: 2.351284\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. . . . . he , he the <unknown> , harry , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he him to to the feet . `` he had <unknown> of to . \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was to the air . . . he . <unknown> . \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' '' `` i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be <unknown> right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he could not and was up to the he . his . \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` was a he mcgonagall was was was to . he 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\tto , he was going out wand . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas the the <unknown> . and the <unknown> , the , '' , to 's \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he , he said have been to <unknown> of the <unknown> . he \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he his to the . he . head . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> was . of the . `` a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had was been out to <unknown> . and that he had see \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3395, Accuracy: 334141/549120 (61%)\n",
            "\n",
            "generated sample\t harry potter  . then they stood in car now emitted toward the contents again , protective .\n",
            "generated sample\t harry potter  from he felt his turkey <unknown> . `` yeah , '' he said information .\n",
            "generated sample\t harry potter  . there ... seamus , i flicked crammed down . '' hagrid threw and out\n",
            "generated sample\t harry potter  . harry and opened his heads dark face . emptying it obviously , he without\n",
            "generated sample\t harry potter  . hey malfoy was revealing very lest to gaze stacked . either , and jealousy\n",
            "generated sample\t harry potter  , there killed with sirius , hermione were affronted with a whooshing of meals that\n",
            "generated sample\t harry potter  . `` i knew you lip , '' said ginny , storm on . he\n",
            "generated sample\t harry potter  was orbs of angry days , the turban for the nose and only quite seemed\n",
            "generated sample\t harry potter  . hermione walked up to out . he 's arm had narcissa , ze ,\n",
            "generated sample\t harry potter  . `` you 're keepin a passes i almost defense into dumbledore 's an hand\n",
            "generated beam\t\t harry potter  . apparently hoping , he was getting going to harry way his wand with a\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 28 [0/73472 (0%)]\tLoss: 2.690582\n",
            "Train Epoch: 28 [1280/73472 (2%)]\tLoss: 2.247452\n",
            "Train Epoch: 28 [2560/73472 (3%)]\tLoss: 2.284711\n",
            "Train Epoch: 28 [3840/73472 (5%)]\tLoss: 2.311034\n",
            "Train Epoch: 28 [5120/73472 (7%)]\tLoss: 2.277435\n",
            "Train Epoch: 28 [6400/73472 (9%)]\tLoss: 2.247375\n",
            "Train Epoch: 28 [7680/73472 (10%)]\tLoss: 2.319363\n",
            "Train Epoch: 28 [8960/73472 (12%)]\tLoss: 2.284806\n",
            "Train Epoch: 28 [10240/73472 (14%)]\tLoss: 2.303636\n",
            "Train Epoch: 28 [11520/73472 (16%)]\tLoss: 2.274559\n",
            "Train Epoch: 28 [12800/73472 (17%)]\tLoss: 2.284165\n",
            "Train Epoch: 28 [14080/73472 (19%)]\tLoss: 2.223908\n",
            "Train Epoch: 28 [15360/73472 (21%)]\tLoss: 2.276874\n",
            "Train Epoch: 28 [16640/73472 (23%)]\tLoss: 2.298593\n",
            "Train Epoch: 28 [17920/73472 (24%)]\tLoss: 2.306692\n",
            "Train Epoch: 28 [19200/73472 (26%)]\tLoss: 2.335372\n",
            "Train Epoch: 28 [20480/73472 (28%)]\tLoss: 2.333211\n",
            "Train Epoch: 28 [21760/73472 (30%)]\tLoss: 2.332608\n",
            "Train Epoch: 28 [23040/73472 (31%)]\tLoss: 2.329333\n",
            "Train Epoch: 28 [24320/73472 (33%)]\tLoss: 2.275717\n",
            "Train Epoch: 28 [25600/73472 (35%)]\tLoss: 2.256514\n",
            "Train Epoch: 28 [26880/73472 (37%)]\tLoss: 2.273491\n",
            "Train Epoch: 28 [28160/73472 (38%)]\tLoss: 2.253491\n",
            "Train Epoch: 28 [29440/73472 (40%)]\tLoss: 2.273196\n",
            "Train Epoch: 28 [30720/73472 (42%)]\tLoss: 2.235320\n",
            "Train Epoch: 28 [32000/73472 (44%)]\tLoss: 2.246883\n",
            "Train Epoch: 28 [33280/73472 (45%)]\tLoss: 2.329482\n",
            "Train Epoch: 28 [34560/73472 (47%)]\tLoss: 2.285535\n",
            "Train Epoch: 28 [35840/73472 (49%)]\tLoss: 2.312059\n",
            "Train Epoch: 28 [37120/73472 (51%)]\tLoss: 2.227722\n",
            "Train Epoch: 28 [38400/73472 (52%)]\tLoss: 2.262822\n",
            "Train Epoch: 28 [39680/73472 (54%)]\tLoss: 2.233133\n",
            "Train Epoch: 28 [40960/73472 (56%)]\tLoss: 2.234249\n",
            "Train Epoch: 28 [42240/73472 (57%)]\tLoss: 2.302149\n",
            "Train Epoch: 28 [43520/73472 (59%)]\tLoss: 2.278157\n",
            "Train Epoch: 28 [44800/73472 (61%)]\tLoss: 2.163425\n",
            "Train Epoch: 28 [46080/73472 (63%)]\tLoss: 2.237718\n",
            "Train Epoch: 28 [47360/73472 (64%)]\tLoss: 2.227847\n",
            "Train Epoch: 28 [48640/73472 (66%)]\tLoss: 2.242535\n",
            "Train Epoch: 28 [49920/73472 (68%)]\tLoss: 2.326020\n",
            "Train Epoch: 28 [51200/73472 (70%)]\tLoss: 2.244428\n",
            "Train Epoch: 28 [52480/73472 (71%)]\tLoss: 2.216579\n",
            "Train Epoch: 28 [53760/73472 (73%)]\tLoss: 2.254621\n",
            "Train Epoch: 28 [55040/73472 (75%)]\tLoss: 2.269835\n",
            "Train Epoch: 28 [56320/73472 (77%)]\tLoss: 2.344470\n",
            "Train Epoch: 28 [57600/73472 (78%)]\tLoss: 2.285969\n",
            "Train Epoch: 28 [58880/73472 (80%)]\tLoss: 2.248523\n",
            "Train Epoch: 28 [60160/73472 (82%)]\tLoss: 2.250655\n",
            "Train Epoch: 28 [61440/73472 (84%)]\tLoss: 2.248415\n",
            "Train Epoch: 28 [62720/73472 (85%)]\tLoss: 2.202126\n",
            "Train Epoch: 28 [64000/73472 (87%)]\tLoss: 2.277085\n",
            "Train Epoch: 28 [65280/73472 (89%)]\tLoss: 2.235079\n",
            "Train Epoch: 28 [66560/73472 (91%)]\tLoss: 2.243054\n",
            "Train Epoch: 28 [67840/73472 (92%)]\tLoss: 2.305206\n",
            "Train Epoch: 28 [69120/73472 (94%)]\tLoss: 2.259028\n",
            "Train Epoch: 28 [70400/73472 (96%)]\tLoss: 2.254608\n",
            "Train Epoch: 28 [71680/73472 (98%)]\tLoss: 2.297027\n",
            "Train Epoch: 28 [72960/73472 (99%)]\tLoss: 2.347840\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. . . . '' i , he the <unknown> , '' . , '' \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he the to to the feet . he he were <unknown> of to , \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was to the air . . . he . <unknown> . \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' , '' . `` you n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be a right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he could not and was up to the he . his . \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore was was was to . he 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\tto , he 'm going out <unknown> . . '' you , '' you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas the the <unknown> . and the <unknown> , the . '' , to potter \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he . he said have been to <unknown> of the <unknown> . '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he his to the . . . head . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> , . of the . `` a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had . been out the <unknown> . and that he had see \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3365, Accuracy: 334223/549120 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/028.pt\n",
            "\n",
            "generated sample\t harry potter  , i did ... '' so she backed up . then downstairs had ever sit\n",
            "generated sample\t harry potter  humming <unknown> , not nothing to you . dumbledore was the sides has all over\n",
            "generated sample\t harry potter  . we 'd <unknown> enough ... oh cornelius you charmed by sinus ! '' whispered\n",
            "generated sample\t harry potter  . winky soar , he was an very much thing to than the tower .\n",
            "generated sample\t harry potter  and being regretting it bowed straight . . dumbledore waited preferred . there could have\n",
            "generated sample\t harry potter  for concentrating under a fact that we saw . `` you expect a right too\n",
            "generated sample\t harry potter  . but he rushing times - '' there was perfectly long question to letter and\n",
            "generated sample\t harry potter  , '' said harry . `` you gettin ' got a important really foreseen in\n",
            "generated sample\t harry potter  . '' 'i were in a closely swelled , it say , as harry piles\n",
            "generated sample\t harry potter  . out for either , for this , i make it coming in story that\n",
            "generated beam\t\t harry potter  in one <unknown> but at hogwarts ... he came here . he was trying to\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 29 [0/73472 (0%)]\tLoss: 2.698409\n",
            "Train Epoch: 29 [1280/73472 (2%)]\tLoss: 2.253794\n",
            "Train Epoch: 29 [2560/73472 (3%)]\tLoss: 2.281315\n",
            "Train Epoch: 29 [3840/73472 (5%)]\tLoss: 2.302286\n",
            "Train Epoch: 29 [5120/73472 (7%)]\tLoss: 2.275403\n",
            "Train Epoch: 29 [6400/73472 (9%)]\tLoss: 2.249704\n",
            "Train Epoch: 29 [7680/73472 (10%)]\tLoss: 2.315452\n",
            "Train Epoch: 29 [8960/73472 (12%)]\tLoss: 2.284050\n",
            "Train Epoch: 29 [10240/73472 (14%)]\tLoss: 2.302453\n",
            "Train Epoch: 29 [11520/73472 (16%)]\tLoss: 2.275869\n",
            "Train Epoch: 29 [12800/73472 (17%)]\tLoss: 2.285066\n",
            "Train Epoch: 29 [14080/73472 (19%)]\tLoss: 2.222239\n",
            "Train Epoch: 29 [15360/73472 (21%)]\tLoss: 2.273242\n",
            "Train Epoch: 29 [16640/73472 (23%)]\tLoss: 2.298766\n",
            "Train Epoch: 29 [17920/73472 (24%)]\tLoss: 2.302765\n",
            "Train Epoch: 29 [19200/73472 (26%)]\tLoss: 2.337492\n",
            "Train Epoch: 29 [20480/73472 (28%)]\tLoss: 2.330998\n",
            "Train Epoch: 29 [21760/73472 (30%)]\tLoss: 2.330565\n",
            "Train Epoch: 29 [23040/73472 (31%)]\tLoss: 2.328315\n",
            "Train Epoch: 29 [24320/73472 (33%)]\tLoss: 2.275478\n",
            "Train Epoch: 29 [25600/73472 (35%)]\tLoss: 2.252863\n",
            "Train Epoch: 29 [26880/73472 (37%)]\tLoss: 2.271333\n",
            "Train Epoch: 29 [28160/73472 (38%)]\tLoss: 2.252176\n",
            "Train Epoch: 29 [29440/73472 (40%)]\tLoss: 2.269769\n",
            "Train Epoch: 29 [30720/73472 (42%)]\tLoss: 2.230275\n",
            "Train Epoch: 29 [32000/73472 (44%)]\tLoss: 2.250969\n",
            "Train Epoch: 29 [33280/73472 (45%)]\tLoss: 2.327182\n",
            "Train Epoch: 29 [34560/73472 (47%)]\tLoss: 2.283534\n",
            "Train Epoch: 29 [35840/73472 (49%)]\tLoss: 2.309989\n",
            "Train Epoch: 29 [37120/73472 (51%)]\tLoss: 2.220665\n",
            "Train Epoch: 29 [38400/73472 (52%)]\tLoss: 2.261048\n",
            "Train Epoch: 29 [39680/73472 (54%)]\tLoss: 2.233089\n",
            "Train Epoch: 29 [40960/73472 (56%)]\tLoss: 2.232529\n",
            "Train Epoch: 29 [42240/73472 (57%)]\tLoss: 2.300784\n",
            "Train Epoch: 29 [43520/73472 (59%)]\tLoss: 2.275692\n",
            "Train Epoch: 29 [44800/73472 (61%)]\tLoss: 2.161582\n",
            "Train Epoch: 29 [46080/73472 (63%)]\tLoss: 2.236614\n",
            "Train Epoch: 29 [47360/73472 (64%)]\tLoss: 2.226070\n",
            "Train Epoch: 29 [48640/73472 (66%)]\tLoss: 2.240149\n",
            "Train Epoch: 29 [49920/73472 (68%)]\tLoss: 2.323022\n",
            "Train Epoch: 29 [51200/73472 (70%)]\tLoss: 2.242244\n",
            "Train Epoch: 29 [52480/73472 (71%)]\tLoss: 2.215142\n",
            "Train Epoch: 29 [53760/73472 (73%)]\tLoss: 2.253423\n",
            "Train Epoch: 29 [55040/73472 (75%)]\tLoss: 2.270281\n",
            "Train Epoch: 29 [56320/73472 (77%)]\tLoss: 2.340741\n",
            "Train Epoch: 29 [57600/73472 (78%)]\tLoss: 2.283719\n",
            "Train Epoch: 29 [58880/73472 (80%)]\tLoss: 2.246188\n",
            "Train Epoch: 29 [60160/73472 (82%)]\tLoss: 2.247346\n",
            "Train Epoch: 29 [61440/73472 (84%)]\tLoss: 2.249605\n",
            "Train Epoch: 29 [62720/73472 (85%)]\tLoss: 2.199762\n",
            "Train Epoch: 29 [64000/73472 (87%)]\tLoss: 2.275517\n",
            "Train Epoch: 29 [65280/73472 (89%)]\tLoss: 2.231958\n",
            "Train Epoch: 29 [66560/73472 (91%)]\tLoss: 2.238554\n",
            "Train Epoch: 29 [67840/73472 (92%)]\tLoss: 2.302183\n",
            "Train Epoch: 29 [69120/73472 (94%)]\tLoss: 2.258704\n",
            "Train Epoch: 29 [70400/73472 (96%)]\tLoss: 2.254349\n",
            "Train Epoch: 29 [71680/73472 (98%)]\tLoss: 2.295623\n",
            "Train Epoch: 29 [72960/73472 (99%)]\tLoss: 2.345616\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. . . . . he , he the <unknown> , harry , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he him to to the feet . he he could <unknown> of to to \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. `` <unknown> <unknown> was to the air . . . he . <unknown> . \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' well , '' . '' . `` i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's not to be a right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he could not and was up to the he . his . \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore was was was to . i 'm n't know of \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\tto , he 'm going out <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas the the <unknown> . and the <unknown> . the . '' , to 's \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he . '' <unknown> have been to <unknown> of the <unknown> . '' \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\tthe wand . he his to the . . . head . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, <unknown> , . of the . `` a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had . been out to <unknown> . and that he were see \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3353, Accuracy: 334248/549120 (61%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/029.pt\n",
            "\n",
            "generated sample\t harry potter  to top their lane into father ... . ron could see cases like him to\n",
            "generated sample\t harry potter  . '' `` where much you are , '' he said , d.a. . those\n",
            "generated sample\t harry potter  from the third <unknown> . containing forward to weasley he could n't have leaning able\n",
            "generated sample\t harry potter  in black . ginny merely ca . even that only something <unknown> . she got\n",
            "generated sample\t harry potter  behind them ? it stopped right ... ! and you did n't taken your chicken\n",
            "generated sample\t harry potter  to really <unknown> how much they all with umbridge - . '' harry lost by\n",
            "generated sample\t harry potter  from their room , between -- come . i '' he told him instead ,\n",
            "generated sample\t harry potter  . '' he was lingering again . cho involve . `` oh . <unknown> !\n",
            "generated sample\t harry potter  ! today , dimly for a contents . i want to suit this . ''\n",
            "generated sample\t harry potter  that , did n't trouble , which should be an shorter <unknown> . `` trouble\n",
            "generated beam\t\t harry potter  . ... '' <unknown> out of the floor wall . `` you do n't though\n",
            "\n",
            "enumerate train   574\n",
            "Train Epoch: 30 [0/73472 (0%)]\tLoss: 2.695664\n",
            "Train Epoch: 30 [1280/73472 (2%)]\tLoss: 2.249668\n",
            "Train Epoch: 30 [2560/73472 (3%)]\tLoss: 2.279230\n",
            "Train Epoch: 30 [3840/73472 (5%)]\tLoss: 2.309242\n",
            "Train Epoch: 30 [5120/73472 (7%)]\tLoss: 2.272777\n",
            "Train Epoch: 30 [6400/73472 (9%)]\tLoss: 2.243735\n",
            "Train Epoch: 30 [7680/73472 (10%)]\tLoss: 2.311809\n",
            "Train Epoch: 30 [8960/73472 (12%)]\tLoss: 2.282888\n",
            "Train Epoch: 30 [10240/73472 (14%)]\tLoss: 2.300080\n",
            "Train Epoch: 30 [11520/73472 (16%)]\tLoss: 2.274551\n",
            "Train Epoch: 30 [12800/73472 (17%)]\tLoss: 2.276728\n",
            "Train Epoch: 30 [14080/73472 (19%)]\tLoss: 2.220620\n",
            "Train Epoch: 30 [15360/73472 (21%)]\tLoss: 2.271568\n",
            "Train Epoch: 30 [16640/73472 (23%)]\tLoss: 2.302112\n",
            "Train Epoch: 30 [17920/73472 (24%)]\tLoss: 2.301708\n",
            "Train Epoch: 30 [19200/73472 (26%)]\tLoss: 2.334340\n",
            "Train Epoch: 30 [20480/73472 (28%)]\tLoss: 2.325560\n",
            "Train Epoch: 30 [21760/73472 (30%)]\tLoss: 2.328494\n",
            "Train Epoch: 30 [23040/73472 (31%)]\tLoss: 2.327605\n",
            "Train Epoch: 30 [24320/73472 (33%)]\tLoss: 2.269520\n",
            "Train Epoch: 30 [25600/73472 (35%)]\tLoss: 2.252737\n",
            "Train Epoch: 30 [26880/73472 (37%)]\tLoss: 2.272194\n",
            "Train Epoch: 30 [28160/73472 (38%)]\tLoss: 2.252687\n",
            "Train Epoch: 30 [29440/73472 (40%)]\tLoss: 2.266667\n",
            "Train Epoch: 30 [30720/73472 (42%)]\tLoss: 2.229558\n",
            "Train Epoch: 30 [32000/73472 (44%)]\tLoss: 2.247214\n",
            "Train Epoch: 30 [33280/73472 (45%)]\tLoss: 2.323470\n",
            "Train Epoch: 30 [34560/73472 (47%)]\tLoss: 2.279956\n",
            "Train Epoch: 30 [35840/73472 (49%)]\tLoss: 2.307448\n",
            "Train Epoch: 30 [37120/73472 (51%)]\tLoss: 2.214165\n",
            "Train Epoch: 30 [38400/73472 (52%)]\tLoss: 2.256869\n",
            "Train Epoch: 30 [39680/73472 (54%)]\tLoss: 2.235709\n",
            "Train Epoch: 30 [40960/73472 (56%)]\tLoss: 2.232272\n",
            "Train Epoch: 30 [42240/73472 (57%)]\tLoss: 2.297342\n",
            "Train Epoch: 30 [43520/73472 (59%)]\tLoss: 2.273950\n",
            "Train Epoch: 30 [44800/73472 (61%)]\tLoss: 2.160283\n",
            "Train Epoch: 30 [46080/73472 (63%)]\tLoss: 2.236343\n",
            "Train Epoch: 30 [47360/73472 (64%)]\tLoss: 2.223271\n",
            "Train Epoch: 30 [48640/73472 (66%)]\tLoss: 2.238524\n",
            "Train Epoch: 30 [49920/73472 (68%)]\tLoss: 2.317927\n",
            "Train Epoch: 30 [51200/73472 (70%)]\tLoss: 2.241338\n",
            "Train Epoch: 30 [52480/73472 (71%)]\tLoss: 2.215038\n",
            "Train Epoch: 30 [53760/73472 (73%)]\tLoss: 2.253028\n",
            "Train Epoch: 30 [55040/73472 (75%)]\tLoss: 2.267170\n",
            "Train Epoch: 30 [56320/73472 (77%)]\tLoss: 2.334135\n",
            "Train Epoch: 30 [57600/73472 (78%)]\tLoss: 2.282573\n",
            "Train Epoch: 30 [58880/73472 (80%)]\tLoss: 2.244267\n",
            "Train Epoch: 30 [60160/73472 (82%)]\tLoss: 2.243989\n",
            "Train Epoch: 30 [61440/73472 (84%)]\tLoss: 2.245369\n",
            "Train Epoch: 30 [62720/73472 (85%)]\tLoss: 2.197980\n",
            "Train Epoch: 30 [64000/73472 (87%)]\tLoss: 2.273563\n",
            "Train Epoch: 30 [65280/73472 (89%)]\tLoss: 2.231945\n",
            "Train Epoch: 30 [66560/73472 (91%)]\tLoss: 2.238199\n",
            "Train Epoch: 30 [67840/73472 (92%)]\tLoss: 2.298702\n",
            "Train Epoch: 30 [69120/73472 (94%)]\tLoss: 2.259299\n",
            "Train Epoch: 30 [70400/73472 (96%)]\tLoss: 2.254301\n",
            "Train Epoch: 30 [71680/73472 (98%)]\tLoss: 2.293135\n",
            "Train Epoch: 30 [72960/73472 (99%)]\tLoss: 2.345947\n",
            "enumerate test    143\n",
            "Input\t voldemorts shattered soul ... but then , through the darkness , fire erupted : crimson\n",
            "GT\tvoldemorts shattered soul ... but then , through the darkness , fire erupted : crimson \n",
            "pred\t. . . . . i , he the <unknown> , he , , the \n",
            "\n",
            "\n",
            "Input\t to <unknown> the fire in which they were enclosed . . . . dumbledore scooped\n",
            "GT\tto <unknown> the fire in which they were enclosed . . . . dumbledore scooped \n",
            "pred\tand the the <unknown> . the he had <unknown> . `` . . . was \n",
            "\n",
            "\n",
            "Input\t and helped him back to his seat . once they were both safely jammed inside\n",
            "GT\tand helped him back to his seat . once they were both safely jammed inside \n",
            "pred\tto he him to to the feet . he he could <unknown> of to to \n",
            "\n",
            "\n",
            "Input\t . the little boat sank into the water once more ; clanking and tinkling ,\n",
            "GT\t. the little boat sank into the water once more ; clanking and tinkling , \n",
            "pred\t. he <unknown> <unknown> was to the air . . . he . <unknown> . \n",
            "\n",
            "\n",
            "Input\t , very well , harry . ... '' `` do n't talk now , ''\n",
            "GT\t, very well , harry . ... '' `` do n't talk now , '' \n",
            "pred\t, '' <unknown> , '' . '' '' `` i n't you to , '' \n",
            "\n",
            "\n",
            "Input\t it 's going to be all right , sir , '' harry said over and\n",
            "GT\tit 's going to be all right , sir , '' harry said over and \n",
            "pred\tyou 's a to be a right , '' , '' said said , . \n",
            "\n",
            "\n",
            "Input\t as tightly as he could , he stepped forwards into that feeling of horrible <unknown>\n",
            "GT\tas tightly as he could , he stepped forwards into that feeling of horrible <unknown> \n",
            "pred\t. he as he could not he was to to the he . his . \n",
            "\n",
            "\n",
            "Input\t his <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler\n",
            "GT\this <unknown> apparition had thrown dumbledore <unknown> ; then he saw his face , paler \n",
            "pred\the <unknown> was was been him 's . he he was his wand , and \n",
            "\n",
            "\n",
            "Input\t 'it is ... professor snape whom i need ... but i do not think ...\n",
            "GT\t'it is ... professor snape whom i need ... but i do not think ... \n",
            "pred\t`` , a he dumbledore was he was to . i 'm n't know he \n",
            "\n",
            "\n",
            "Input\t apparate as i was pulling my bedroom curtains ! thank goodness , thank goodness ,\n",
            "GT\tapparate as i was pulling my bedroom curtains ! thank goodness , thank goodness , \n",
            "pred\tto , he was going out <unknown> . . '' you , i you , \n",
            "\n",
            "\n",
            "Input\t pointed into the sky , in the direction of hogwarts . dread flooded harry at\n",
            "GT\tpointed into the sky , in the direction of hogwarts . dread flooded harry at \n",
            "pred\twas him the <unknown> . and the <unknown> , the , '' , to 's \n",
            "\n",
            "\n",
            "Input\t transport - brooms - ' 'i 've got a couple behind the bar , '\n",
            "GT\ttransport - brooms - ' 'i 've got a couple behind the bar , ' \n",
            "pred\tto to he . '' <unknown> have been to <unknown> of the <unknown> . he \n",
            "\n",
            "\n",
            "Input\t his pocket and threw it over himself before mounting his broom ; madam rosmerta was\n",
            "GT\this pocket and threw it over himself before mounting his broom ; madam rosmerta was \n",
            "pred\this wand . he his to the . he . head . he pomfrey was \n",
            "\n",
            "\n",
            "Input\t 's luck run out by now ? was it one of them who had caused\n",
            "GT\t's luck run out by now ? was it one of them who had caused \n",
            "pred\t, eyes was . of the . `` a to of the , had been \n",
            "\n",
            "\n",
            "Input\t the enchantments he himself had set around the castle , so that they could enter\n",
            "GT\tthe enchantments he himself had set around the castle , so that they could enter \n",
            "pred\t. <unknown> of had . been out to <unknown> . and that he had be \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.3366, Accuracy: 333947/549120 (61%)\n",
            "\n",
            "generated sample\t harry potter  ... `` <unknown> ! ' class , well ! '' ron yelled . `` that\n",
            "generated sample\t harry potter  or <unknown> . you 're very good about your school ! '' harry pretence harry\n",
            "generated sample\t harry potter  . and you 'll <unknown> that believed i was anxious . unless they persuade your\n",
            "generated sample\t harry potter  . '' a answer dress . harry had to say cheerfully . he did ,\n",
            "generated sample\t harry potter  . '' harry and pinching made her to snape to dock the long ; hagrid\n",
            "generated sample\t harry potter  , and smugness people 've ... . severus i did n't know about any excitement\n",
            "generated sample\t harry potter  . '' he could not dangerous twist hard ... . he stood up to the\n",
            "generated sample\t harry potter  . harry was holding their way quite <unknown> ; where malfoy had been cardboard to\n",
            "generated sample\t harry potter  . this pomfrey pressed across up of the way for a purpose , tattered !\n",
            "generated sample\t harry potter  . '' `` you do n't the krum 's dear , '' said dumbledore ,\n",
            "generated beam\t\t harry potter  , yeh gliding across them to divide . hermione had to picked a recently pleased\n",
            "\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss_punct/030.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M19pb-BdjjkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 41946
        },
        "outputId": "6350c0a0-164b-4902-827c-c9668f725554"
      },
      "cell_type": "code",
      "source": [
        "## Without PUNCTUATION\n",
        "SEQUENCE_LENGTH = 30\n",
        "BATCH_SIZE = 128\n",
        "FEATURE_SIZE = 512\n",
        "TEST_BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs_wordss/log.pkl'\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "data_train = HarryPotterWordDataset(DATA_PATH + 'harry_potter_word_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = HarryPotterWordDataset(DATA_PATH + 'harry_potter_word_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = HarryPotterWordNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "# Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "# We will talk more about different optimization methods in class.\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints_wordss')\n",
        "\n",
        "train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(model, device, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints_wordss/%03d.pt' % epoch)\n",
        "        seed_words = 'Harry Potter '\n",
        "        for ii in range(10):\n",
        "            generated_sentence = generate_language(model, device, seed_words, 30, vocab, 'sample')\n",
        "            print('generated sample\\t', generated_sentence)\n",
        "        generated_sentence = generate_language(model, device, seed_words, 30, vocab, 'beam')\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    model.save_model(DATA_PATH + 'checkpoints_wordss/%03d.pt' % epoch, 0)\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "Restoring:\n",
            "encoder.weight -> \ttorch.Size([8826, 512]) = 18MB\n",
            "gru.weight_ih_l0 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.weight_hh_l0 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.bias_ih_l0 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.bias_hh_l0 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.weight_ih_l1 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.weight_hh_l1 -> \ttorch.Size([1536, 512]) = 3MB\n",
            "gru.bias_ih_l1 -> \ttorch.Size([1536]) = 0MB\n",
            "gru.bias_hh_l1 -> \ttorch.Size([1536]) = 0MB\n",
            "decoder.weight -> \ttorch.Size([8826, 512]) = 18MB\n",
            "decoder.bias -> \ttorch.Size([8826]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/013.pt\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and the and to the room table and he the <unknown> and the world and\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of and been to had hermione had <unknown> in <unknown> and the world\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t his the castle and was and said know to be a right said you about\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the room arts and the and the and and the <unknown> and and the and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> and and <unknown> of eaters had the them and were been the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and and and them <unknown> to the <unknown> of harry i they had the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of of and eyes and the the and and the face he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the room and had still and <unknown> and had had been out the <unknown> and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t i i i i me i he been to i me to be and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had that and and to the <unknown> and the floor of the room and harry\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was a and the <unknown> and i are been it\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t it course harry i <unknown> than you what he do it said do know harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7340, Accuracy: 245409/430080 (57%)\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 13 [0/57344 (0%)]\tLoss: 2.896113\n",
            "Train Epoch: 13 [1280/57344 (2%)]\tLoss: 2.744953\n",
            "Train Epoch: 13 [2560/57344 (4%)]\tLoss: 2.758263\n",
            "Train Epoch: 13 [3840/57344 (7%)]\tLoss: 2.742844\n",
            "Train Epoch: 13 [5120/57344 (9%)]\tLoss: 2.695429\n",
            "Train Epoch: 13 [6400/57344 (11%)]\tLoss: 2.685828\n",
            "Train Epoch: 13 [7680/57344 (13%)]\tLoss: 2.699373\n",
            "Train Epoch: 13 [8960/57344 (16%)]\tLoss: 2.735604\n",
            "Train Epoch: 13 [10240/57344 (18%)]\tLoss: 2.718533\n",
            "Train Epoch: 13 [11520/57344 (20%)]\tLoss: 2.625089\n",
            "Train Epoch: 13 [12800/57344 (22%)]\tLoss: 2.649931\n",
            "Train Epoch: 13 [14080/57344 (25%)]\tLoss: 2.696243\n",
            "Train Epoch: 13 [15360/57344 (27%)]\tLoss: 2.652065\n",
            "Train Epoch: 13 [16640/57344 (29%)]\tLoss: 2.626372\n",
            "Train Epoch: 13 [17920/57344 (31%)]\tLoss: 2.620573\n",
            "Train Epoch: 13 [19200/57344 (33%)]\tLoss: 2.666321\n",
            "Train Epoch: 13 [20480/57344 (36%)]\tLoss: 2.700424\n",
            "Train Epoch: 13 [21760/57344 (38%)]\tLoss: 2.632189\n",
            "Train Epoch: 13 [23040/57344 (40%)]\tLoss: 2.719441\n",
            "Train Epoch: 13 [24320/57344 (42%)]\tLoss: 2.739330\n",
            "Train Epoch: 13 [25600/57344 (45%)]\tLoss: 2.687642\n",
            "Train Epoch: 13 [26880/57344 (47%)]\tLoss: 2.649804\n",
            "Train Epoch: 13 [28160/57344 (49%)]\tLoss: 2.652977\n",
            "Train Epoch: 13 [29440/57344 (51%)]\tLoss: 2.674070\n",
            "Train Epoch: 13 [30720/57344 (54%)]\tLoss: 2.619102\n",
            "Train Epoch: 13 [32000/57344 (56%)]\tLoss: 2.636312\n",
            "Train Epoch: 13 [33280/57344 (58%)]\tLoss: 2.643725\n",
            "Train Epoch: 13 [34560/57344 (60%)]\tLoss: 2.635522\n",
            "Train Epoch: 13 [35840/57344 (62%)]\tLoss: 2.699578\n",
            "Train Epoch: 13 [37120/57344 (65%)]\tLoss: 2.665638\n",
            "Train Epoch: 13 [38400/57344 (67%)]\tLoss: 2.685178\n",
            "Train Epoch: 13 [39680/57344 (69%)]\tLoss: 2.649132\n",
            "Train Epoch: 13 [40960/57344 (71%)]\tLoss: 2.664667\n",
            "Train Epoch: 13 [42240/57344 (74%)]\tLoss: 2.733887\n",
            "Train Epoch: 13 [43520/57344 (76%)]\tLoss: 2.690536\n",
            "Train Epoch: 13 [44800/57344 (78%)]\tLoss: 2.691188\n",
            "Train Epoch: 13 [46080/57344 (80%)]\tLoss: 2.725784\n",
            "Train Epoch: 13 [47360/57344 (83%)]\tLoss: 2.619912\n",
            "Train Epoch: 13 [48640/57344 (85%)]\tLoss: 2.637693\n",
            "Train Epoch: 13 [49920/57344 (87%)]\tLoss: 2.638304\n",
            "Train Epoch: 13 [51200/57344 (89%)]\tLoss: 2.627512\n",
            "Train Epoch: 13 [52480/57344 (92%)]\tLoss: 2.627690\n",
            "Train Epoch: 13 [53760/57344 (94%)]\tLoss: 2.648249\n",
            "Train Epoch: 13 [55040/57344 (96%)]\tLoss: 2.703168\n",
            "Train Epoch: 13 [56320/57344 (98%)]\tLoss: 2.690807\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and the and to the air room of he the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in a to was ron was still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large <unknown> and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a large of and <unknown> of eaters and the the he were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of said him <unknown> to the <unknown> of harry <unknown> they was the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still to <unknown> of was to a off the door to\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he i he he him he he you to i him to the said you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t was it he and into the <unknown> of the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was a to his head and harry were said it\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t it the harry i <unknown> and you what you do it said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7318, Accuracy: 244656/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/013.pt\n",
            "\n",
            "generated sample\t harry potter  he hagrid said ron he looked as though he had been so he had been\n",
            "generated sample\t harry potter  harry he said dumbledore you were there you were a very good said harry i\n",
            "generated sample\t harry potter  harry hermione did not to go to the <unknown> of the <unknown> of a couple\n",
            "generated sample\t harry potter  he i have a wizard said harry he was to be a very cold and\n",
            "generated sample\t harry potter i have a <unknown> of the ministry have a <unknown> said harry but he was \n",
            "generated sample\t harry potter harry i need to go to get in the world i ever ca see you \n",
            "generated sample\t harry potter   harry and ron and hermione were looking at harry and hermione and ron gave \n",
            "generated sample\t harry potter  what i been just said harry but i know it was a <unknown> of the\n",
            "generated sample\t harry potter   that it was a very <unknown> and it was a <unknown> <unknown> <unknown> in \n",
            "generated sample\t harry potter  saidharryharry harry and hermione went to harry and ron and ron and hermione said\n",
            "generated beam\t\t harry potter  i i have to make it at all there was a <unknown> of his death\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 14 [0/57344 (0%)]\tLoss: 2.898393\n",
            "Train Epoch: 14 [1280/57344 (2%)]\tLoss: 2.691561\n",
            "Train Epoch: 14 [2560/57344 (4%)]\tLoss: 2.712975\n",
            "Train Epoch: 14 [3840/57344 (7%)]\tLoss: 2.693165\n",
            "Train Epoch: 14 [5120/57344 (9%)]\tLoss: 2.659650\n",
            "Train Epoch: 14 [6400/57344 (11%)]\tLoss: 2.654320\n",
            "Train Epoch: 14 [7680/57344 (13%)]\tLoss: 2.670378\n",
            "Train Epoch: 14 [8960/57344 (16%)]\tLoss: 2.711839\n",
            "Train Epoch: 14 [10240/57344 (18%)]\tLoss: 2.701032\n",
            "Train Epoch: 14 [11520/57344 (20%)]\tLoss: 2.612098\n",
            "Train Epoch: 14 [12800/57344 (22%)]\tLoss: 2.633235\n",
            "Train Epoch: 14 [14080/57344 (25%)]\tLoss: 2.683090\n",
            "Train Epoch: 14 [15360/57344 (27%)]\tLoss: 2.645595\n",
            "Train Epoch: 14 [16640/57344 (29%)]\tLoss: 2.614242\n",
            "Train Epoch: 14 [17920/57344 (31%)]\tLoss: 2.614727\n",
            "Train Epoch: 14 [19200/57344 (33%)]\tLoss: 2.660210\n",
            "Train Epoch: 14 [20480/57344 (36%)]\tLoss: 2.689197\n",
            "Train Epoch: 14 [21760/57344 (38%)]\tLoss: 2.622066\n",
            "Train Epoch: 14 [23040/57344 (40%)]\tLoss: 2.712411\n",
            "Train Epoch: 14 [24320/57344 (42%)]\tLoss: 2.735105\n",
            "Train Epoch: 14 [25600/57344 (45%)]\tLoss: 2.682398\n",
            "Train Epoch: 14 [26880/57344 (47%)]\tLoss: 2.644040\n",
            "Train Epoch: 14 [28160/57344 (49%)]\tLoss: 2.650989\n",
            "Train Epoch: 14 [29440/57344 (51%)]\tLoss: 2.672819\n",
            "Train Epoch: 14 [30720/57344 (54%)]\tLoss: 2.618199\n",
            "Train Epoch: 14 [32000/57344 (56%)]\tLoss: 2.635548\n",
            "Train Epoch: 14 [33280/57344 (58%)]\tLoss: 2.635420\n",
            "Train Epoch: 14 [34560/57344 (60%)]\tLoss: 2.628530\n",
            "Train Epoch: 14 [35840/57344 (62%)]\tLoss: 2.693151\n",
            "Train Epoch: 14 [37120/57344 (65%)]\tLoss: 2.659054\n",
            "Train Epoch: 14 [38400/57344 (67%)]\tLoss: 2.681146\n",
            "Train Epoch: 14 [39680/57344 (69%)]\tLoss: 2.644965\n",
            "Train Epoch: 14 [40960/57344 (71%)]\tLoss: 2.659845\n",
            "Train Epoch: 14 [42240/57344 (74%)]\tLoss: 2.728987\n",
            "Train Epoch: 14 [43520/57344 (76%)]\tLoss: 2.680810\n",
            "Train Epoch: 14 [44800/57344 (78%)]\tLoss: 2.685178\n",
            "Train Epoch: 14 [46080/57344 (80%)]\tLoss: 2.716802\n",
            "Train Epoch: 14 [47360/57344 (83%)]\tLoss: 2.611866\n",
            "Train Epoch: 14 [48640/57344 (85%)]\tLoss: 2.628550\n",
            "Train Epoch: 14 [49920/57344 (87%)]\tLoss: 2.633169\n",
            "Train Epoch: 14 [51200/57344 (89%)]\tLoss: 2.620963\n",
            "Train Epoch: 14 [52480/57344 (92%)]\tLoss: 2.623566\n",
            "Train Epoch: 14 [53760/57344 (94%)]\tLoss: 2.641475\n",
            "Train Epoch: 14 [55040/57344 (96%)]\tLoss: 2.700466\n",
            "Train Epoch: 14 [56320/57344 (98%)]\tLoss: 2.691408\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a large of and <unknown> of eaters and the the harry were to the large\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of said him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head and he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still to <unknown> of was was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he i he he him he he you to i him to the said you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t was the he and into the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry and was still to the head and harry were got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t said the harry and <unknown> and you what you do said said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7290, Accuracy: 244967/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/014.pt\n",
            "\n",
            "generated sample\t harry potter harry and hermione and ron and hermione looked at the ceiling of the air and \n",
            "generated sample\t harry potter i do want to be quiet and i do want to be a <unknown> to \n",
            "generated sample\t harry potter  harry harry asked hermione i do know what he got to the ministry of magic\n",
            "generated sample\t harry potter  what he was doing that he was wearing his hand and harry and hermione were\n",
            "generated sample\t harry potter  harry said harry i know what he said the door was what about ron said\n",
            "generated sample\t harry potter  harry i think it is i going to be to be a new time i\n",
            "generated sample\t harry potter   harry it was a <unknown> <unknown> of a <unknown> of his hair and a \n",
            "generated sample\t harry potter   the ministry of magic said harry and ron did have to go and then \n",
            "generated sample\t harry potter  harry harry and hermione walked down the air as they reached their heads into the\n",
            "generated sample\t harry potter  but we are to go and with a <unknown> of the <unknown> harry said hermione\n",
            "generated beam\t\t harry potter  something what i think to be a bit of the ministry of the <unknown> i\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 15 [0/57344 (0%)]\tLoss: 2.899544\n",
            "Train Epoch: 15 [1280/57344 (2%)]\tLoss: 2.687825\n",
            "Train Epoch: 15 [2560/57344 (4%)]\tLoss: 2.711598\n",
            "Train Epoch: 15 [3840/57344 (7%)]\tLoss: 2.684158\n",
            "Train Epoch: 15 [5120/57344 (9%)]\tLoss: 2.660974\n",
            "Train Epoch: 15 [6400/57344 (11%)]\tLoss: 2.655190\n",
            "Train Epoch: 15 [7680/57344 (13%)]\tLoss: 2.664734\n",
            "Train Epoch: 15 [8960/57344 (16%)]\tLoss: 2.710556\n",
            "Train Epoch: 15 [10240/57344 (18%)]\tLoss: 2.700315\n",
            "Train Epoch: 15 [11520/57344 (20%)]\tLoss: 2.607923\n",
            "Train Epoch: 15 [12800/57344 (22%)]\tLoss: 2.628465\n",
            "Train Epoch: 15 [14080/57344 (25%)]\tLoss: 2.677637\n",
            "Train Epoch: 15 [15360/57344 (27%)]\tLoss: 2.642118\n",
            "Train Epoch: 15 [16640/57344 (29%)]\tLoss: 2.609623\n",
            "Train Epoch: 15 [17920/57344 (31%)]\tLoss: 2.609276\n",
            "Train Epoch: 15 [19200/57344 (33%)]\tLoss: 2.652598\n",
            "Train Epoch: 15 [20480/57344 (36%)]\tLoss: 2.689791\n",
            "Train Epoch: 15 [21760/57344 (38%)]\tLoss: 2.624699\n",
            "Train Epoch: 15 [23040/57344 (40%)]\tLoss: 2.712073\n",
            "Train Epoch: 15 [24320/57344 (42%)]\tLoss: 2.730021\n",
            "Train Epoch: 15 [25600/57344 (45%)]\tLoss: 2.681206\n",
            "Train Epoch: 15 [26880/57344 (47%)]\tLoss: 2.640656\n",
            "Train Epoch: 15 [28160/57344 (49%)]\tLoss: 2.648762\n",
            "Train Epoch: 15 [29440/57344 (51%)]\tLoss: 2.668476\n",
            "Train Epoch: 15 [30720/57344 (54%)]\tLoss: 2.613700\n",
            "Train Epoch: 15 [32000/57344 (56%)]\tLoss: 2.630193\n",
            "Train Epoch: 15 [33280/57344 (58%)]\tLoss: 2.634696\n",
            "Train Epoch: 15 [34560/57344 (60%)]\tLoss: 2.626814\n",
            "Train Epoch: 15 [35840/57344 (62%)]\tLoss: 2.691231\n",
            "Train Epoch: 15 [37120/57344 (65%)]\tLoss: 2.660391\n",
            "Train Epoch: 15 [38400/57344 (67%)]\tLoss: 2.683581\n",
            "Train Epoch: 15 [39680/57344 (69%)]\tLoss: 2.642916\n",
            "Train Epoch: 15 [40960/57344 (71%)]\tLoss: 2.660905\n",
            "Train Epoch: 15 [42240/57344 (74%)]\tLoss: 2.729609\n",
            "Train Epoch: 15 [43520/57344 (76%)]\tLoss: 2.680374\n",
            "Train Epoch: 15 [44800/57344 (78%)]\tLoss: 2.684866\n",
            "Train Epoch: 15 [46080/57344 (80%)]\tLoss: 2.712393\n",
            "Train Epoch: 15 [47360/57344 (83%)]\tLoss: 2.612721\n",
            "Train Epoch: 15 [48640/57344 (85%)]\tLoss: 2.626525\n",
            "Train Epoch: 15 [49920/57344 (87%)]\tLoss: 2.627055\n",
            "Train Epoch: 15 [51200/57344 (89%)]\tLoss: 2.619323\n",
            "Train Epoch: 15 [52480/57344 (92%)]\tLoss: 2.622478\n",
            "Train Epoch: 15 [53760/57344 (94%)]\tLoss: 2.638453\n",
            "Train Epoch: 15 [55040/57344 (96%)]\tLoss: 2.697340\n",
            "Train Epoch: 15 [56320/57344 (98%)]\tLoss: 2.686541\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in a to was ron was still in he to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a large of and <unknown> of eaters and the him he were to the large\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of he him <unknown> to the <unknown> of harry i they was the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still to <unknown> of had was been off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the to you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t was his he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to his head and harry were got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry i <unknown> and you what you do it said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7267, Accuracy: 244942/430080 (57%)\n",
            "\n",
            "generated sample\t harry potter   what he got to eat his eyes and his eyes had been wearing his \n",
            "generated sample\t harry potter it no said harry i told you and i think i think he got to \n",
            "generated sample\t harry potter  you it was a <unknown> to the <unknown> of the fat lady and the man\n",
            "generated sample\t harry potter  harry he was right to the door of the <unknown> of the air on the\n",
            "generated sample\t harry potter  you i do know what you are doing this in the hufflepuffs i got to\n",
            "generated sample\t harry potter  harry dumbledore had been in a very <unknown> but as though he had not heard\n",
            "generated sample\t harry potter  the the time he was wearing a large <unknown> of his <unknown> <unknown> and the\n",
            "generated sample\t harry potter  i do that i do want to come on the quidditch floor you know that\n",
            "generated sample\t harry potter  his his face he was wearing his voice he had already not to think he\n",
            "generated sample\t harry potter  said harry he could have just to think that he had been to be back\n",
            "generated beam\t\t harry potter  harry i do think i do want to get out of the door in the\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 16 [0/57344 (0%)]\tLoss: 2.898986\n",
            "Train Epoch: 16 [1280/57344 (2%)]\tLoss: 2.686961\n",
            "Train Epoch: 16 [2560/57344 (4%)]\tLoss: 2.710438\n",
            "Train Epoch: 16 [3840/57344 (7%)]\tLoss: 2.683677\n",
            "Train Epoch: 16 [5120/57344 (9%)]\tLoss: 2.658004\n",
            "Train Epoch: 16 [6400/57344 (11%)]\tLoss: 2.649545\n",
            "Train Epoch: 16 [7680/57344 (13%)]\tLoss: 2.665916\n",
            "Train Epoch: 16 [8960/57344 (16%)]\tLoss: 2.710474\n",
            "Train Epoch: 16 [10240/57344 (18%)]\tLoss: 2.694123\n",
            "Train Epoch: 16 [11520/57344 (20%)]\tLoss: 2.607891\n",
            "Train Epoch: 16 [12800/57344 (22%)]\tLoss: 2.624680\n",
            "Train Epoch: 16 [14080/57344 (25%)]\tLoss: 2.674641\n",
            "Train Epoch: 16 [15360/57344 (27%)]\tLoss: 2.638432\n",
            "Train Epoch: 16 [16640/57344 (29%)]\tLoss: 2.610735\n",
            "Train Epoch: 16 [17920/57344 (31%)]\tLoss: 2.606730\n",
            "Train Epoch: 16 [19200/57344 (33%)]\tLoss: 2.655011\n",
            "Train Epoch: 16 [20480/57344 (36%)]\tLoss: 2.686804\n",
            "Train Epoch: 16 [21760/57344 (38%)]\tLoss: 2.623789\n",
            "Train Epoch: 16 [23040/57344 (40%)]\tLoss: 2.712327\n",
            "Train Epoch: 16 [24320/57344 (42%)]\tLoss: 2.730376\n",
            "Train Epoch: 16 [25600/57344 (45%)]\tLoss: 2.680457\n",
            "Train Epoch: 16 [26880/57344 (47%)]\tLoss: 2.639666\n",
            "Train Epoch: 16 [28160/57344 (49%)]\tLoss: 2.649419\n",
            "Train Epoch: 16 [29440/57344 (51%)]\tLoss: 2.664468\n",
            "Train Epoch: 16 [30720/57344 (54%)]\tLoss: 2.611165\n",
            "Train Epoch: 16 [32000/57344 (56%)]\tLoss: 2.627490\n",
            "Train Epoch: 16 [33280/57344 (58%)]\tLoss: 2.631939\n",
            "Train Epoch: 16 [34560/57344 (60%)]\tLoss: 2.623315\n",
            "Train Epoch: 16 [35840/57344 (62%)]\tLoss: 2.691188\n",
            "Train Epoch: 16 [37120/57344 (65%)]\tLoss: 2.659554\n",
            "Train Epoch: 16 [38400/57344 (67%)]\tLoss: 2.681735\n",
            "Train Epoch: 16 [39680/57344 (69%)]\tLoss: 2.640251\n",
            "Train Epoch: 16 [40960/57344 (71%)]\tLoss: 2.658403\n",
            "Train Epoch: 16 [42240/57344 (74%)]\tLoss: 2.728041\n",
            "Train Epoch: 16 [43520/57344 (76%)]\tLoss: 2.675903\n",
            "Train Epoch: 16 [44800/57344 (78%)]\tLoss: 2.679196\n",
            "Train Epoch: 16 [46080/57344 (80%)]\tLoss: 2.710787\n",
            "Train Epoch: 16 [47360/57344 (83%)]\tLoss: 2.610108\n",
            "Train Epoch: 16 [48640/57344 (85%)]\tLoss: 2.624229\n",
            "Train Epoch: 16 [49920/57344 (87%)]\tLoss: 2.624911\n",
            "Train Epoch: 16 [51200/57344 (89%)]\tLoss: 2.617070\n",
            "Train Epoch: 16 [52480/57344 (92%)]\tLoss: 2.619604\n",
            "Train Epoch: 16 [53760/57344 (94%)]\tLoss: 2.636960\n",
            "Train Epoch: 16 [55040/57344 (96%)]\tLoss: 2.695008\n",
            "Train Epoch: 16 [56320/57344 (98%)]\tLoss: 2.683408\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron was still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the he were to the large\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of he him <unknown> to the <unknown> of harry he they was the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was wearing to <unknown> of had was been off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the to you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t was the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he were got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t said the harry and <unknown> and you what you do it said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7254, Accuracy: 245116/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/016.pt\n",
            "\n",
            "generated sample\t harry potter harry said harry and hermione saw the <unknown> of his eyes and looking around at \n",
            "generated sample\t harry potter  snape i do know you ca get it so said harry he was going to\n",
            "generated sample\t harry potter  harry i think it was the <unknown> of the boy and also i am not\n",
            "generated sample\t harry potter  hagrid he just still at the time he had to be a good good time\n",
            "generated sample\t harry potter  harry for a moment it was a little <unknown> and the <unknown> was <unknown> so\n",
            "generated sample\t harry potter  harry harry potter hermione said hermione who was very much to have a very good\n",
            "generated sample\t harry potter  he it was enough to <unknown> harry i think i do think i must know\n",
            "generated sample\t harry potter  harry harry was a <unknown> of his hair and his wand was a small <unknown>\n",
            "generated sample\t harry potter  i get a <unknown> harry i know i know you were there he was looking\n",
            "generated sample\t harry potter  said harry i do want to go to the cloak he said to ron and\n",
            "generated beam\t\t harry potter  he he said to harry ron and ron and goyle were still around there was\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 17 [0/57344 (0%)]\tLoss: 2.897551\n",
            "Train Epoch: 17 [1280/57344 (2%)]\tLoss: 2.686561\n",
            "Train Epoch: 17 [2560/57344 (4%)]\tLoss: 2.707341\n",
            "Train Epoch: 17 [3840/57344 (7%)]\tLoss: 2.682779\n",
            "Train Epoch: 17 [5120/57344 (9%)]\tLoss: 2.658348\n",
            "Train Epoch: 17 [6400/57344 (11%)]\tLoss: 2.646769\n",
            "Train Epoch: 17 [7680/57344 (13%)]\tLoss: 2.659299\n",
            "Train Epoch: 17 [8960/57344 (16%)]\tLoss: 2.707099\n",
            "Train Epoch: 17 [10240/57344 (18%)]\tLoss: 2.694853\n",
            "Train Epoch: 17 [11520/57344 (20%)]\tLoss: 2.604971\n",
            "Train Epoch: 17 [12800/57344 (22%)]\tLoss: 2.622716\n",
            "Train Epoch: 17 [14080/57344 (25%)]\tLoss: 2.676321\n",
            "Train Epoch: 17 [15360/57344 (27%)]\tLoss: 2.634295\n",
            "Train Epoch: 17 [16640/57344 (29%)]\tLoss: 2.605572\n",
            "Train Epoch: 17 [17920/57344 (31%)]\tLoss: 2.606053\n",
            "Train Epoch: 17 [19200/57344 (33%)]\tLoss: 2.651948\n",
            "Train Epoch: 17 [20480/57344 (36%)]\tLoss: 2.683158\n",
            "Train Epoch: 17 [21760/57344 (38%)]\tLoss: 2.619957\n",
            "Train Epoch: 17 [23040/57344 (40%)]\tLoss: 2.707165\n",
            "Train Epoch: 17 [24320/57344 (42%)]\tLoss: 2.725562\n",
            "Train Epoch: 17 [25600/57344 (45%)]\tLoss: 2.678068\n",
            "Train Epoch: 17 [26880/57344 (47%)]\tLoss: 2.637671\n",
            "Train Epoch: 17 [28160/57344 (49%)]\tLoss: 2.647432\n",
            "Train Epoch: 17 [29440/57344 (51%)]\tLoss: 2.662833\n",
            "Train Epoch: 17 [30720/57344 (54%)]\tLoss: 2.609001\n",
            "Train Epoch: 17 [32000/57344 (56%)]\tLoss: 2.623127\n",
            "Train Epoch: 17 [33280/57344 (58%)]\tLoss: 2.629755\n",
            "Train Epoch: 17 [34560/57344 (60%)]\tLoss: 2.620538\n",
            "Train Epoch: 17 [35840/57344 (62%)]\tLoss: 2.686845\n",
            "Train Epoch: 17 [37120/57344 (65%)]\tLoss: 2.654397\n",
            "Train Epoch: 17 [38400/57344 (67%)]\tLoss: 2.675225\n",
            "Train Epoch: 17 [39680/57344 (69%)]\tLoss: 2.641201\n",
            "Train Epoch: 17 [40960/57344 (71%)]\tLoss: 2.655400\n",
            "Train Epoch: 17 [42240/57344 (74%)]\tLoss: 2.725224\n",
            "Train Epoch: 17 [43520/57344 (76%)]\tLoss: 2.674357\n",
            "Train Epoch: 17 [44800/57344 (78%)]\tLoss: 2.678207\n",
            "Train Epoch: 17 [46080/57344 (80%)]\tLoss: 2.707471\n",
            "Train Epoch: 17 [47360/57344 (83%)]\tLoss: 2.608971\n",
            "Train Epoch: 17 [48640/57344 (85%)]\tLoss: 2.623563\n",
            "Train Epoch: 17 [49920/57344 (87%)]\tLoss: 2.623908\n",
            "Train Epoch: 17 [51200/57344 (89%)]\tLoss: 2.617473\n",
            "Train Epoch: 17 [52480/57344 (92%)]\tLoss: 2.616125\n",
            "Train Epoch: 17 [53760/57344 (94%)]\tLoss: 2.635479\n",
            "Train Epoch: 17 [55040/57344 (96%)]\tLoss: 2.693397\n",
            "Train Epoch: 17 [56320/57344 (98%)]\tLoss: 2.682426\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in a to was ron was still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the harry were to the large\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of he him <unknown> to the <unknown> of harry in they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head and he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was wearing to <unknown> of was was been off the door to\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t was the he and into the <unknown> of the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry and was still to the head and he were got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t said the ron and <unknown> and you what you do said said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7267, Accuracy: 245167/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/017.pt\n",
            "\n",
            "generated sample\t harry potter  harry no he said it was the walnut of the class had a very <unknown>\n",
            "generated sample\t harry potter  i i think i not <unknown> to think of course i do know said harry\n",
            "generated sample\t harry potter  harry no time i do know what you know said harry as they were lying\n",
            "generated sample\t harry potter  what he was a very very good i do said harry i do know what\n",
            "generated sample\t harry potter  harry hagrid was just to do his <unknown> and his head was going to think\n",
            "generated sample\t harry potter  it i never said harry he said ron you see it said harry and then\n",
            "generated sample\t harry potter  harry the death eaters had been wearing a <unknown> of a small <unknown> of her\n",
            "generated sample\t harry potter  i do it was a good said hermione angrily at harry and ron and hermione\n",
            "generated sample\t harry potter  harry he said the <unknown> of the <unknown> of the <unknown> of the <unknown> of\n",
            "generated sample\t harry potter  harry he said he had been very good to the <unknown> of the time he\n",
            "generated beam\t\t harry potter   harry said dumbledore but they were all back to the common room and harry \n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 18 [0/57344 (0%)]\tLoss: 2.898407\n",
            "Train Epoch: 18 [1280/57344 (2%)]\tLoss: 2.683030\n",
            "Train Epoch: 18 [2560/57344 (4%)]\tLoss: 2.704592\n",
            "Train Epoch: 18 [3840/57344 (7%)]\tLoss: 2.680967\n",
            "Train Epoch: 18 [5120/57344 (9%)]\tLoss: 2.655778\n",
            "Train Epoch: 18 [6400/57344 (11%)]\tLoss: 2.646648\n",
            "Train Epoch: 18 [7680/57344 (13%)]\tLoss: 2.661858\n",
            "Train Epoch: 18 [8960/57344 (16%)]\tLoss: 2.705291\n",
            "Train Epoch: 18 [10240/57344 (18%)]\tLoss: 2.690218\n",
            "Train Epoch: 18 [11520/57344 (20%)]\tLoss: 2.605067\n",
            "Train Epoch: 18 [12800/57344 (22%)]\tLoss: 2.620435\n",
            "Train Epoch: 18 [14080/57344 (25%)]\tLoss: 2.672452\n",
            "Train Epoch: 18 [15360/57344 (27%)]\tLoss: 2.632193\n",
            "Train Epoch: 18 [16640/57344 (29%)]\tLoss: 2.604578\n",
            "Train Epoch: 18 [17920/57344 (31%)]\tLoss: 2.606498\n",
            "Train Epoch: 18 [19200/57344 (33%)]\tLoss: 2.649565\n",
            "Train Epoch: 18 [20480/57344 (36%)]\tLoss: 2.683542\n",
            "Train Epoch: 18 [21760/57344 (38%)]\tLoss: 2.618402\n",
            "Train Epoch: 18 [23040/57344 (40%)]\tLoss: 2.705297\n",
            "Train Epoch: 18 [24320/57344 (42%)]\tLoss: 2.723957\n",
            "Train Epoch: 18 [25600/57344 (45%)]\tLoss: 2.677550\n",
            "Train Epoch: 18 [26880/57344 (47%)]\tLoss: 2.638571\n",
            "Train Epoch: 18 [28160/57344 (49%)]\tLoss: 2.645239\n",
            "Train Epoch: 18 [29440/57344 (51%)]\tLoss: 2.665611\n",
            "Train Epoch: 18 [30720/57344 (54%)]\tLoss: 2.610838\n",
            "Train Epoch: 18 [32000/57344 (56%)]\tLoss: 2.631340\n",
            "Train Epoch: 18 [33280/57344 (58%)]\tLoss: 2.627866\n",
            "Train Epoch: 18 [34560/57344 (60%)]\tLoss: 2.618276\n",
            "Train Epoch: 18 [35840/57344 (62%)]\tLoss: 2.683770\n",
            "Train Epoch: 18 [37120/57344 (65%)]\tLoss: 2.654331\n",
            "Train Epoch: 18 [38400/57344 (67%)]\tLoss: 2.673848\n",
            "Train Epoch: 18 [39680/57344 (69%)]\tLoss: 2.640246\n",
            "Train Epoch: 18 [40960/57344 (71%)]\tLoss: 2.652312\n",
            "Train Epoch: 18 [42240/57344 (74%)]\tLoss: 2.723608\n",
            "Train Epoch: 18 [43520/57344 (76%)]\tLoss: 2.670619\n",
            "Train Epoch: 18 [44800/57344 (78%)]\tLoss: 2.675735\n",
            "Train Epoch: 18 [46080/57344 (80%)]\tLoss: 2.705892\n",
            "Train Epoch: 18 [47360/57344 (83%)]\tLoss: 2.604157\n",
            "Train Epoch: 18 [48640/57344 (85%)]\tLoss: 2.626031\n",
            "Train Epoch: 18 [49920/57344 (87%)]\tLoss: 2.627014\n",
            "Train Epoch: 18 [51200/57344 (89%)]\tLoss: 2.616195\n",
            "Train Epoch: 18 [52480/57344 (92%)]\tLoss: 2.615125\n",
            "Train Epoch: 18 [53760/57344 (94%)]\tLoss: 2.634089\n",
            "Train Epoch: 18 [55040/57344 (96%)]\tLoss: 2.694732\n",
            "Train Epoch: 18 [56320/57344 (98%)]\tLoss: 2.686723\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of he the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the harry were to the large\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of said him <unknown> to the <unknown> of harry in they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head and he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still to <unknown> of had to to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he to him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t was the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry he was still to the head and he were got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry and <unknown> and you what he do to said think think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7211, Accuracy: 245408/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/018.pt\n",
            "\n",
            "generated sample\t harry potter  harry his face was at the moment harry had told him to be to have\n",
            "generated sample\t harry potter   harry <unknown> it he was looking at him as though he was a <unknown> \n",
            "generated sample\t harry potter  dumbledore he said i not a <unknown> and harry looked around at him he had\n",
            "generated sample\t harry potter  harry it was a very long <unknown> and his <unknown> <unknown> in the air of\n",
            "generated sample\t harry potter  his his mind and a <unknown> and he looked around at him he had to\n",
            "generated sample\t harry potter  harry i got she said harry he said i not not to do it said\n",
            "generated sample\t harry potter  what it was just to be a bit of a <unknown> harry potter i not\n",
            "generated sample\t harry potter  harry he said harry i think you do said harry i said harry he said\n",
            "generated sample\t harry potter   he said ron quietly i do know he said malfoy looking at him and \n",
            "generated sample\t harry potter  that harry could see him again and the <unknown> of the snitch was <unknown> with\n",
            "generated beam\t\t harry potter  harry he had never been to <unknown> his head but it was a <unknown> of\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 19 [0/57344 (0%)]\tLoss: 2.897472\n",
            "Train Epoch: 19 [1280/57344 (2%)]\tLoss: 2.682355\n",
            "Train Epoch: 19 [2560/57344 (4%)]\tLoss: 2.703308\n",
            "Train Epoch: 19 [3840/57344 (7%)]\tLoss: 2.679232\n",
            "Train Epoch: 19 [5120/57344 (9%)]\tLoss: 2.649617\n",
            "Train Epoch: 19 [6400/57344 (11%)]\tLoss: 2.644665\n",
            "Train Epoch: 19 [7680/57344 (13%)]\tLoss: 2.659515\n",
            "Train Epoch: 19 [8960/57344 (16%)]\tLoss: 2.704300\n",
            "Train Epoch: 19 [10240/57344 (18%)]\tLoss: 2.690388\n",
            "Train Epoch: 19 [11520/57344 (20%)]\tLoss: 2.604626\n",
            "Train Epoch: 19 [12800/57344 (22%)]\tLoss: 2.618742\n",
            "Train Epoch: 19 [14080/57344 (25%)]\tLoss: 2.671681\n",
            "Train Epoch: 19 [15360/57344 (27%)]\tLoss: 2.631725\n",
            "Train Epoch: 19 [16640/57344 (29%)]\tLoss: 2.604566\n",
            "Train Epoch: 19 [17920/57344 (31%)]\tLoss: 2.606757\n",
            "Train Epoch: 19 [19200/57344 (33%)]\tLoss: 2.650984\n",
            "Train Epoch: 19 [20480/57344 (36%)]\tLoss: 2.682336\n",
            "Train Epoch: 19 [21760/57344 (38%)]\tLoss: 2.615976\n",
            "Train Epoch: 19 [23040/57344 (40%)]\tLoss: 2.704487\n",
            "Train Epoch: 19 [24320/57344 (42%)]\tLoss: 2.722369\n",
            "Train Epoch: 19 [25600/57344 (45%)]\tLoss: 2.674299\n",
            "Train Epoch: 19 [26880/57344 (47%)]\tLoss: 2.639331\n",
            "Train Epoch: 19 [28160/57344 (49%)]\tLoss: 2.645469\n",
            "Train Epoch: 19 [29440/57344 (51%)]\tLoss: 2.662545\n",
            "Train Epoch: 19 [30720/57344 (54%)]\tLoss: 2.610073\n",
            "Train Epoch: 19 [32000/57344 (56%)]\tLoss: 2.626912\n",
            "Train Epoch: 19 [33280/57344 (58%)]\tLoss: 2.626798\n",
            "Train Epoch: 19 [34560/57344 (60%)]\tLoss: 2.615402\n",
            "Train Epoch: 19 [35840/57344 (62%)]\tLoss: 2.684189\n",
            "Train Epoch: 19 [37120/57344 (65%)]\tLoss: 2.651679\n",
            "Train Epoch: 19 [38400/57344 (67%)]\tLoss: 2.672406\n",
            "Train Epoch: 19 [39680/57344 (69%)]\tLoss: 2.636607\n",
            "Train Epoch: 19 [40960/57344 (71%)]\tLoss: 2.649854\n",
            "Train Epoch: 19 [42240/57344 (74%)]\tLoss: 2.719344\n",
            "Train Epoch: 19 [43520/57344 (76%)]\tLoss: 2.671480\n",
            "Train Epoch: 19 [44800/57344 (78%)]\tLoss: 2.674296\n",
            "Train Epoch: 19 [46080/57344 (80%)]\tLoss: 2.705827\n",
            "Train Epoch: 19 [47360/57344 (83%)]\tLoss: 2.605560\n",
            "Train Epoch: 19 [48640/57344 (85%)]\tLoss: 2.622200\n",
            "Train Epoch: 19 [49920/57344 (87%)]\tLoss: 2.624920\n",
            "Train Epoch: 19 [51200/57344 (89%)]\tLoss: 2.611657\n",
            "Train Epoch: 19 [52480/57344 (92%)]\tLoss: 2.614081\n",
            "Train Epoch: 19 [53760/57344 (94%)]\tLoss: 2.635335\n",
            "Train Epoch: 19 [55040/57344 (96%)]\tLoss: 2.694384\n",
            "Train Epoch: 19 [56320/57344 (98%)]\tLoss: 2.683289\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the they were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry in they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and to of and and the head and he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was wearing to <unknown> of had was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he to him he he you to i him to the said you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t was the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry he was still to the head and he were got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t said the ron and <unknown> and you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7220, Accuracy: 245300/430080 (57%)\n",
            "\n",
            "generated sample\t harry potter  harry he told ron to see it through the ground and the <unknown> of the\n",
            "generated sample\t harry potter  you harry said quietly his eyes was still as he turned to the door into\n",
            "generated sample\t harry potter  he now said the great voice of the <unknown> <unknown> it was a <unknown> of\n",
            "generated sample\t harry potter  said harry and hermione in the air but they was bad to be <unknown> in\n",
            "generated sample\t harry potter  it yeah said harry in a voice to harry and hermione and hermione left the\n",
            "generated sample\t harry potter  harry and then harry turned to him but he had to make a <unknown> of\n",
            "generated sample\t harry potter  harry the <unknown> harry was wearing his head on the floor and he was shaking\n",
            "generated sample\t harry potter  this harry said ron with a large voice and the man was staring at harry\n",
            "generated sample\t harry potter  harry he was the <unknown> of the <unknown> of the crowd of the <unknown> <unknown>\n",
            "generated sample\t harry potter  i do think harry said the door of the air it was no one of\n",
            "generated beam\t\t harry potter  the said fudge angrily i have to keep a large <unknown> of <unknown> the death\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 20 [0/57344 (0%)]\tLoss: 2.898869\n",
            "Train Epoch: 20 [1280/57344 (2%)]\tLoss: 2.680299\n",
            "Train Epoch: 20 [2560/57344 (4%)]\tLoss: 2.701910\n",
            "Train Epoch: 20 [3840/57344 (7%)]\tLoss: 2.678729\n",
            "Train Epoch: 20 [5120/57344 (9%)]\tLoss: 2.651613\n",
            "Train Epoch: 20 [6400/57344 (11%)]\tLoss: 2.643036\n",
            "Train Epoch: 20 [7680/57344 (13%)]\tLoss: 2.655225\n",
            "Train Epoch: 20 [8960/57344 (16%)]\tLoss: 2.702784\n",
            "Train Epoch: 20 [10240/57344 (18%)]\tLoss: 2.687344\n",
            "Train Epoch: 20 [11520/57344 (20%)]\tLoss: 2.606818\n",
            "Train Epoch: 20 [12800/57344 (22%)]\tLoss: 2.620498\n",
            "Train Epoch: 20 [14080/57344 (25%)]\tLoss: 2.674547\n",
            "Train Epoch: 20 [15360/57344 (27%)]\tLoss: 2.631846\n",
            "Train Epoch: 20 [16640/57344 (29%)]\tLoss: 2.605848\n",
            "Train Epoch: 20 [17920/57344 (31%)]\tLoss: 2.600222\n",
            "Train Epoch: 20 [19200/57344 (33%)]\tLoss: 2.645136\n",
            "Train Epoch: 20 [20480/57344 (36%)]\tLoss: 2.681254\n",
            "Train Epoch: 20 [21760/57344 (38%)]\tLoss: 2.613129\n",
            "Train Epoch: 20 [23040/57344 (40%)]\tLoss: 2.705922\n",
            "Train Epoch: 20 [24320/57344 (42%)]\tLoss: 2.721545\n",
            "Train Epoch: 20 [25600/57344 (45%)]\tLoss: 2.673655\n",
            "Train Epoch: 20 [26880/57344 (47%)]\tLoss: 2.637993\n",
            "Train Epoch: 20 [28160/57344 (49%)]\tLoss: 2.642795\n",
            "Train Epoch: 20 [29440/57344 (51%)]\tLoss: 2.662312\n",
            "Train Epoch: 20 [30720/57344 (54%)]\tLoss: 2.608970\n",
            "Train Epoch: 20 [32000/57344 (56%)]\tLoss: 2.626982\n",
            "Train Epoch: 20 [33280/57344 (58%)]\tLoss: 2.626388\n",
            "Train Epoch: 20 [34560/57344 (60%)]\tLoss: 2.614634\n",
            "Train Epoch: 20 [35840/57344 (62%)]\tLoss: 2.683417\n",
            "Train Epoch: 20 [37120/57344 (65%)]\tLoss: 2.651647\n",
            "Train Epoch: 20 [38400/57344 (67%)]\tLoss: 2.667327\n",
            "Train Epoch: 20 [39680/57344 (69%)]\tLoss: 2.633851\n",
            "Train Epoch: 20 [40960/57344 (71%)]\tLoss: 2.647382\n",
            "Train Epoch: 20 [42240/57344 (74%)]\tLoss: 2.720500\n",
            "Train Epoch: 20 [43520/57344 (76%)]\tLoss: 2.671172\n",
            "Train Epoch: 20 [44800/57344 (78%)]\tLoss: 2.675841\n",
            "Train Epoch: 20 [46080/57344 (80%)]\tLoss: 2.703597\n",
            "Train Epoch: 20 [47360/57344 (83%)]\tLoss: 2.603619\n",
            "Train Epoch: 20 [48640/57344 (85%)]\tLoss: 2.623768\n",
            "Train Epoch: 20 [49920/57344 (87%)]\tLoss: 2.622075\n",
            "Train Epoch: 20 [51200/57344 (89%)]\tLoss: 2.612803\n",
            "Train Epoch: 20 [52480/57344 (92%)]\tLoss: 2.612205\n",
            "Train Epoch: 20 [53760/57344 (94%)]\tLoss: 2.632267\n",
            "Train Epoch: 20 [55040/57344 (96%)]\tLoss: 2.692823\n",
            "Train Epoch: 20 [56320/57344 (98%)]\tLoss: 2.684701\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the harry were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry in they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and the of and and the head and he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was wearing to <unknown> of had was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the door of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry he was still to the head and he were got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry in <unknown> and you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7192, Accuracy: 245531/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/020.pt\n",
            "\n",
            "generated sample\t harry potter  harry harry and hermione he looked around and was a clear to the prime wing\n",
            "generated sample\t harry potter  harry he got to go to get on the <unknown> of the class and the\n",
            "generated sample\t harry potter  i harry said quietly they were still in a very <unknown> of the <unknown> of\n",
            "generated sample\t harry potter  harry said dumbledore and harry looked around at the moment he turned to harry and\n",
            "generated sample\t harry potter   what it was a good <unknown> you know said ron in a <unknown> voice \n",
            "generated sample\t harry potter  he harry he said harry did not want to go to the world said harry\n",
            "generated sample\t harry potter harry said harry in a voice he was <unknown> to the door of the entrance \n",
            "generated sample\t harry potter  <unknown> harry potter and <unknown> it was a <unknown> in the <unknown> <unknown> not to\n",
            "generated sample\t harry potter  ron said harry and ron went to see the door and saw a large <unknown>\n",
            "generated sample\t harry potter  harry he said <unknown> but it was a very <unknown> he looked at him he\n",
            "generated beam\t\t harry potter  said harry but he was going to be all in the air and <unknown> them\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 21 [0/57344 (0%)]\tLoss: 2.900044\n",
            "Train Epoch: 21 [1280/57344 (2%)]\tLoss: 2.680008\n",
            "Train Epoch: 21 [2560/57344 (4%)]\tLoss: 2.702442\n",
            "Train Epoch: 21 [3840/57344 (7%)]\tLoss: 2.677436\n",
            "Train Epoch: 21 [5120/57344 (9%)]\tLoss: 2.650137\n",
            "Train Epoch: 21 [6400/57344 (11%)]\tLoss: 2.643286\n",
            "Train Epoch: 21 [7680/57344 (13%)]\tLoss: 2.653574\n",
            "Train Epoch: 21 [8960/57344 (16%)]\tLoss: 2.701240\n",
            "Train Epoch: 21 [10240/57344 (18%)]\tLoss: 2.686177\n",
            "Train Epoch: 21 [11520/57344 (20%)]\tLoss: 2.602995\n",
            "Train Epoch: 21 [12800/57344 (22%)]\tLoss: 2.616006\n",
            "Train Epoch: 21 [14080/57344 (25%)]\tLoss: 2.669029\n",
            "Train Epoch: 21 [15360/57344 (27%)]\tLoss: 2.629516\n",
            "Train Epoch: 21 [16640/57344 (29%)]\tLoss: 2.604145\n",
            "Train Epoch: 21 [17920/57344 (31%)]\tLoss: 2.598990\n",
            "Train Epoch: 21 [19200/57344 (33%)]\tLoss: 2.644048\n",
            "Train Epoch: 21 [20480/57344 (36%)]\tLoss: 2.679873\n",
            "Train Epoch: 21 [21760/57344 (38%)]\tLoss: 2.612681\n",
            "Train Epoch: 21 [23040/57344 (40%)]\tLoss: 2.703180\n",
            "Train Epoch: 21 [24320/57344 (42%)]\tLoss: 2.720504\n",
            "Train Epoch: 21 [25600/57344 (45%)]\tLoss: 2.673382\n",
            "Train Epoch: 21 [26880/57344 (47%)]\tLoss: 2.636404\n",
            "Train Epoch: 21 [28160/57344 (49%)]\tLoss: 2.640333\n",
            "Train Epoch: 21 [29440/57344 (51%)]\tLoss: 2.661199\n",
            "Train Epoch: 21 [30720/57344 (54%)]\tLoss: 2.607678\n",
            "Train Epoch: 21 [32000/57344 (56%)]\tLoss: 2.625592\n",
            "Train Epoch: 21 [33280/57344 (58%)]\tLoss: 2.625804\n",
            "Train Epoch: 21 [34560/57344 (60%)]\tLoss: 2.613525\n",
            "Train Epoch: 21 [35840/57344 (62%)]\tLoss: 2.681395\n",
            "Train Epoch: 21 [37120/57344 (65%)]\tLoss: 2.649678\n",
            "Train Epoch: 21 [38400/57344 (67%)]\tLoss: 2.666654\n",
            "Train Epoch: 21 [39680/57344 (69%)]\tLoss: 2.635319\n",
            "Train Epoch: 21 [40960/57344 (71%)]\tLoss: 2.647438\n",
            "Train Epoch: 21 [42240/57344 (74%)]\tLoss: 2.720626\n",
            "Train Epoch: 21 [43520/57344 (76%)]\tLoss: 2.670730\n",
            "Train Epoch: 21 [44800/57344 (78%)]\tLoss: 2.674784\n",
            "Train Epoch: 21 [46080/57344 (80%)]\tLoss: 2.701924\n",
            "Train Epoch: 21 [47360/57344 (83%)]\tLoss: 2.601225\n",
            "Train Epoch: 21 [48640/57344 (85%)]\tLoss: 2.621653\n",
            "Train Epoch: 21 [49920/57344 (87%)]\tLoss: 2.621975\n",
            "Train Epoch: 21 [51200/57344 (89%)]\tLoss: 2.611642\n",
            "Train Epoch: 21 [52480/57344 (92%)]\tLoss: 2.611332\n",
            "Train Epoch: 21 [53760/57344 (94%)]\tLoss: 2.632211\n",
            "Train Epoch: 21 [55040/57344 (96%)]\tLoss: 2.690084\n",
            "Train Epoch: 21 [56320/57344 (98%)]\tLoss: 2.683301\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of he the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the he were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of he him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was wearing to <unknown> of had was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry he was still to the head and he know got said\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry he <unknown> and you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7182, Accuracy: 245577/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/021.pt\n",
            "\n",
            "generated sample\t harry potter  i i do think he wanted to get on the dark arts and come on\n",
            "generated sample\t harry potter  harry harry and harry and hermione were <unknown> over a small <unknown> of the room\n",
            "generated sample\t harry potter   harry he said ron and hermione glanced at him he was looking at harry \n",
            "generated sample\t harry potter  harry said harry i think he really said harry he did not know he was\n",
            "generated sample\t harry potter  said hagrid i know he did said hagrid i really have to do i mean\n",
            "generated sample\t harry potter  harry said harry and hermione moved up to the cloak and then and ron pulled\n",
            "generated sample\t harry potter  i do you know said harry you know you know if it was you know\n",
            "generated sample\t harry potter  harry harry said he was no said harry looking at him and you see you\n",
            "generated sample\t harry potter   harry said harry i know said harry looking at the <unknown> of the room \n",
            "generated sample\t harry potter madam said harry and hermione and hermione and hermione were still going to get out \n",
            "generated beam\t\t harry potter  this said harry quickly and he went to see the other of the <unknown> of\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 22 [0/57344 (0%)]\tLoss: 2.900000\n",
            "Train Epoch: 22 [1280/57344 (2%)]\tLoss: 2.679576\n",
            "Train Epoch: 22 [2560/57344 (4%)]\tLoss: 2.701459\n",
            "Train Epoch: 22 [3840/57344 (7%)]\tLoss: 2.678083\n",
            "Train Epoch: 22 [5120/57344 (9%)]\tLoss: 2.648193\n",
            "Train Epoch: 22 [6400/57344 (11%)]\tLoss: 2.640246\n",
            "Train Epoch: 22 [7680/57344 (13%)]\tLoss: 2.652917\n",
            "Train Epoch: 22 [8960/57344 (16%)]\tLoss: 2.700822\n",
            "Train Epoch: 22 [10240/57344 (18%)]\tLoss: 2.686757\n",
            "Train Epoch: 22 [11520/57344 (20%)]\tLoss: 2.600842\n",
            "Train Epoch: 22 [12800/57344 (22%)]\tLoss: 2.613663\n",
            "Train Epoch: 22 [14080/57344 (25%)]\tLoss: 2.668585\n",
            "Train Epoch: 22 [15360/57344 (27%)]\tLoss: 2.629958\n",
            "Train Epoch: 22 [16640/57344 (29%)]\tLoss: 2.601154\n",
            "Train Epoch: 22 [17920/57344 (31%)]\tLoss: 2.597196\n",
            "Train Epoch: 22 [19200/57344 (33%)]\tLoss: 2.643939\n",
            "Train Epoch: 22 [20480/57344 (36%)]\tLoss: 2.678962\n",
            "Train Epoch: 22 [21760/57344 (38%)]\tLoss: 2.611250\n",
            "Train Epoch: 22 [23040/57344 (40%)]\tLoss: 2.703549\n",
            "Train Epoch: 22 [24320/57344 (42%)]\tLoss: 2.718002\n",
            "Train Epoch: 22 [25600/57344 (45%)]\tLoss: 2.671958\n",
            "Train Epoch: 22 [26880/57344 (47%)]\tLoss: 2.632840\n",
            "Train Epoch: 22 [28160/57344 (49%)]\tLoss: 2.641103\n",
            "Train Epoch: 22 [29440/57344 (51%)]\tLoss: 2.659578\n",
            "Train Epoch: 22 [30720/57344 (54%)]\tLoss: 2.607300\n",
            "Train Epoch: 22 [32000/57344 (56%)]\tLoss: 2.623089\n",
            "Train Epoch: 22 [33280/57344 (58%)]\tLoss: 2.626379\n",
            "Train Epoch: 22 [34560/57344 (60%)]\tLoss: 2.612727\n",
            "Train Epoch: 22 [35840/57344 (62%)]\tLoss: 2.680264\n",
            "Train Epoch: 22 [37120/57344 (65%)]\tLoss: 2.648792\n",
            "Train Epoch: 22 [38400/57344 (67%)]\tLoss: 2.667481\n",
            "Train Epoch: 22 [39680/57344 (69%)]\tLoss: 2.634749\n",
            "Train Epoch: 22 [40960/57344 (71%)]\tLoss: 2.647273\n",
            "Train Epoch: 22 [42240/57344 (74%)]\tLoss: 2.718380\n",
            "Train Epoch: 22 [43520/57344 (76%)]\tLoss: 2.669751\n",
            "Train Epoch: 22 [44800/57344 (78%)]\tLoss: 2.675691\n",
            "Train Epoch: 22 [46080/57344 (80%)]\tLoss: 2.703446\n",
            "Train Epoch: 22 [47360/57344 (83%)]\tLoss: 2.602532\n",
            "Train Epoch: 22 [48640/57344 (85%)]\tLoss: 2.619812\n",
            "Train Epoch: 22 [49920/57344 (87%)]\tLoss: 2.621847\n",
            "Train Epoch: 22 [51200/57344 (89%)]\tLoss: 2.611643\n",
            "Train Epoch: 22 [52480/57344 (92%)]\tLoss: 2.610330\n",
            "Train Epoch: 22 [53760/57344 (94%)]\tLoss: 2.632409\n",
            "Train Epoch: 22 [55040/57344 (96%)]\tLoss: 2.689274\n",
            "Train Epoch: 22 [56320/57344 (98%)]\tLoss: 2.681271\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of he the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still to <unknown> of had was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he i he he him i he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry i <unknown> and you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7162, Accuracy: 245732/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/022.pt\n",
            "\n",
            "generated sample\t harry potter  he harry said ron and hermione looked around at the train in the wall of\n",
            "generated sample\t harry potter  harry said harry you do he said to harry and hermione and the <unknown> of\n",
            "generated sample\t harry potter  we the <unknown> of the stone punishment was full of <unknown> <unknown> in the air\n",
            "generated sample\t harry potter  harry i know it was good said harry and the door on the back of\n",
            "generated sample\t harry potter  i i have to do said harry quickly i am want to do it would\n",
            "generated sample\t harry potter  i just to do it he said quietly he is the mirror of the time\n",
            "generated sample\t harry potter  harry said harry and hermione had the first time to his <unknown> and the class\n",
            "generated sample\t harry potter  said harry and hermione looked around at harry and he did see he was going\n",
            "generated sample\t harry potter  no i do know it is it said ron looking at harry and recognized to\n",
            "generated sample\t harry potter   the start of the ministry of magic and then then you were going to \n",
            "generated beam\t\t harry potter  harry said harry and he looked at him and the other of the class was\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 23 [0/57344 (0%)]\tLoss: 2.898453\n",
            "Train Epoch: 23 [1280/57344 (2%)]\tLoss: 2.677378\n",
            "Train Epoch: 23 [2560/57344 (4%)]\tLoss: 2.699830\n",
            "Train Epoch: 23 [3840/57344 (7%)]\tLoss: 2.676929\n",
            "Train Epoch: 23 [5120/57344 (9%)]\tLoss: 2.645381\n",
            "Train Epoch: 23 [6400/57344 (11%)]\tLoss: 2.639712\n",
            "Train Epoch: 23 [7680/57344 (13%)]\tLoss: 2.652086\n",
            "Train Epoch: 23 [8960/57344 (16%)]\tLoss: 2.699201\n",
            "Train Epoch: 23 [10240/57344 (18%)]\tLoss: 2.684793\n",
            "Train Epoch: 23 [11520/57344 (20%)]\tLoss: 2.601846\n",
            "Train Epoch: 23 [12800/57344 (22%)]\tLoss: 2.613691\n",
            "Train Epoch: 23 [14080/57344 (25%)]\tLoss: 2.669003\n",
            "Train Epoch: 23 [15360/57344 (27%)]\tLoss: 2.628391\n",
            "Train Epoch: 23 [16640/57344 (29%)]\tLoss: 2.598202\n",
            "Train Epoch: 23 [17920/57344 (31%)]\tLoss: 2.597049\n",
            "Train Epoch: 23 [19200/57344 (33%)]\tLoss: 2.642329\n",
            "Train Epoch: 23 [20480/57344 (36%)]\tLoss: 2.678314\n",
            "Train Epoch: 23 [21760/57344 (38%)]\tLoss: 2.608682\n",
            "Train Epoch: 23 [23040/57344 (40%)]\tLoss: 2.700461\n",
            "Train Epoch: 23 [24320/57344 (42%)]\tLoss: 2.715336\n",
            "Train Epoch: 23 [25600/57344 (45%)]\tLoss: 2.671604\n",
            "Train Epoch: 23 [26880/57344 (47%)]\tLoss: 2.631588\n",
            "Train Epoch: 23 [28160/57344 (49%)]\tLoss: 2.639167\n",
            "Train Epoch: 23 [29440/57344 (51%)]\tLoss: 2.658223\n",
            "Train Epoch: 23 [30720/57344 (54%)]\tLoss: 2.606170\n",
            "Train Epoch: 23 [32000/57344 (56%)]\tLoss: 2.622849\n",
            "Train Epoch: 23 [33280/57344 (58%)]\tLoss: 2.624841\n",
            "Train Epoch: 23 [34560/57344 (60%)]\tLoss: 2.611527\n",
            "Train Epoch: 23 [35840/57344 (62%)]\tLoss: 2.678560\n",
            "Train Epoch: 23 [37120/57344 (65%)]\tLoss: 2.648860\n",
            "Train Epoch: 23 [38400/57344 (67%)]\tLoss: 2.666869\n",
            "Train Epoch: 23 [39680/57344 (69%)]\tLoss: 2.634136\n",
            "Train Epoch: 23 [40960/57344 (71%)]\tLoss: 2.647144\n",
            "Train Epoch: 23 [42240/57344 (74%)]\tLoss: 2.715721\n",
            "Train Epoch: 23 [43520/57344 (76%)]\tLoss: 2.668587\n",
            "Train Epoch: 23 [44800/57344 (78%)]\tLoss: 2.671059\n",
            "Train Epoch: 23 [46080/57344 (80%)]\tLoss: 2.700394\n",
            "Train Epoch: 23 [47360/57344 (83%)]\tLoss: 2.601403\n",
            "Train Epoch: 23 [48640/57344 (85%)]\tLoss: 2.619985\n",
            "Train Epoch: 23 [49920/57344 (87%)]\tLoss: 2.619419\n",
            "Train Epoch: 23 [51200/57344 (89%)]\tLoss: 2.608857\n",
            "Train Epoch: 23 [52480/57344 (92%)]\tLoss: 2.610332\n",
            "Train Epoch: 23 [53760/57344 (94%)]\tLoss: 2.632067\n",
            "Train Epoch: 23 [55040/57344 (96%)]\tLoss: 2.687728\n",
            "Train Epoch: 23 [56320/57344 (98%)]\tLoss: 2.681193\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters and the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still to <unknown> of had was to off the door to\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry he <unknown> and you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7159, Accuracy: 245739/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/023.pt\n",
            "\n",
            "generated sample\t harry potter  there harry i want to go in the ministry of the <unknown> of the <unknown>\n",
            "generated sample\t harry potter  harry harry said dumbledore the <unknown> is he said to his head and oh i\n",
            "generated sample\t harry potter  what i got to <unknown> harry potter and he had been <unknown> to the <unknown>\n",
            "generated sample\t harry potter  it there was a very <unknown> to the other of the <unknown> and the <unknown>\n",
            "generated sample\t harry potter  harry said harry and harry looked at the ceiling from the end of his robes\n",
            "generated sample\t harry potter   harry potter harry potter and ron and ron and hermione walked to the high \n",
            "generated sample\t harry potter  the dumbledore potter he did not see or i do know it is no said\n",
            "generated sample\t harry potter  this i do want to ask him potter i am you know dumbledore said ron\n",
            "generated sample\t harry potter   this harry said harry and he reached the door in his head and then \n",
            "generated sample\t harry potter  he all the time before he tried to get here up and the dark lord\n",
            "generated beam\t\t harry potter   harry i think i do think said harry nodding in the air and the \n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 24 [0/57344 (0%)]\tLoss: 2.897262\n",
            "Train Epoch: 24 [1280/57344 (2%)]\tLoss: 2.676299\n",
            "Train Epoch: 24 [2560/57344 (4%)]\tLoss: 2.699135\n",
            "Train Epoch: 24 [3840/57344 (7%)]\tLoss: 2.675647\n",
            "Train Epoch: 24 [5120/57344 (9%)]\tLoss: 2.645273\n",
            "Train Epoch: 24 [6400/57344 (11%)]\tLoss: 2.638720\n",
            "Train Epoch: 24 [7680/57344 (13%)]\tLoss: 2.651479\n",
            "Train Epoch: 24 [8960/57344 (16%)]\tLoss: 2.698424\n",
            "Train Epoch: 24 [10240/57344 (18%)]\tLoss: 2.684227\n",
            "Train Epoch: 24 [11520/57344 (20%)]\tLoss: 2.600749\n",
            "Train Epoch: 24 [12800/57344 (22%)]\tLoss: 2.611167\n",
            "Train Epoch: 24 [14080/57344 (25%)]\tLoss: 2.669554\n",
            "Train Epoch: 24 [15360/57344 (27%)]\tLoss: 2.629175\n",
            "Train Epoch: 24 [16640/57344 (29%)]\tLoss: 2.600206\n",
            "Train Epoch: 24 [17920/57344 (31%)]\tLoss: 2.599005\n",
            "Train Epoch: 24 [19200/57344 (33%)]\tLoss: 2.641313\n",
            "Train Epoch: 24 [20480/57344 (36%)]\tLoss: 2.677215\n",
            "Train Epoch: 24 [21760/57344 (38%)]\tLoss: 2.606471\n",
            "Train Epoch: 24 [23040/57344 (40%)]\tLoss: 2.698662\n",
            "Train Epoch: 24 [24320/57344 (42%)]\tLoss: 2.716331\n",
            "Train Epoch: 24 [25600/57344 (45%)]\tLoss: 2.670238\n",
            "Train Epoch: 24 [26880/57344 (47%)]\tLoss: 2.631935\n",
            "Train Epoch: 24 [28160/57344 (49%)]\tLoss: 2.639606\n",
            "Train Epoch: 24 [29440/57344 (51%)]\tLoss: 2.657754\n",
            "Train Epoch: 24 [30720/57344 (54%)]\tLoss: 2.604985\n",
            "Train Epoch: 24 [32000/57344 (56%)]\tLoss: 2.622936\n",
            "Train Epoch: 24 [33280/57344 (58%)]\tLoss: 2.623669\n",
            "Train Epoch: 24 [34560/57344 (60%)]\tLoss: 2.608892\n",
            "Train Epoch: 24 [35840/57344 (62%)]\tLoss: 2.675950\n",
            "Train Epoch: 24 [37120/57344 (65%)]\tLoss: 2.650008\n",
            "Train Epoch: 24 [38400/57344 (67%)]\tLoss: 2.668385\n",
            "Train Epoch: 24 [39680/57344 (69%)]\tLoss: 2.630052\n",
            "Train Epoch: 24 [40960/57344 (71%)]\tLoss: 2.645374\n",
            "Train Epoch: 24 [42240/57344 (74%)]\tLoss: 2.713726\n",
            "Train Epoch: 24 [43520/57344 (76%)]\tLoss: 2.667039\n",
            "Train Epoch: 24 [44800/57344 (78%)]\tLoss: 2.674447\n",
            "Train Epoch: 24 [46080/57344 (80%)]\tLoss: 2.702214\n",
            "Train Epoch: 24 [47360/57344 (83%)]\tLoss: 2.599196\n",
            "Train Epoch: 24 [48640/57344 (85%)]\tLoss: 2.617833\n",
            "Train Epoch: 24 [49920/57344 (87%)]\tLoss: 2.620002\n",
            "Train Epoch: 24 [51200/57344 (89%)]\tLoss: 2.608677\n",
            "Train Epoch: 24 [52480/57344 (92%)]\tLoss: 2.610046\n",
            "Train Epoch: 24 [53760/57344 (94%)]\tLoss: 2.633108\n",
            "Train Epoch: 24 [55040/57344 (96%)]\tLoss: 2.689164\n",
            "Train Epoch: 24 [56320/57344 (98%)]\tLoss: 2.679438\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of he the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters had the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry in they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still to <unknown> of had was to off the door to\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him i he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the door of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry i <unknown> than you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7142, Accuracy: 245812/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/024.pt\n",
            "\n",
            "generated sample\t harry potter  you what harry said i do want to get to the <unknown> of the ministry\n",
            "generated sample\t harry potter   harry potter and he said his eyes <unknown> i have got a lot of \n",
            "generated sample\t harry potter   you harry said ron and hermione and hermione to the rest of the crowd \n",
            "generated sample\t harry potter  i mean it will be no said harry he was still they were trying to\n",
            "generated sample\t harry potter  he would not understand he looked toward the door and harry did see the dursleys\n",
            "generated sample\t harry potter  what it was a very good harry said harry he said he did not to\n",
            "generated sample\t harry potter  harry potter i think you have said ron and hermione and the door was still\n",
            "generated sample\t harry potter   what he might be spending about the first time i did not think he \n",
            "generated sample\t harry potter   harry said snape and the <unknown> of the quaffle had turned to the dark \n",
            "generated sample\t harry potter  you mean harry said he not to tell him to do said harry watching me\n",
            "generated beam\t\t harry potter  harry harry and hermione and hermione who were looking <unknown> at them they were all\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 25 [0/57344 (0%)]\tLoss: 2.897640\n",
            "Train Epoch: 25 [1280/57344 (2%)]\tLoss: 2.674811\n",
            "Train Epoch: 25 [2560/57344 (4%)]\tLoss: 2.699223\n",
            "Train Epoch: 25 [3840/57344 (7%)]\tLoss: 2.675308\n",
            "Train Epoch: 25 [5120/57344 (9%)]\tLoss: 2.645451\n",
            "Train Epoch: 25 [6400/57344 (11%)]\tLoss: 2.637611\n",
            "Train Epoch: 25 [7680/57344 (13%)]\tLoss: 2.651653\n",
            "Train Epoch: 25 [8960/57344 (16%)]\tLoss: 2.699040\n",
            "Train Epoch: 25 [10240/57344 (18%)]\tLoss: 2.683836\n",
            "Train Epoch: 25 [11520/57344 (20%)]\tLoss: 2.600528\n",
            "Train Epoch: 25 [12800/57344 (22%)]\tLoss: 2.611604\n",
            "Train Epoch: 25 [14080/57344 (25%)]\tLoss: 2.666684\n",
            "Train Epoch: 25 [15360/57344 (27%)]\tLoss: 2.625923\n",
            "Train Epoch: 25 [16640/57344 (29%)]\tLoss: 2.597743\n",
            "Train Epoch: 25 [17920/57344 (31%)]\tLoss: 2.595952\n",
            "Train Epoch: 25 [19200/57344 (33%)]\tLoss: 2.640082\n",
            "Train Epoch: 25 [20480/57344 (36%)]\tLoss: 2.677253\n",
            "Train Epoch: 25 [21760/57344 (38%)]\tLoss: 2.607655\n",
            "Train Epoch: 25 [23040/57344 (40%)]\tLoss: 2.698345\n",
            "Train Epoch: 25 [24320/57344 (42%)]\tLoss: 2.712945\n",
            "Train Epoch: 25 [25600/57344 (45%)]\tLoss: 2.669764\n",
            "Train Epoch: 25 [26880/57344 (47%)]\tLoss: 2.628768\n",
            "Train Epoch: 25 [28160/57344 (49%)]\tLoss: 2.637760\n",
            "Train Epoch: 25 [29440/57344 (51%)]\tLoss: 2.656759\n",
            "Train Epoch: 25 [30720/57344 (54%)]\tLoss: 2.605298\n",
            "Train Epoch: 25 [32000/57344 (56%)]\tLoss: 2.622553\n",
            "Train Epoch: 25 [33280/57344 (58%)]\tLoss: 2.623772\n",
            "Train Epoch: 25 [34560/57344 (60%)]\tLoss: 2.607897\n",
            "Train Epoch: 25 [35840/57344 (62%)]\tLoss: 2.676395\n",
            "Train Epoch: 25 [37120/57344 (65%)]\tLoss: 2.648805\n",
            "Train Epoch: 25 [38400/57344 (67%)]\tLoss: 2.667027\n",
            "Train Epoch: 25 [39680/57344 (69%)]\tLoss: 2.628414\n",
            "Train Epoch: 25 [40960/57344 (71%)]\tLoss: 2.644458\n",
            "Train Epoch: 25 [42240/57344 (74%)]\tLoss: 2.713250\n",
            "Train Epoch: 25 [43520/57344 (76%)]\tLoss: 2.666222\n",
            "Train Epoch: 25 [44800/57344 (78%)]\tLoss: 2.676056\n",
            "Train Epoch: 25 [46080/57344 (80%)]\tLoss: 2.699828\n",
            "Train Epoch: 25 [47360/57344 (83%)]\tLoss: 2.598637\n",
            "Train Epoch: 25 [48640/57344 (85%)]\tLoss: 2.616821\n",
            "Train Epoch: 25 [49920/57344 (87%)]\tLoss: 2.614224\n",
            "Train Epoch: 25 [51200/57344 (89%)]\tLoss: 2.609932\n",
            "Train Epoch: 25 [52480/57344 (92%)]\tLoss: 2.607894\n",
            "Train Epoch: 25 [53760/57344 (94%)]\tLoss: 2.628737\n",
            "Train Epoch: 25 [55040/57344 (96%)]\tLoss: 2.688198\n",
            "Train Epoch: 25 [56320/57344 (98%)]\tLoss: 2.678504\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the in the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a <unknown> and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> was eaters had the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still and <unknown> of had was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry he <unknown> than you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7118, Accuracy: 246082/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/025.pt\n",
            "\n",
            "generated sample\t harry potter  what i can said dumbledore come on the <unknown> of the veela and i be\n",
            "generated sample\t harry potter  harry said harry <unknown> his eyes and <unknown> with the <unknown> of the air of\n",
            "generated sample\t harry potter   harry who was wearing a large <unknown> and the door of the lake was \n",
            "generated sample\t harry potter   harry and hermione and hermione were watching him were <unknown> and ron and ron \n",
            "generated sample\t harry potter   i yeah said harry he said harry and ron had to their <unknown> with \n",
            "generated sample\t harry potter  he could be a good <unknown> of the class and you never really a good\n",
            "generated sample\t harry potter   the <unknown> that the only <unknown> <unknown> <unknown> the dark lord will be a \n",
            "generated sample\t harry potter   harry harry and hermione were still as they went through the air they were \n",
            "generated sample\t harry potter  harry he said harry who was it was a <unknown> of the <unknown> of his\n",
            "generated sample\t harry potter  harry harry felt the book and the <unknown> of the water was suddenly <unknown> and\n",
            "generated beam\t\t harry potter  the the rest of the class was not to be in his head and the\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 26 [0/57344 (0%)]\tLoss: 2.895359\n",
            "Train Epoch: 26 [1280/57344 (2%)]\tLoss: 2.674857\n",
            "Train Epoch: 26 [2560/57344 (4%)]\tLoss: 2.699028\n",
            "Train Epoch: 26 [3840/57344 (7%)]\tLoss: 2.674130\n",
            "Train Epoch: 26 [5120/57344 (9%)]\tLoss: 2.645159\n",
            "Train Epoch: 26 [6400/57344 (11%)]\tLoss: 2.637493\n",
            "Train Epoch: 26 [7680/57344 (13%)]\tLoss: 2.648639\n",
            "Train Epoch: 26 [8960/57344 (16%)]\tLoss: 2.696437\n",
            "Train Epoch: 26 [10240/57344 (18%)]\tLoss: 2.682428\n",
            "Train Epoch: 26 [11520/57344 (20%)]\tLoss: 2.600604\n",
            "Train Epoch: 26 [12800/57344 (22%)]\tLoss: 2.612286\n",
            "Train Epoch: 26 [14080/57344 (25%)]\tLoss: 2.666187\n",
            "Train Epoch: 26 [15360/57344 (27%)]\tLoss: 2.627432\n",
            "Train Epoch: 26 [16640/57344 (29%)]\tLoss: 2.602970\n",
            "Train Epoch: 26 [17920/57344 (31%)]\tLoss: 2.600200\n",
            "Train Epoch: 26 [19200/57344 (33%)]\tLoss: 2.638554\n",
            "Train Epoch: 26 [20480/57344 (36%)]\tLoss: 2.678959\n",
            "Train Epoch: 26 [21760/57344 (38%)]\tLoss: 2.606691\n",
            "Train Epoch: 26 [23040/57344 (40%)]\tLoss: 2.698113\n",
            "Train Epoch: 26 [24320/57344 (42%)]\tLoss: 2.715565\n",
            "Train Epoch: 26 [25600/57344 (45%)]\tLoss: 2.671960\n",
            "Train Epoch: 26 [26880/57344 (47%)]\tLoss: 2.628093\n",
            "Train Epoch: 26 [28160/57344 (49%)]\tLoss: 2.634917\n",
            "Train Epoch: 26 [29440/57344 (51%)]\tLoss: 2.655734\n",
            "Train Epoch: 26 [30720/57344 (54%)]\tLoss: 2.602661\n",
            "Train Epoch: 26 [32000/57344 (56%)]\tLoss: 2.623603\n",
            "Train Epoch: 26 [33280/57344 (58%)]\tLoss: 2.623732\n",
            "Train Epoch: 26 [34560/57344 (60%)]\tLoss: 2.606271\n",
            "Train Epoch: 26 [35840/57344 (62%)]\tLoss: 2.675044\n",
            "Train Epoch: 26 [37120/57344 (65%)]\tLoss: 2.648387\n",
            "Train Epoch: 26 [38400/57344 (67%)]\tLoss: 2.663879\n",
            "Train Epoch: 26 [39680/57344 (69%)]\tLoss: 2.627962\n",
            "Train Epoch: 26 [40960/57344 (71%)]\tLoss: 2.643079\n",
            "Train Epoch: 26 [42240/57344 (74%)]\tLoss: 2.712893\n",
            "Train Epoch: 26 [43520/57344 (76%)]\tLoss: 2.663671\n",
            "Train Epoch: 26 [44800/57344 (78%)]\tLoss: 2.673450\n",
            "Train Epoch: 26 [46080/57344 (80%)]\tLoss: 2.698746\n",
            "Train Epoch: 26 [47360/57344 (83%)]\tLoss: 2.597259\n",
            "Train Epoch: 26 [48640/57344 (85%)]\tLoss: 2.617143\n",
            "Train Epoch: 26 [49920/57344 (87%)]\tLoss: 2.616805\n",
            "Train Epoch: 26 [51200/57344 (89%)]\tLoss: 2.609675\n",
            "Train Epoch: 26 [52480/57344 (92%)]\tLoss: 2.607653\n",
            "Train Epoch: 26 [53760/57344 (94%)]\tLoss: 2.626333\n",
            "Train Epoch: 26 [55040/57344 (96%)]\tLoss: 2.687357\n",
            "Train Epoch: 26 [56320/57344 (98%)]\tLoss: 2.676445\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> was eaters had the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still and <unknown> of had was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he i he he him i he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry in <unknown> than you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7116, Accuracy: 246023/430080 (57%)\n",
            "\n",
            "generated sample\t harry potter  harry harry saw a loud voice and a large large robes was to the <unknown>\n",
            "generated sample\t harry potter  professor dumbledore said harry <unknown> the small <unknown> <unknown> <unknown> harry thought he were looking\n",
            "generated sample\t harry potter  harry if you know she opened the water and his eyes was a very long\n",
            "generated sample\t harry potter   hagrid harry said to himself to be back to the ceiling harry felt his \n",
            "generated sample\t harry potter  i i was thinking you know it was a <unknown> he said ron in the\n",
            "generated sample\t harry potter  harry and he could not see him to the <unknown> of the quidditch cup he\n",
            "generated sample\t harry potter  it harry and he saw it was a large <unknown> of the <unknown> had a\n",
            "generated sample\t harry potter   dumbledore said ron looking at her and she was still still what did she \n",
            "generated sample\t harry potter  harry and harry said harry and ron and hermione and hermione had been <unknown> and\n",
            "generated sample\t harry potter harry he was as though he had been <unknown> and <unknown> and <unknown> the ministry \n",
            "generated beam\t\t harry potter  snape snape had never got to be a bit of those <unknown> harry and hermione\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 27 [0/57344 (0%)]\tLoss: 2.894442\n",
            "Train Epoch: 27 [1280/57344 (2%)]\tLoss: 2.673928\n",
            "Train Epoch: 27 [2560/57344 (4%)]\tLoss: 2.698174\n",
            "Train Epoch: 27 [3840/57344 (7%)]\tLoss: 2.673656\n",
            "Train Epoch: 27 [5120/57344 (9%)]\tLoss: 2.642632\n",
            "Train Epoch: 27 [6400/57344 (11%)]\tLoss: 2.635442\n",
            "Train Epoch: 27 [7680/57344 (13%)]\tLoss: 2.648971\n",
            "Train Epoch: 27 [8960/57344 (16%)]\tLoss: 2.695956\n",
            "Train Epoch: 27 [10240/57344 (18%)]\tLoss: 2.682317\n",
            "Train Epoch: 27 [11520/57344 (20%)]\tLoss: 2.597988\n",
            "Train Epoch: 27 [12800/57344 (22%)]\tLoss: 2.608761\n",
            "Train Epoch: 27 [14080/57344 (25%)]\tLoss: 2.667946\n",
            "Train Epoch: 27 [15360/57344 (27%)]\tLoss: 2.625729\n",
            "Train Epoch: 27 [16640/57344 (29%)]\tLoss: 2.599223\n",
            "Train Epoch: 27 [17920/57344 (31%)]\tLoss: 2.598705\n",
            "Train Epoch: 27 [19200/57344 (33%)]\tLoss: 2.637762\n",
            "Train Epoch: 27 [20480/57344 (36%)]\tLoss: 2.676441\n",
            "Train Epoch: 27 [21760/57344 (38%)]\tLoss: 2.605748\n",
            "Train Epoch: 27 [23040/57344 (40%)]\tLoss: 2.696658\n",
            "Train Epoch: 27 [24320/57344 (42%)]\tLoss: 2.713729\n",
            "Train Epoch: 27 [25600/57344 (45%)]\tLoss: 2.668835\n",
            "Train Epoch: 27 [26880/57344 (47%)]\tLoss: 2.627800\n",
            "Train Epoch: 27 [28160/57344 (49%)]\tLoss: 2.635468\n",
            "Train Epoch: 27 [29440/57344 (51%)]\tLoss: 2.657817\n",
            "Train Epoch: 27 [30720/57344 (54%)]\tLoss: 2.602636\n",
            "Train Epoch: 27 [32000/57344 (56%)]\tLoss: 2.622594\n",
            "Train Epoch: 27 [33280/57344 (58%)]\tLoss: 2.622900\n",
            "Train Epoch: 27 [34560/57344 (60%)]\tLoss: 2.607203\n",
            "Train Epoch: 27 [35840/57344 (62%)]\tLoss: 2.674779\n",
            "Train Epoch: 27 [37120/57344 (65%)]\tLoss: 2.646255\n",
            "Train Epoch: 27 [38400/57344 (67%)]\tLoss: 2.663258\n",
            "Train Epoch: 27 [39680/57344 (69%)]\tLoss: 2.627677\n",
            "Train Epoch: 27 [40960/57344 (71%)]\tLoss: 2.643033\n",
            "Train Epoch: 27 [42240/57344 (74%)]\tLoss: 2.712687\n",
            "Train Epoch: 27 [43520/57344 (76%)]\tLoss: 2.664969\n",
            "Train Epoch: 27 [44800/57344 (78%)]\tLoss: 2.670882\n",
            "Train Epoch: 27 [46080/57344 (80%)]\tLoss: 2.697031\n",
            "Train Epoch: 27 [47360/57344 (83%)]\tLoss: 2.598237\n",
            "Train Epoch: 27 [48640/57344 (85%)]\tLoss: 2.616372\n",
            "Train Epoch: 27 [49920/57344 (87%)]\tLoss: 2.617728\n",
            "Train Epoch: 27 [51200/57344 (89%)]\tLoss: 2.605184\n",
            "Train Epoch: 27 [52480/57344 (92%)]\tLoss: 2.608341\n",
            "Train Epoch: 27 [53760/57344 (94%)]\tLoss: 2.628864\n",
            "Train Epoch: 27 [55040/57344 (96%)]\tLoss: 2.684591\n",
            "Train Epoch: 27 [56320/57344 (98%)]\tLoss: 2.677104\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters had the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and and and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and to of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still and <unknown> of had was to off the door to\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and into the <unknown> and the door of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry he was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry he <unknown> than you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7124, Accuracy: 245911/430080 (57%)\n",
            "\n",
            "generated sample\t harry potter   harry it was a very <unknown> of the <unknown> of the ministry was it \n",
            "generated sample\t harry potter   harry i wanted to see the boy harry asked harry and harry opened the \n",
            "generated sample\t harry potter  harry said harry i think i saw you have to do i see you ca\n",
            "generated sample\t harry potter   harry he turned to his face and the <unknown> of the rain was trying \n",
            "generated sample\t harry potter   harry asked he still grinning <unknown> we were you doing it a minute and \n",
            "generated sample\t harry potter   harry harry said he was the dementors of the <unknown> of the ministry he \n",
            "generated sample\t harry potter  i not to take it in the <unknown> of the <unknown> of the ministry and\n",
            "generated sample\t harry potter   what you are going to be back to the dark lord said ron but \n",
            "generated sample\t harry potter   harry let go and he said to harry and hermione and harry saw the \n",
            "generated sample\t harry potter   dumbledore said ron i think it was a <unknown> he said it do you \n",
            "generated beam\t\t harry potter  i said harry but he was going to see the <unknown> of his seat and\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 28 [0/57344 (0%)]\tLoss: 2.896544\n",
            "Train Epoch: 28 [1280/57344 (2%)]\tLoss: 2.673396\n",
            "Train Epoch: 28 [2560/57344 (4%)]\tLoss: 2.696934\n",
            "Train Epoch: 28 [3840/57344 (7%)]\tLoss: 2.673729\n",
            "Train Epoch: 28 [5120/57344 (9%)]\tLoss: 2.642029\n",
            "Train Epoch: 28 [6400/57344 (11%)]\tLoss: 2.636202\n",
            "Train Epoch: 28 [7680/57344 (13%)]\tLoss: 2.648448\n",
            "Train Epoch: 28 [8960/57344 (16%)]\tLoss: 2.697021\n",
            "Train Epoch: 28 [10240/57344 (18%)]\tLoss: 2.681800\n",
            "Train Epoch: 28 [11520/57344 (20%)]\tLoss: 2.596707\n",
            "Train Epoch: 28 [12800/57344 (22%)]\tLoss: 2.609197\n",
            "Train Epoch: 28 [14080/57344 (25%)]\tLoss: 2.664891\n",
            "Train Epoch: 28 [15360/57344 (27%)]\tLoss: 2.623270\n",
            "Train Epoch: 28 [16640/57344 (29%)]\tLoss: 2.599735\n",
            "Train Epoch: 28 [17920/57344 (31%)]\tLoss: 2.596887\n",
            "Train Epoch: 28 [19200/57344 (33%)]\tLoss: 2.634437\n",
            "Train Epoch: 28 [20480/57344 (36%)]\tLoss: 2.677376\n",
            "Train Epoch: 28 [21760/57344 (38%)]\tLoss: 2.604830\n",
            "Train Epoch: 28 [23040/57344 (40%)]\tLoss: 2.696449\n",
            "Train Epoch: 28 [24320/57344 (42%)]\tLoss: 2.713304\n",
            "Train Epoch: 28 [25600/57344 (45%)]\tLoss: 2.669553\n",
            "Train Epoch: 28 [26880/57344 (47%)]\tLoss: 2.627532\n",
            "Train Epoch: 28 [28160/57344 (49%)]\tLoss: 2.634364\n",
            "Train Epoch: 28 [29440/57344 (51%)]\tLoss: 2.654527\n",
            "Train Epoch: 28 [30720/57344 (54%)]\tLoss: 2.602177\n",
            "Train Epoch: 28 [32000/57344 (56%)]\tLoss: 2.618689\n",
            "Train Epoch: 28 [33280/57344 (58%)]\tLoss: 2.622720\n",
            "Train Epoch: 28 [34560/57344 (60%)]\tLoss: 2.606605\n",
            "Train Epoch: 28 [35840/57344 (62%)]\tLoss: 2.673493\n",
            "Train Epoch: 28 [37120/57344 (65%)]\tLoss: 2.646100\n",
            "Train Epoch: 28 [38400/57344 (67%)]\tLoss: 2.664086\n",
            "Train Epoch: 28 [39680/57344 (69%)]\tLoss: 2.628043\n",
            "Train Epoch: 28 [40960/57344 (71%)]\tLoss: 2.641725\n",
            "Train Epoch: 28 [42240/57344 (74%)]\tLoss: 2.711717\n",
            "Train Epoch: 28 [43520/57344 (76%)]\tLoss: 2.663579\n",
            "Train Epoch: 28 [44800/57344 (78%)]\tLoss: 2.670694\n",
            "Train Epoch: 28 [46080/57344 (80%)]\tLoss: 2.699202\n",
            "Train Epoch: 28 [47360/57344 (83%)]\tLoss: 2.597464\n",
            "Train Epoch: 28 [48640/57344 (85%)]\tLoss: 2.616552\n",
            "Train Epoch: 28 [49920/57344 (87%)]\tLoss: 2.614736\n",
            "Train Epoch: 28 [51200/57344 (89%)]\tLoss: 2.606690\n",
            "Train Epoch: 28 [52480/57344 (92%)]\tLoss: 2.607159\n",
            "Train Epoch: 28 [53760/57344 (94%)]\tLoss: 2.626336\n",
            "Train Epoch: 28 [55040/57344 (96%)]\tLoss: 2.685447\n",
            "Train Epoch: 28 [56320/57344 (98%)]\tLoss: 2.676359\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a <unknown> and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> was eaters had the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still and <unknown> of had was to off the door and\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him he he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he were got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry in <unknown> than you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7110, Accuracy: 246048/430080 (57%)\n",
            "\n",
            "generated sample\t harry potter  harry harry was still the <unknown> of his eyes had been to make a lot\n",
            "generated sample\t harry potter  harry harry and ron looked back at the door of the <unknown> of the <unknown>\n",
            "generated sample\t harry potter  professor harry he was still <unknown> he was right in the <unknown> of the department\n",
            "generated sample\t harry potter  harry harry had to be to be there to be a <unknown> but his eyes\n",
            "generated sample\t harry potter   harry he turned to his head and he said quietly he was wearing a \n",
            "generated sample\t harry potter professor snape he said it was a very long <unknown> and a <unknown> of the \n",
            "generated sample\t harry potter  dumbledore harry said ron and hermione looked up at the door of the room of\n",
            "generated sample\t harry potter  harry come on harry said hagrid he looked at the <unknown> and the great hall\n",
            "generated sample\t harry potter  i do think you know said harry in a <unknown> voice he said you got\n",
            "generated sample\t harry potter  to the <unknown> of the <unknown> of the <unknown> of the ministry and the ministry\n",
            "generated beam\t\t harry potter  the first time you were right the <unknown> of the <unknown> of the class who\n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 29 [0/57344 (0%)]\tLoss: 2.894684\n",
            "Train Epoch: 29 [1280/57344 (2%)]\tLoss: 2.672139\n",
            "Train Epoch: 29 [2560/57344 (4%)]\tLoss: 2.695912\n",
            "Train Epoch: 29 [3840/57344 (7%)]\tLoss: 2.671974\n",
            "Train Epoch: 29 [5120/57344 (9%)]\tLoss: 2.641025\n",
            "Train Epoch: 29 [6400/57344 (11%)]\tLoss: 2.634044\n",
            "Train Epoch: 29 [7680/57344 (13%)]\tLoss: 2.648310\n",
            "Train Epoch: 29 [8960/57344 (16%)]\tLoss: 2.696251\n",
            "Train Epoch: 29 [10240/57344 (18%)]\tLoss: 2.682481\n",
            "Train Epoch: 29 [11520/57344 (20%)]\tLoss: 2.595967\n",
            "Train Epoch: 29 [12800/57344 (22%)]\tLoss: 2.606654\n",
            "Train Epoch: 29 [14080/57344 (25%)]\tLoss: 2.662771\n",
            "Train Epoch: 29 [15360/57344 (27%)]\tLoss: 2.623175\n",
            "Train Epoch: 29 [16640/57344 (29%)]\tLoss: 2.595219\n",
            "Train Epoch: 29 [17920/57344 (31%)]\tLoss: 2.596520\n",
            "Train Epoch: 29 [19200/57344 (33%)]\tLoss: 2.637269\n",
            "Train Epoch: 29 [20480/57344 (36%)]\tLoss: 2.675592\n",
            "Train Epoch: 29 [21760/57344 (38%)]\tLoss: 2.601341\n",
            "Train Epoch: 29 [23040/57344 (40%)]\tLoss: 2.695202\n",
            "Train Epoch: 29 [24320/57344 (42%)]\tLoss: 2.712379\n",
            "Train Epoch: 29 [25600/57344 (45%)]\tLoss: 2.666801\n",
            "Train Epoch: 29 [26880/57344 (47%)]\tLoss: 2.627639\n",
            "Train Epoch: 29 [28160/57344 (49%)]\tLoss: 2.634328\n",
            "Train Epoch: 29 [29440/57344 (51%)]\tLoss: 2.655071\n",
            "Train Epoch: 29 [30720/57344 (54%)]\tLoss: 2.602361\n",
            "Train Epoch: 29 [32000/57344 (56%)]\tLoss: 2.621248\n",
            "Train Epoch: 29 [33280/57344 (58%)]\tLoss: 2.621619\n",
            "Train Epoch: 29 [34560/57344 (60%)]\tLoss: 2.606536\n",
            "Train Epoch: 29 [35840/57344 (62%)]\tLoss: 2.672626\n",
            "Train Epoch: 29 [37120/57344 (65%)]\tLoss: 2.645734\n",
            "Train Epoch: 29 [38400/57344 (67%)]\tLoss: 2.663176\n",
            "Train Epoch: 29 [39680/57344 (69%)]\tLoss: 2.628030\n",
            "Train Epoch: 29 [40960/57344 (71%)]\tLoss: 2.643725\n",
            "Train Epoch: 29 [42240/57344 (74%)]\tLoss: 2.712806\n",
            "Train Epoch: 29 [43520/57344 (76%)]\tLoss: 2.664557\n",
            "Train Epoch: 29 [44800/57344 (78%)]\tLoss: 2.671847\n",
            "Train Epoch: 29 [46080/57344 (80%)]\tLoss: 2.696975\n",
            "Train Epoch: 29 [47360/57344 (83%)]\tLoss: 2.596531\n",
            "Train Epoch: 29 [48640/57344 (85%)]\tLoss: 2.615175\n",
            "Train Epoch: 29 [49920/57344 (87%)]\tLoss: 2.613876\n",
            "Train Epoch: 29 [51200/57344 (89%)]\tLoss: 2.605429\n",
            "Train Epoch: 29 [52480/57344 (92%)]\tLoss: 2.606830\n",
            "Train Epoch: 29 [53760/57344 (94%)]\tLoss: 2.626661\n",
            "Train Epoch: 29 [55040/57344 (96%)]\tLoss: 2.686366\n",
            "Train Epoch: 29 [56320/57344 (98%)]\tLoss: 2.675431\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and a large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters had the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his of and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still and <unknown> of had was to off the door to\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him i he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry i <unknown> than you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7105, Accuracy: 246075/430080 (57%)\n",
            "\n",
            "generated sample\t harry potter   harry the man was still still <unknown> and he was a very large <unknown> \n",
            "generated sample\t harry potter  hermione harry muttered and the sound of the end of the pensieve harry and ron\n",
            "generated sample\t harry potter  it and he saw what he was <unknown> he had been been in the room\n",
            "generated sample\t harry potter  harry i got to get to the end of the dark arts said hagrid but\n",
            "generated sample\t harry potter  you harry potter and he reached the door and the rest of the class who\n",
            "generated sample\t harry potter  harry said harry and the <unknown> of them had to be <unknown> in the right\n",
            "generated sample\t harry potter   harry said harry grinning i think he going to be a new wizard to \n",
            "generated sample\t harry potter  you i expect you will be a <unknown> said harry <unknown> his head and a\n",
            "generated sample\t harry potter   harry potter he was still back in the air and a very pale <unknown> \n",
            "generated sample\t harry potter  harry and he was wearing his hand in his office he found the door and\n",
            "generated beam\t\t harry potter   i do not to go on the end of the slytherin first year i \n",
            "\n",
            "enumerate train   448\n",
            "Train Epoch: 30 [0/57344 (0%)]\tLoss: 2.895162\n",
            "Train Epoch: 30 [1280/57344 (2%)]\tLoss: 2.671082\n",
            "Train Epoch: 30 [2560/57344 (4%)]\tLoss: 2.695832\n",
            "Train Epoch: 30 [3840/57344 (7%)]\tLoss: 2.671719\n",
            "Train Epoch: 30 [5120/57344 (9%)]\tLoss: 2.641150\n",
            "Train Epoch: 30 [6400/57344 (11%)]\tLoss: 2.634395\n",
            "Train Epoch: 30 [7680/57344 (13%)]\tLoss: 2.647074\n",
            "Train Epoch: 30 [8960/57344 (16%)]\tLoss: 2.695454\n",
            "Train Epoch: 30 [10240/57344 (18%)]\tLoss: 2.680995\n",
            "Train Epoch: 30 [11520/57344 (20%)]\tLoss: 2.595346\n",
            "Train Epoch: 30 [12800/57344 (22%)]\tLoss: 2.606122\n",
            "Train Epoch: 30 [14080/57344 (25%)]\tLoss: 2.662323\n",
            "Train Epoch: 30 [15360/57344 (27%)]\tLoss: 2.622382\n",
            "Train Epoch: 30 [16640/57344 (29%)]\tLoss: 2.597014\n",
            "Train Epoch: 30 [17920/57344 (31%)]\tLoss: 2.594643\n",
            "Train Epoch: 30 [19200/57344 (33%)]\tLoss: 2.633849\n",
            "Train Epoch: 30 [20480/57344 (36%)]\tLoss: 2.676141\n",
            "Train Epoch: 30 [21760/57344 (38%)]\tLoss: 2.603784\n",
            "Train Epoch: 30 [23040/57344 (40%)]\tLoss: 2.697669\n",
            "Train Epoch: 30 [24320/57344 (42%)]\tLoss: 2.712660\n",
            "Train Epoch: 30 [25600/57344 (45%)]\tLoss: 2.667556\n",
            "Train Epoch: 30 [26880/57344 (47%)]\tLoss: 2.626467\n",
            "Train Epoch: 30 [28160/57344 (49%)]\tLoss: 2.634128\n",
            "Train Epoch: 30 [29440/57344 (51%)]\tLoss: 2.651825\n",
            "Train Epoch: 30 [30720/57344 (54%)]\tLoss: 2.603168\n",
            "Train Epoch: 30 [32000/57344 (56%)]\tLoss: 2.618693\n",
            "Train Epoch: 30 [33280/57344 (58%)]\tLoss: 2.622048\n",
            "Train Epoch: 30 [34560/57344 (60%)]\tLoss: 2.605957\n",
            "Train Epoch: 30 [35840/57344 (62%)]\tLoss: 2.673016\n",
            "Train Epoch: 30 [37120/57344 (65%)]\tLoss: 2.643830\n",
            "Train Epoch: 30 [38400/57344 (67%)]\tLoss: 2.660864\n",
            "Train Epoch: 30 [39680/57344 (69%)]\tLoss: 2.627395\n",
            "Train Epoch: 30 [40960/57344 (71%)]\tLoss: 2.642087\n",
            "Train Epoch: 30 [42240/57344 (74%)]\tLoss: 2.710845\n",
            "Train Epoch: 30 [43520/57344 (76%)]\tLoss: 2.660851\n",
            "Train Epoch: 30 [44800/57344 (78%)]\tLoss: 2.670471\n",
            "Train Epoch: 30 [46080/57344 (80%)]\tLoss: 2.695782\n",
            "Train Epoch: 30 [47360/57344 (83%)]\tLoss: 2.595957\n",
            "Train Epoch: 30 [48640/57344 (85%)]\tLoss: 2.614344\n",
            "Train Epoch: 30 [49920/57344 (87%)]\tLoss: 2.610744\n",
            "Train Epoch: 30 [51200/57344 (89%)]\tLoss: 2.606028\n",
            "Train Epoch: 30 [52480/57344 (92%)]\tLoss: 2.606961\n",
            "Train Epoch: 30 [53760/57344 (94%)]\tLoss: 2.625836\n",
            "Train Epoch: 30 [55040/57344 (96%)]\tLoss: 2.687950\n",
            "Train Epoch: 30 [56320/57344 (98%)]\tLoss: 2.675150\n",
            "enumerate test    112\n",
            "Input\tharry helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff \n",
            "GT\t helped dumbledore back into the icy <unknown> that filled the <unknown> in the cliff it\n",
            "pred\t and and and to the air room of was the door of the air of\n",
            "\n",
            "\n",
            "Input\tsalt the sea breeze had gone he and dumbledore were shivering and dripping in the \n",
            "GT\t the sea breeze had gone he and dumbledore were shivering and dripping in the middle\n",
            "pred\t and <unknown> of in been to was ron had still in the to the air\n",
            "\n",
            "\n",
            "Input\ton to the ground it ok sir you going to be all right do worry \n",
            "GT\t to the ground it ok sir you going to be all right do worry he\n",
            "pred\t the the door and was he he know to be a right said you said\n",
            "\n",
            "\n",
            "Input\tdown the dark street towards them on fluffy slippers wearing a silk embroidered with dragons \n",
            "GT\t the dark street towards them on fluffy slippers wearing a silk embroidered with dragons saw\n",
            "pred\t the door arts and the and the and and his large and and a and\n",
            "\n",
            "\n",
            "Input\twith a serpent tongue the mark death eaters left behind whenever they had entered a \n",
            "GT\t a serpent tongue the mark death eaters left behind whenever they had entered a building\n",
            "pred\t a <unknown> of and <unknown> of eaters had the the the were to the <unknown>\n",
            "\n",
            "\n",
            "Input\tat waist height please send a message to the ministry said dumbledore as he mounted \n",
            "GT\t waist height please send a message to the ministry said dumbledore as he mounted the\n",
            "pred\t the and of and him <unknown> to the <unknown> of harry <unknown> they walked the\n",
            "\n",
            "\n",
            "Input\tlike a venomous bubble <unknown> his lungs driving all other discomfort from his mind how \n",
            "GT\t a venomous bubble <unknown> his lungs driving all other discomfort from his mind how long\n",
            "pred\t a <unknown> of and and eyes and his the and and the head he he\n",
            "\n",
            "\n",
            "Input\tinto the grounds dumbledore was <unknown> the enchantments he himself had set around the castle \n",
            "GT\t the grounds dumbledore was <unknown> the enchantments he himself had set around the castle so\n",
            "pred\t the air and was still and <unknown> of had was to off the door to\n",
            "\n",
            "\n",
            "Input\tdumbledore faintly but clearly tell him what has happened and bring him to me do \n",
            "GT\t faintly but clearly tell him what has happened and bring him to me do ing\n",
            "pred\t he he he he him i he you to i him to the and you\n",
            "\n",
            "\n",
            "Input\the saw dumbledore wand flying in an arc over the edge of the ramparts and \n",
            "GT\t saw dumbledore wand flying in an arc over the edge of the ramparts and dumbledore\n",
            "pred\t had the he and around the <unknown> and the <unknown> of the air of the\n",
            "\n",
            "\n",
            "Input\tin did you said malfoy who was panting under your nose and you never realised \n",
            "GT\t did you said malfoy who was panting under your nose and you never realised said\n",
            "pred\t the you know harry i was still to the head and he know got you\n",
            "\n",
            "\n",
            "Input\tcapable of said malfoy more forcefully do know what i done yes i do said \n",
            "GT\t of said malfoy more forcefully do know what i done yes i do said dumbledore\n",
            "pred\t of the harry in <unknown> than you what you do to said do think harry\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 2.7092, Accuracy: 246109/430080 (57%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/030.pt\n",
            "\n",
            "generated sample\t harry potter   hagrid said harry and he was still sure of his head in the <unknown> \n",
            "generated sample\t harry potter  it the second time i do want it to the ministry of magic for the\n",
            "generated sample\t harry potter  harry i do think i do want to get you in the man said harry\n",
            "generated sample\t harry potter   harry said harry as a moment he had never been <unknown> to him at \n",
            "generated sample\t harry potter   she was still in the air of his seat and the <unknown> of the \n",
            "generated sample\t harry potter  harry harry and hermione looked around and goyle were lying in front of the castle\n",
            "generated sample\t harry potter  they i not the first time i do think you do to get a <unknown>\n",
            "generated sample\t harry potter  harry he said he said nothing to be a <unknown> about <unknown> that he could\n",
            "generated sample\t harry potter   the floor ron and hermione and hermione were still in the <unknown> on the \n",
            "generated sample\t harry potter  harry i think it are you going to find you in the ministry of magic\n",
            "generated beam\t\t harry potter   he said harry and ron he had to go and ron and hermione were \n",
            "\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints_wordss/030.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6gNVIMbzBMhR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 6: Experiments"
      ]
    },
    {
      "metadata": {
        "id": "b_g4gedSN443",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "50a805be-4cd7-4ac2-b60a-c7c1f475c61a"
      },
      "cell_type": "code",
      "source": [
        "## With PUNCTUATION\n",
        "seed_words = 'harry and ron'\n",
        "sequence_length = 50\n",
        "\n",
        "generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'max')\n",
        "print('generated with max\\t', generated_sentence)\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'sample')\n",
        "    print('generated with sample\\t', generated_sentence)\n",
        "    \n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with max\t harry and ron . '' `` i 'm going to be a <unknown> , '' said harry , `` i 'm not going to be a <unknown> .\n",
            "generated with sample\t harry and ron . '' fred weasley had seen , but he did i have talked to me anything , but he cleared his head , and then\n",
            "generated with sample\t harry and ron will `` but she would have to under this game ! '' the portrait hand sounded . but <unknown> , professor . ... `` i\n",
            "generated with sample\t harry and ron <unknown> every large . there ! get deserves into excitement , below <unknown> will be visiting on and <unknown> anything . ... maybe it was\n",
            "generated with sample\t harry and ron now . and then , and lay . . . . . `` nightfall ? '' said harry , . he went from a bag\n",
            "generated with sample\t harry and ron . . . . . <unknown> lord . harry might have dad put to remain into his intend once at the other so i 'm\n",
            "generated with sample\t harry and ron bang . she turned her hair to live her son , yet hermione was not care of ravenclaw 's present . cat was happening ...\n",
            "generated with sample\t harry and ron of subjects he <unknown> out . '' `` what 're of the gloves ? '' harry hissed to him . `` our roar of a\n",
            "generated with sample\t harry and ron . will we do ? '' harry a spell with ginny , harry felt mental , `` could n't it , '' said ron ,\n",
            "generated with sample\t harry and ron on one words . `` sir , '' fudge said . `` but we even known from harry . '' `` completely voice , ''\n",
            "generated with sample\t harry and ron . he had mastered the homework by <unknown> . . . . there fell from ... of sir quidditch , could you dock them ,\n",
            "generated with beam\t harry and ron over with quidditch unconscious . . birthdays he was right about it . umbridge was very scornfully of a first much information . `` that\n",
            "generated with beam\t harry and ron , or i galloping to <unknown> . `` wizards you should friends ' <unknown> knowin '' and you already straightening , normally <unknown> ... ''\n",
            "generated with beam\t harry and ron ' but even gone . `` is there ? harry er as i fed her , '' said vote , scrambling . `` can i\n",
            "generated with beam\t harry and ron . . to slytherins in a manner world when hagrid , then , what ? '' the <unknown> of cold was little height to his\n",
            "generated with beam\t harry and ron . you is n't nothing love about hogwarts ... at the ministry splash several -well from the <unknown> merlin . he 's nearly brains in\n",
            "generated with beam\t harry and ron , earlier a lot of <unknown> all the dog school . dumbledore fangs from him . '' he card , whoosh the back of gathering\n",
            "generated with beam\t harry and ron knows that edged there were . `` its keeps right , yeah , is very shrink , the same maniac of death eaters that by\n",
            "generated with beam\t harry and ron and make it at hogwarts . every case that landed saves that black <unknown> , uncomfortably . . . '' he back . weasley had\n",
            "generated with beam\t harry and ron could <unknown> , hitting snape , some of swearing from you in this scar for the school can <unknown> . it seemed to be here\n",
            "generated with beam\t harry and ron . a girl . harry 's day likes someone not been given almost strung . he , turning to the shop , he had lifting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y2qtJg8hugX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "1d880337-8ab5-4d58-85b6-c0120e4de765"
      },
      "cell_type": "code",
      "source": [
        "## Without PUNCTUATION\n",
        "seed_words = 'harry and ron'\n",
        "sequence_length = 40\n",
        "\n",
        "generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'max')\n",
        "print('generated with max\\t', generated_sentence)\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'sample')\n",
        "    print('generated with sample\\t', generated_sentence)\n",
        "    \n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with max\t harry and ron harry said harry and hermione and hermione were still <unknown> and they were still <unknown> and they were still <unknown>\n",
            "generated with sample\t harry and ron he killed harry i would want to this he told me harry come back at neville bed he <unknown> to\n",
            "generated with sample\t harry and ron with harry potter she walked splintered silencio his face as lockhart completely opened the book he felt like the <unknown>\n",
            "generated with sample\t harry and ron shot harry said smiling at once and eerily loves was shafts that remembering harry never turned late in every two\n",
            "generated with sample\t harry and ron all if harry school by those here you without joke and looking at hermione who had put his natural of\n",
            "generated with sample\t harry and ron feel he finished i can months for us some because he seized his friends off he took the crowd of\n",
            "generated with sample\t harry and ron an apparated work on his own comment yes though they loads into an easier yeh have to mclaggen forward whiff\n",
            "generated with sample\t harry and ron i yes said hermione quickly all the same you no asked harry harry continued to turn into the air unscathed\n",
            "generated with sample\t harry and ron by the diary on gaping for attempt and that next it appeared i for the slice of potions before why\n",
            "generated with sample\t harry and ron something more badly in the admire as though not said ron opened but weasley clearing her paces was looking like\n",
            "generated with sample\t harry and ron outstretched eleven it handed trapdoor his bullfrog contact and a new behind began to bread again when i granger <unknown>\n",
            "generated with beam\t harry and ron he said harry i do know said harry he turned to look at him as though he was looking at\n",
            "generated with beam\t harry and ron i said harry quickly i know i did not want to go to the door in the end of the\n",
            "generated with beam\t harry and ron said harry and hermione turned to look at the end of the end of the air back in the air\n",
            "generated with beam\t harry and ron i think you going to do it said harry i do want to go and i want to go to\n",
            "generated with beam\t harry and ron but it was a very <unknown> of the rest of the <unknown> of the <unknown> of the ministry of magic\n",
            "generated with beam\t harry and ron harry turned to harry and ron and hermione looked around at the door they were going to be <unknown> in\n",
            "generated with beam\t harry and ron harry turned out of the air in the air he looked around at the end of the <unknown> of the\n",
            "generated with beam\t harry and ron said fudge smiling at harry i see you know said harry and i do want to think it was a\n",
            "generated with beam\t harry and ron said harry at the ministry of magic he said hermione and ron and hermione said harry and ron and hermione\n",
            "generated with beam\t harry and ron fred and george looked around harry and hermione and ron and ginny were staring at the end of the room\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}